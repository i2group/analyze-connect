{
  "content/example-connectors/connector-async.html": {
    "href": "content/example-connectors/connector-async.html",
    "title": "Asynchronous services",
    "keywords": "Asynchronous services The Async connector demonstrates the ability for a connector service to query data asynchronously. Introduced with i2 Analyze 4.3.3, asynchronous queries allow you to run multiple queries concurrently, and allow long-running queries to persist after the user logs out. The example connector connects to a JSON file that is populated with sample data (people and their friends) and marshals the data into entities, links and properties. There are a number of fields in the dataset: Field Name Type id String forename String surname String dob String ssn String issuedDateAndTime String friends Array of Strings image String Data model The Async schema is modelled on the fields of the people.json data source which can be found in async\\async-connector\\src\\main\\resources . Each entry in the people object is represented by a Person entity and a Friends With link to their friends alongside the properties that are extracted from each field. The schema (and charting schemes) for the Async connector are in the schema directory of this repository. Entity: Person Represents a person in the people object. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Property Type Logical Type Derived From First Name SINGLE_LINE_STRING forename Last Name SINGLE_LINE_STRING surname Year of Birth DATE dob Social Security Number SINGLE_LINE_STRING ssn SSN Issued Date and Time DATE_AND_TIME issuedDateAndTime Link Establishes a connection between two Person entities. Link Type Link Ends Friends With Person --> Person Adding an async service 1. Adding the async service For a general understanding about how to add a service, see Adding a service . To define a service as asynchronous, the service object must be updated. Field Description async Indicates that the service must be called asynchronously, and provides configuration settings. If this is present, then acquireUrl must not be present. Asynchronous configuration Field Description queriesResource This specifies the URL for i2 Analyze to use to get data asynchronously from the service. pollingIntervalInSeconds (Optional) The recommended interval at which clients should poll asynchronous endpoints for changes. The connector configuration matches the example below when it contains an async service. The full configuration of the example asynchronous connector is in connector\\async\\async-connector\\src\\main\\resources\\config.json . { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"sample-async-service\", \"name\": \"Sample Connector: Async\", \"description\": \"A sample service that runs the sevice asynchronously\", \"async\": { \"queriesResource\": \"/async\", \"pollingIntervalInSeconds\": 1 }, \"clientConfigType\": \"NONE\", } ] } 2. Implementing the acquire endpoint This endpoint starts an asynchronous query to fetch data from the service in the specified query resource. If the service in question also specifies the validateUrl property, then the gateway uses the validateUrl endpoint before it uses the queriesResource endpoint. This service calls the POST method on the /ASYNC_SERVICE_URL endpoint and takes in a request parameter. It is a payload that the service can interpret to modify its behaviour. The service can have one of three types of client configuration: 'NONE' - the payload never contains conditions. 'FORM' - it contains conditions that have a fixed structure. 'CUSTOM' - the contents of the conditions object are free-form. The response must contain a query ID that the client can use in subsequent requests to retrieve further information. A sample solution has been written in Java and can be found in the asyncAcquireService() method in ConnectorController with it being implemented in asyncAcquire() in ExternalConnectorDataService . 3. Implementing the status endpoint This endpoint fetches the status of the specified asynchronous query from the specified queries resource. After a client starts an asynchronous query, it uses polling to determine its progress. The client polls the i2 Connect gateway, and the gateway calls through to the connector to retrieve its status. This service calls the GET method on the /ASYNC_SERVICE_URL/{queryId} endpoint. It queries that endpoint using the provided queryId to identify the current state of the query. The response must contain one of three state values that indicate the status of the query: 'STARTED', 'SUCCEEDED', or 'FAILED'. The response can also contain a series of substatuses that report progress from the underlying data source. The four valid types of substatus are 'INFORMATION', 'WARNING', 'ERROR', and 'SUCCESS'. A sample solution has been written in Java and can be found in the asyncStatusService() method in ConnectorController with it being implemented in asyncStatus() in ExternalConnectorDataService . 4. Implementing a results endpoint This endpoint fetches the results of the specified asynchronous query from the specified queries resource. Clients can attempt to retrieve results only from an asynchronous query whose state is 'SUCCEEDED'. An attempt to fetch results from a query in any other state must fail. This service calls the GET method on the /ASYNC_SERVICE_URL/{queryId}/results endpoint. It queries that endpoint using the provided queryId to fetch the results of the successful query. The structure of the response is identical to the response from a synchronous query. A sample solution has been written in Java and can be found in the asyncResultsService() method in ConnectorController with it being implemented in asyncResults() in ExternalConnectorDataService . 5. Implementing a delete endpoint This endpoint deletes the specified asynchronous query from the specified queries resource when it is no longer needed. The i2 Connect gateway calls the DELETE method when a query has succeeded, been cancelled, or failed. A connector can respond to the call by cleaning up any resources associated with processing the query. This service calls the DELETE method on the /ASYNC_SERVICE_URL/{queryId} endpoint. It queries that endpoint using the provided queryId to cancel the query and delete the queryId . The response must be returned with the queryId removed. A sample solution has been written in Java and can be found in the asyncDeleteService() method in ConnectorController with it being implemented in asyncDelete() in ExternalConnectorDataService . Setup These instructions are for setting up and running the Async connector. The solution uses a client configuration of type FORM to demonstrate the use of an async query. The duration condition is used to demonstrate the time taken for a long running query to be executed. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the Async connector: <wars> <war ... name=\"opal-services-daod\" ... > ... <connector-ids> <connector-id value=\"async-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9085\" name=\"Async Connector\" id=\"async-connector\"/> </connectors> </ns1:topology> Ensure that you are using the same port as specified in application.properties ( 9085 by default) and that the value of the id attribute is the same as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the Async schema as a connector schema or a gateway schema. Connector schema By default, your connector has the Async schema configured as a connector schema. schemaUrl , chartingSchemesUrl , and schemaShortName are defined in the Async connector's config.json file, in async\\async-connector\\src\\main\\resources . Additionally, ensure the following is true: The connector's config.json does not contain a gatewaySchema property. The Async <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. For more information, see configuring a connector schema . Gateway schema If you want to set up the Async schema as a gateway schema, follow the guidelines for configuring a gateway schema using the Async schema and charting scheme in the schema directory of this repository. Additionally, ensure the following is true: The connector's config.json in async\\async-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default. If they are present, remove them. The Async <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. (Optional) Configure the query timeout By default, i2 Analyze expects asynchronous queries to complete within 10 minutes of being started. If that doesn't happen, the gateway stops asking the connector for its status. Next time the client polls for information, the server informs it of the failure. To change the default timeout period and support longer-running queries, you can add a setting named AsyncQueryTimeoutSeconds to the DiscoServerSettingsCommon.properties file. For example, to set the timeout to an hour: AsyncQueryTimeoutSeconds=3600 4. Run the Async connector To run the connector, navigate to connector\\async\\async-connector in your terminal and run the application using the following command: mvnw spring-boot:run 5. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-auth.html": {
    "href": "content/example-connectors/connector-auth.html",
    "title": "Authentication",
    "keywords": "Authentication The Auth connector demonstrates the ability to configure authentication for individual services on a connector. Since i2 Analyze version 4.3.4, you can configure a connector service to require a user's credentials before initiating a query, be it synchronous or asynchronous. This mechanism enables connectors to authenticate users with third-party services. Similar to client configurations, you can define reusable authentication configurations that can be shared across services in the connector configuration. These are groups of form fields that the user fills in with their credentials in order to authenticate. After a user authenticates with a service using a specific authentication configuration, the user is able to query all services that use the same authentication configuration. Note : Connector service authentication is completely separate from Liberty's user authentication. Connector service authentication is managed on the connector domain and controls access to connector services. Liberty's user authentication is managed by the i2 Analyze server, controlling access to the i2 Connect gateway. Configuring authentication for a synchronous service The following are the steps required to successfully set up authentication for a synchronous service. 1. Defining an authentication configuration Authentication configurations are defined in the authConfigs array of a connector's configuration. For example, the Auth connector defines two authentication configurations in its config.json file: { \"defaultValues\": { ... }, \"services\": [ ... ], \"authConfigs\": [ { \"id\": \"authConfig1\", \"loginUrl\": \"/login/userpass\", \"form\": { \"description\": \"This service requires a username and password.\", \"fields\": [ { \"id\": \"username\", \"label\": \"Username\", \"type\": \"text\" }, { \"id\": \"password\", \"label\": \"Password\", \"type\": \"password\" } ] } }, { \"id\": \"authConfig2\", \"loginUrl\": \"/login/apikey\", \"form\": { \"description\": \"This service requires an API key.\", \"fields\": [ { \"id\": \"apikey\", \"label\": \"API Key\", \"type\": \"password\" } ] } } ] } In the above, two authConfigs are defined. Each configuration has: An id which needs to be unique among the authConfigs . A loginUrl which is relative to the connector's base URL. This is used to determine where authentication requests should be made. A form object which includes the following: A description briefly describing the form. A fields property which is an array of form fields. Each form field is an object containing: An id unique among the form fields. A label shown on the user interface alongside the form field. A type indicating the type of user input. Accepted values are text and password . 2. Assigning an authentication configuration to a service For a general understanding about how to add a service, see Adding a service . To configure a service to use an authentication configuration, add an authConfigId field in the connector service definition and assign it the ID of the authentication configuration. For example: { \"defaultValues\": { ... }, \"services\": [ { \"id\": \"acquire-service\", \"name\": \"Acquire Service\", \"description\": \"Acquires all data. Requires authentication.\", \"clientConfigType\": \"NONE\", \"authConfigId\": \"authConfig1\" } ], \"authConfigs\": [ ... ] } You cannot assign multiple authentication configurations to a single service. But you can assign the same authentication configuration to multiple services. 3. Implementing the login endpoint The login endpoint is called when the user submits their credentials for a service using an authentication configuration. Here, the submitted credentials should be checked to ensure they are valid. If they are, the login endpoint should generate an authorization token string and include that in the response body like so: { token: \"authorization-token-string\" } The Auth connector example generates signed JSON Web Tokens (JWTs) which the endpoint responds with as an example. You are free to use your own token generation and verification mechanisms. This generated token is cached by the gateway and used in subsequent connector requests to services using the same authentication configuration. If the credentials are invalid, the endpoint should respond with a 401 status code alongside an RFC7807 ( ProblemDetails ) response body. This ProblemDetails object contains the following fields: The title ; a brief summary of the problem. The detail ; An explanation specific to the occurrence of the problem. The type ; a URI reference that identifies the problem type. To indicate the problem is caused by a connector authentication error, this value must be set to: urn:uuid:264caa46-75cb-4ac5-891a-11adeb48b6fb The status ; the HTTP response code (usually a 401 ). The instance ; an optional URI reference which identifies the specific occurrence of the problem, relative to the connector's base URL. Absolute URLs are also accepted. 4. Implementing authentication verification on endpoints The gateway uses bearer authentication to authorize requests made to protected resources. In this authentication scheme, security tokens are sent in the Authorization header of the request, looking like the following: Authorization: Bearer <token> To implement verification of authentication for the service, the token submitted in the Authorization header of the request needs to be validated. If you are not outsourcing the verification of authentication tokens to a third-party service, your connector implementation will need to ensure the following: The Authorization header exists on the request It starts with the Bearer prefix The <token> exists and is valid (i.e. is the same as the one generated in the login request). If the token is valid, the acquire endpoint should respond with entities and links as usual. If the above validation fails, the acquire endpoint should respond with a 401 and a ProblemDetails body. Additionally, if the connector service has a validate endpoint, the same restriction logic needs to be applied for it. Configuring authentication for an asynchronous service If you are not familiar with setting up and using asynchronous services on a connector, refer to the Async connector . Similarly to configuring authentication for a synchronous service , an authentication configuration needs be defined and assigned to the asynchronous service you want to configure authentication for in the connector configuration. To ensure requests to an asynchronous service are made by an authenticated user, all the query endpoints must validate the contents of the Authorization header, similarly to how an acquire endpoint is configured for authentication. This includes the: Async acquire endpoint Status endpoint Results endpoint This is because, even after launching the asynchronous query, it is possible for the token to expire or otherwise become invalidated while the query is running. In which case, the connector should respond with a 401 and ProblemDetails body to the gateway to indicate that the user needs to reauthenticate before the query can continue. In such events, the client would request for the user to re-input credentials before the running query resumes or a completed query displays results. The Delete endpoint can also validate the authentication token provided although this is not strictly necessary. Setup These instructions are for setting up and running the Auth connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the Auth connector: <wars> <war ... name=\"opal-services-daod\" ... > ... <connector-ids> <connector-id value=\"auth-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9086\" name=\"Auth Connector\" id=\"auth-connector\"/> </connectors> </ns1:topology> Ensure that you are using the same port as specified in application.properties ( 9086 by default) and that the value of the id attribute is the same as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the Auth schema as a connector schema or a gateway schema. Connector schema By default, your connector has the Auth schema configured as a connector schema. schemaUrl , chartingSchemesUrl , and schemaShortName are defined in the Auth connector's config.json file, in auth\\auth-connector\\src\\main\\resources . Additionally, ensure the following is true: The connector's config.json does not contain a gatewaySchema property. The Auth <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. For more information, see configuring a connector schema . Gateway schema If you want to set up the Auth schema as a gateway schema, follow the guidelines for configuring a gateway schema using the Auth schema and charting scheme in the schema directory of this repository. Additionally, ensure the following is true: The connector's config.json in auth\\auth-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default. If they are present, remove them. The Auth <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. Run the Auth connector To run the connector, navigate to connector\\auth\\auth-connector in your terminal and run the application using the following command: mvnw spring-boot:run 4. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-eri.html": {
    "href": "content/example-connectors/connector-eri.html",
    "title": "Emergency Response Incidents (ERI) Connector",
    "keywords": "Emergency Response Incidents (ERI) Connector The ERI connector connects to the Emergency Response Incidents Dataset and marshals the data into entities, links and properties. The dataset contains types and locations of incidents which the Office of Emergency Management of New York City have responded to. There are seven fields in the dataset: Column Name Type Incident Type Plain Text Location Plain Text Borough Plain Text Creation Date Date & Time Closed Date Date & Time Latitude Number Longitude Number Data model The ERI schema models the Emergency Response Incidents Dataset by using all seven fields. Each row of data can be represented by two entities (Incident and Location) and a single link between them (Located At) alongside properties extracted from each field. The schema (and charting schemes) for the ERI connector can be found in the schema directory of this repository. Entity: Incident Represents a reported incident. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Property Type Logical Type Derived From Incident Type SELECTED_FROM * Incident Type Incident Subtype SINGLE_LINE_STRING Incident Type Creation Date DATE Creation Date Creation Time TIME Creation Date Closed Date DATE Closed Date Closed Time TIME Closed Date * The possible values for Incident Type are: Administration, Aviation, Fire, HazMat, Law Enforcement, Marine, Medical, Rescue, Structural, Transportation, Utility, Weather and Other . Entity: Location Represents the location at which the incident occurred. Property Type Logical Type Derived From Borough SINGLE_LINE_STRING Borough Address SINGLE_LINE_STRING Location Coordinates GEOSPATIAL Longitude & Latitude Link Associates an incident with the location at which it is reported to have occurred. Link Type Link Ends Located At Incident -> Location Setup These instructions are for setting up and running the ERI connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology: In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the ERI connector: <wars> <war ... > ... <connector-ids> <connector-id value=\"eri-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9084\" name=\"ERI Connector\" id=\"eri-connector\"/> </connectors> </ns1:topology> Ensure that you're using the same port as specified in application.properties ( 9084 by default) and that the value of the id attribute is the name as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the ERI schema as a connector schema or a gateway schema. Connector schema By default, your connector has the ERI schema configured as a connector schema. schemaUrl , chartingSchemesUrl , and schemaShortName are defined in the ERI connector's config.json file, in eri\\eri-connector\\src\\main\\resources . Additionally, ensure the following: The connector's config.json does not contain a gatewaySchema property; The ERI <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. See for more information on configuring a connector schema . Gateway schema If you want to set up the ERI schema as a gateway schema, follow the guidelines for configuring a gateway schema using the ERI schema and charting scheme found in the schema directory of this repository. Additionally, ensure the following is true: The connector's config.json in eri\\eri-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default; if they are present, remove them. The ERI <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. Acquire Socrata token In order to query the external datasource, a Socrata app token is required. If you do not already have a Socrata app token, you will need to generate one. Instructions on how to generate this token can be found here . In the ERI connector's application.properties file at connector\\eri\\eri-connector\\src\\main\\resources , add your token. server.port=9084 socrata.url=https://data.cityofnewyork.us/resource/pasr-j7fb.json # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= 4. Run the ERI connector To run the connector, navigate to connector\\eri\\eri-connector in your terminal and run the application using the following command: mvnw spring-boot:run For more information about running this repository's Java connectors, see running example connectors in Java . 5. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-kcpd.html": {
    "href": "content/example-connectors/connector-kcpd.html",
    "title": "Kansas City Police Department (KCPD) Connector",
    "keywords": "Kansas City Police Department (KCPD) Connector The KCPD connector connects to the KCPD Crime 2020 Dataset as the external datasource and marshals the data into entities, links and properties. This dataset contains reports on the criminal incidents which the Kansas City Police Department of Missouri have recorded. There are a number of fields in the dataset: Column Name Type Report_No Plain Text Reported_Date Date & Time Reported_Time Plain Text From_Date Date & Time From_Time Plain Text To_Date Date & Time To_Time Plain Text Offense Plain Text IBRS Plain Text Description Plain Text Beat Plain Text Address Plain Text City Plain Text Zip Code Plain Text Rep_Dist Plain Text Area Plain Text DVFlag Plain Text Involvement Plain Text Race Plain Text Sex Plain Text Age Number Firearm Used Flag Checkbox Location Location Data model The KCPD schema models the KCPD Crime 2020 Dataset using its relevant fields. Each row of data can be represented by three entities (Report, Person and Location) and a number of appropriate links between them alongside properties extracted from each field. The schema (and charting schemes) for the KCPD connector can be found in the schema directory of this repository. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Entity: Report Represents a report about a crime. Property Type Logical Type Derived From Report Number SINGLE_LINE_STRING Report_No Report Date DATE Reported_Date From Date DATE From_Date To Date DATE To_Date From Time TIME From_Time To Time TIME To_Time Offense SINGLE_LINE_STRING Offense Offense Description SINGLE_LINE_STRING Description Domestic Violence BOOLEAN DVFlag Entity: Person Represents a person somehow involved in a reported crime. Property Type Logical Type Derived From Race SINGLE_LINE_STRING Race Sex SUGGESTED_FROM Sex Age SUGGESTED_FROM Age Entity: Location Represents a location at which a reported crime occurred. Property Type Logical Type Derived From City SINGLE_LINE_STRING City Address SINGLE_LINE_STRING Address Zip Code INTEGER Zip Code Coordinates GEOSPATIAL Location Links Establishes some connection between a Report, a Location and a Person. The KCPD Dataset's Involvement field is used to determine how a Person is linked to a Report. Link Type Link Ends Located At Report -> Location Suspect Of Person -> Report Victim Of Person -> Report Complicit In Person -> Report Arrested Person -> Report Charged Person -> Report Setup These instructions are for setting up and running the KCPD connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology: In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the KCPD connector: <wars> <war ... > ... <connector-ids> <connector-id value=\"kcpd-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9083\" name=\"KCPD Connector\" id=\"kcpd-connector\"/> </connectors> </ns1:topology> Ensure that you're using the same port as specified in application.properties ( 9083 by default) and that the value of the id attribute is the name as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the KCPD schema as a connector schema or a gateway schema. Connector schema By default, your connector has the KCPD schema configured as a connector schema. schemaUrl , chartingSchemesUrl , and schemaShortName are defined in the KCPD connector's config.json file, in kcpd\\kcpd-connector\\src\\main\\resources . Additionally, ensure the following: The connector's config.json does not contain a gatewaySchema property; The KCPD <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. See for more information on configuring a connector schema . Gateway schema If you want to set up the KCPD schema as a gateway schema, follow the guidelines for configuring a gateway schema using the KCPD schema and charting scheme found in the schema directory of this repository. Additionally, ensure the following: The connector's config.json in kcpd\\kcpd-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default; if they are present, remove them. The KCPD <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. Acquire Socrata token In order to query the external datasource, a Socrata app token is required. If you do not already have a Socrata app token, you will need to generate one. Instructions on how to generate this token can be found here . In the KCPD connector's application.properties file at connector\\kcpd\\kcpd-connector\\src\\main\\resources , add your token. server.port=9083 socrata.url=https://data.kcmo.org/resource/vsgj-uufz.json # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= 3. Run the KCPD connector To run the connector, navigate to connector\\kcpd\\kcpd-connector in your terminal and run the application using the following command: mvnw spring-boot:run For more information on running this repository's Java connectors, see running example connectors in Java . 4. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-nypd.html": {
    "href": "content/example-connectors/connector-nypd.html",
    "title": "New York Police Department (NYPD) Connector",
    "keywords": "New York Police Department (NYPD) Connector The NYPD connector connects to the NYPD Complaint Data and marshals the data into entities, links and properties. The dataset contains information on the felony, misdemeanor, and violation crimes reported to the New York City Police Department. There are a number of fields in the dataset, of which the following were used: Column Name Type CMPLNT_NUM (Complaint Number) Number ADDR_PCT_CD (Precinct Code) Number BORO_NM (Borough Name) Plain Text CMPLNT_FR_DT (Complaint From Date) Date & Time CMPLNT_FR_TM (Complaint From Time) Plain Text CMPLNT_TO_DT (Complaint To Date) Date & Time CMPLNT_TO_TM (Complaint To Time) Plain Text CRM_ATPT_CPTD_CD (Crime Atempted-Completed Code) Plain Text HADEVELOPT (Housing Development) Plain Text JURISDICTION_CODE (Jurisdiction Code) Number JURIS_DESC (Jurisdiction Description) Plain Text KY_CD (Key Code) Number LAW_CAT_CD (Law Category Code) Plain Text LOC_OF_OCCUR_DESC (Location of Occurence Description) Plain Text OFNS_DESC (Offense Description) Plain Text PARKS_NM (Park Name) Plain Text PATROL_BORO (Patrol Borough) Plain Text PD_CD (PD Code) Number PD_DESC (PD Description) Plain Text PREM_TYPE_DESC (Premise Type Description) Plain Text RPT_DT (Report Date) Date & Time STATION_NAME (Station Name) Plain Text SUSP_AGE_GROUP (Suspect's Age Group) Plain Text SUSP_RACE (Suspect's Race) Plain Text SUSP_SEX (Suspect's Sex) Plain Text TRANSIT_DISTRICT (Transit District) Number VIC_AGE_GROUP (Victim's Age Group) Plain Text VIC_RACE (Victim's Race) Plain Text VIC_SEX (Victim's Sex) Plain Text Latitude Number Longitude Number Data model The NYPD schema models the NYPD Complaint Dataset by using its relevant fields. Each row of data can be represented by three entities (Complaint, Person and Location) and a number of appropriate links between them alongside properties extracted from each field. The schema (and charting schemes) for the NYPD connector can be found in the schema directory of this repository. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Entity: Complaint Represents a crime complaint. Property Type Logical Type Derived From Complaint Number SINGLE_LINE_STRING CMPLNT_NUM Complaint Start Date DATE CMPLNT_FR_DT Complaint End Date DATE CMPLNT_TO_DT Complaint Start Time TIME CMPLNT_FR_TM Complaint End Time TIME CMPLNT_TO_TM Crime Status SUGGESTED_FROM CRM_ATPT_CPTD_CD Jurisdiction Code INTEGER JURISDICTION_CODE Jurisdiction Description SINGLE_LINE_STRING JURIS_DESC Offence Classification Code SINGLE_LINE_STRING KY_CD Level of Offence SUGGESTED_FROM LAW_CAT_CD Offence Description SINGLE_LINE_STRING OFNS_DESC Internal Classification Code INTEGER PD_CD Classification Description SINGLE_LINE_STRING PD_DESC Event Date DATE RPT_DT Location of Occurrence SUGGESTED_FROM LOC_OF_OCCUR_DESC Entity: Location Represents the location of a reported crime. Property Type Logical Type Derived From Precinct Code INTEGER ADDR_PCT_CD Borough Name SINGLE_LINE_STRING BORO_NM Housing Development SINGLE_LINE_STRING HADEVELOPT Park Name SINGLE_LINE_STRING PARKS_NM Patrol Borough SINGLE_LINE_STRING PATROL_BORO Premises Description SINGLE_LINE_STRING PREM_TYP_DESC Station Name SINGLE_LINE_STRING STATION_NAME Transit District INTEGER TRANSIT_DISTRICT Coordinates GEOSPATIAL Latitude & Longitude Entity: Person Represents a person somehow involved in a reported crime. Since each record of the NYPD Complaint Dataset has information on both victims and suspects, two Person entities are created from a single record. Property Type Logical Type Derived From Age Group SUGGESTED_FROM SUSP_AGE_GROUP or VIC_AGE_GROUP Race SINGLE_LINE_STRING SUSP_RACE or VIC_RACE Sex SUGGESTED_FROM SUSP_SEX or VIC_SEX Links Establishes some connection between a Complaint, a Location and a Person. Link Type Link Ends Located At Complaint -> Location Suspect Of Person -> Complaint Victim Of Person -> Complaint Setup These instructions are for setting up and running the NYPD connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway and have not previously done so, you must do so now. If you want to configure your connector to use an Information Store schema, you must deploy i2 Analyze with the Information Store and the i2 Connect gateway . 1. Add connector to topology: In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the NYPD connector: <wars> <war ... > ... <connector-ids> <connector-id value=\"nypd-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9081\" name=\"NYPD Connector\" id=\"nypd-connector\"/> </connectors> </ns1:topology> Ensure that you're using the same port as specified in application.properties ( 9081 by default) and that the value of the id attribute is the name as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the NYPD schema as a connector schema, a gateway schema, or an Information Store schema. Connector schema By default, your connector has the NYPD schema configured as a connector schema. schemaUrl , chartingSchemesUrl , and schemaShortName are defined in the NYPD connector's config.json file, in nypd\\nypd-connector\\src\\main\\resources . Additionally, ensure the following: The connector's config.json does not contain a gatewaySchema property; The NYPD <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. See for more information on configuring a connector schema . Gateway schema If you want to set up the NYPD schema as a gateway schema, follow the guidelines for configuring a gateway schema using the NYPD schema and charting scheme found in the schema directory of this repository. Additionally, ensure the following: The connector's config.json in nypd\\nypd-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default; if they are present, remove them. The NYPD <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. Information Store schema If you want to set up the NYPD schema as an Information Store schema: Copy the NYPD schema and charting scheme found in the schema directory of this repository to the configuration/fragments/common/WEB-INF/classes directory in your i2 Analyze configuration. Open the ApolloServerSettingsMandatory.properties file in the same directory, and update the following properties to point to the relevant files SchemaResource=nypd-complaint-data-schema.xml ChartingSchemesResource=nypd-complaint-data-schema-charting-schemes.xml Additionally, ensure the following: The connector's config.json in nypd\\nypd-connector\\src\\main\\resources does not contain the following properties that exist by default; if they are present, remove them: schemaUrl chartingSchemesUrl schemaShortName gatewaySchema The NYPD <connector> element in your i2 Analyze topology does not contain the following attributes: schema-short-name gateway-schema 3. Acquire Socrata token In order to query the external datasource, a Socrata app token is required. If you do not already have a Socrata app token, you will need to generate one. Instructions on how to generate this token can be found here . In the NYPD connector's application.properties file at connector\\nypd\\nypd-connector\\src\\main\\resources , add your token. server.port=9081 socrata.url=https://data.cityofnewyork.us/resource/7x9x-zpz6.json # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= 3. Run the NYPD connector To run the connector, navigate to connector\\nypd\\nypd-connector in your terminal and run the application using the following command: mvnw spring-boot:run For more information on running this repository's Java connectors, see running example connectors in Java . 4. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/run-java.html": {
    "href": "content/example-connectors/run-java.html",
    "title": "Running a Java connector via the command-line",
    "keywords": "Running a Java connector via the command-line The example Java connector is a simple Spring Boot application. Here's how to run it via the command-line. (You also have the option to run it directly in VSCode.) In the project directory (where the mvnw file is), use the commands below to get your connector up-and-running. There are two ways to go about this. We recommend using the run command, for the reasons detailed below. Run Use the following command to start the connector and run it until you press Ctrl-C . With the Spring Boot Devtools plugin, which we are using in this project, code and resource changes are detected automatically while using this command, and the application is automatically restarted. This is incredibly useful when making changes, since you can see the results almost instantaneously. mvnw spring-boot:run Start and stop To start the connector application and return to the command prompt, use: mvnw spring-boot:start You will then need to stop the connector application manually using mvnw spring-boot:stop Running the connector in VS Code To run your connector directly from VS Code, ensure you have installed the Spring Boot Extension Pack for VSCode, either via the Extensions menu in VSCode, or using this link Then, with your project directory open in VS Code: There should be a Spring Boot Dashboard panel at the bottom, where the \"demo\" app should appear. Right-click on the 'demo' app and click Run. The resulting output in the debug console should print the URL for the connector The app automatically restarts whenever files are updated, which helps speed up the development/test cycle. The screenshot below shows the Spring Boot Dashboard panel with the \"demo\" app appearing where you should expect to see it."
  },
  "content/example-connectors/run-python.html": {
    "href": "content/example-connectors/run-python.html",
    "title": "Running the Python connector",
    "keywords": "Running the Python connector The example Python connector is a simple Flask application built on Python 3.7.4. It is imperative that you have Python installed and added to PATH . Older versions of Python may not allow the application to work as expected. Installation In the project directory (where the Pipfile is located), run setup on Windows or ./setup.sh on Linux to execute the setup script. This will create the virtual environment and install the required dependencies for the working project directory. Starting and stopping the application To start the connector application, run the following command in the working directory: pipenv run python app.py Simply press Ctrl+C to stop the application."
  },
  "content/example-connectors/user-configuration.html": {
    "href": "content/example-connectors/user-configuration.html",
    "title": "User-specific configuration",
    "keywords": "User-specific configuration Introduced at version 4.3.5 of i2 Analyze, user-specific configuration is an advanced feature of i2 Connect that enables a connector to provide different configurations that are customized to specific users or groups of users. With the exception of the schema and the charting scheme (which remain global for a connector), you can customize all elements of connector configuration dynamically. A connector can provide user-specific configuration through a separate /USER_CONFIGURATION_URL endpoint, the URL of which you can provide in your response from the standard /CONFIGURATION_URL endpoint. When an endpoint (excluding the /CONFIGURATION_URL endpoint) is called on behalf of an authenticated user, the i2 Connect gateway provides headers that you can use to identify the user making the request: Header Description Type Example value (when decoded) I2-Principal The user's principal name String cn=user,dn=example I2-Display-Name The user's display name String User Name I2-User-Groups The user's group membership Base64-encoded JSON array [\"Analyst\",\"Clerk\"] I2-Security-Permissions The user's security permissions Base64-encoded JSON array [{\"dimension\":\"SD-SL\",\"permissions\":[{\"dimensionValue\":\"CON\",\"level\":\"UPDATE\"},{\"dimensionValue\":\"UC\",\"level\":\"UPDATE\"}]}] I2-Command-Access-Permissions The user's command access control permissions Base64-encoded JSON array [\"i2:Connectors\",\"i2:Notes\"] Important: At version 4.3.5 of i2 Analyze, only the /USER_CONFIGURATION_URL endpoint received these headers. Since version 4.3.5.3, all endpoints except /CONFIGURATION_URL receive them. Creating user-specific configurations To implement user-specific configurations for your connector, you must first enable and then provide them. Enable user-specific configurations To enable user-specific configurations, you set the userConfigUrl field of a connector's configuration. For example: { \"userConfigUrl\": \"/userconfig\" } Here, the userConfigUrl field defines the (relative) URL at which the user specific configuration can be found. When userConfigUrl is present, the only other fields that can appear in the connector's configuration are: schemaUrl chartingSchemesUrl schemaShortName gatewaySchema Important: You must also instruct the i2 Connect gateway to send the above headers in requests to the connector. The headers can contain sensitive information, so the gateway does not send them by default. To add the headers to requests that reach the connector, you must add the send-sensitive-headers attribute to the entry for the connector in the topology file: <connectors> <connector id=\"...\" name=\"...\" base-url=\"...\" configuration-url=\"...\" send-sensitive-headers=\"true\"/> </connectors> When the send-sensitive-headers attribute is absent, i2 Analyze behaves as if its value is false . Provide user-specific configurations The endpoint at the URL specified in the userConfigUrl field must return a configuration response similar to the one that the standard configuration endpoint returns. The user-specific configuration response supports the following fields: defaultValues services clientConfigs authConfigs When a user connects to the i2 Connect gateway and a connector with user-specific configuration is present, the gateway checks whether it has a configuration cached for that user. If it does not, it calls the userConfigUrl endpoint with the headers that are listed above. The connector can use the headers to determine which configuration to provide for the requesting user. In the example connector, the user-specific configuration is simply provided from two different static files with hard-coded settings. In practice, you can create the configurations in any way that meets your needs, including by dynamically generating them in code. Note: In the current version of i2 Analyze, the user configuration endpoint always receives all the custom HTTP headers. It is good practice, however, to treat them as optional and handle their absence gracefully. Setting up an example connector The instructions in the second half of this topic describe how to extend the Java version of the NYPD example connector to support user-specific configurations. Ensure that you have a working deployment of the Java version of the NYPD connector before you continue: If you already followed the NYPD connector example using Java, you can use it as your starting point and build on it. Otherwise, you can start with the sample solution ( sample-solution/nypd-connector ) and follow the deployment instructions . 1. Add the user-specific example override This example works by overriding some aspects of the NYPD example connector. Make a copy of the sample solution (or your existing NYPD connector example, if you are using it). Copy the contents of the user-config/override directory over your copied solution. The differences between the override and the sample solution are: ConnectorController.java is modified to return global-config.json as the standard configuration, and to return analyst-config.json and other-config.json for the new, user-specific configurations. The existing config.json file is no longer used. 2. Start the connector To start the modified connector, open a command prompt, navigate to the location that you used in the previous step, and run the application: mvnw spring-boot:run 3. Configure group membership To activate the modified connector, ensure that your user.registry.xml file contains at least two users. One must be a member of the Analyst group but not the Clerk group, while the other must have the opposite settings. You can find the file at i2analyze\\deploy\\wlp\\usr\\shared\\config , and a minimal example looks like this: <user name=\"analyst-user\" password=\"password\"/> <user name=\"clerk-user\" password=\"password\"/> <group name=\"Analyst\"> <member name=\"analyst-user\"/> </group> <group name=\"Clerk\"> <member name=\"clerk-user\"/> </group> <group name=\"Controlled\"> <member name=\"analyst-user\"/> <member name=\"clerk-user\"/> </group> Note: The example assumes that you are using the example security schema. Users must also have access to at least one value from each security dimension in the security schema in order to access the system. 4. Configure command access control The group names in the command access control file must match the names of user groups in the user registry file. To ensure that users in the Clerk group are able to view the connector, you must update the command access control file. Navigate to toolkit\\configuration\\fragments\\opal-services\\WEB-INF\\classes and open the example-command-access-control.xml file in a text editor. Add the user group specified in the user registry file to the command access control file, and include the permission for the connector. For example: <CommandAccessPermissions UserGroup=\"Clerk\"> <Permission Value=\"i2:Connectors:nypd-connector\" /> </CommandAccessPermissions> 5. Deploy and start i2 Analyze Next, deploy and start the Liberty server that hosts i2 Analyze: setup -t deployLiberty setup -t startLiberty 6. Test the connector To test the connector, you must log in with different credentials that you expect to receive different configurations. Using Analyst's Notebook Premium, log in as a user that is not an Analyst and open the External Searches tool. Check that you only have access to the NYPD Connector: Search service, and that the Law Category drop-down does not contain FELONY . Log out and log back in as a user that is an Analyst , and open the External Searches tool again. Check that the list now includes the other connector services (for example, NYPD Connector: Expand ), and that the options in the Law Category drop-down include FELONY ."
  },
  "content/miscellaneous/data-model.html": {
    "href": "content/miscellaneous/data-model.html",
    "title": "Property value data formats",
    "keywords": "Property value data formats All the property types defined in a schema specify a logical type that controls the kind of data that properties of that type contain. For example, some properties contain numerical data while others contain strings. Furthermore, of the properties that contain numerical data, some contain integers while others contain floating-point values. To see the logical type of a particular property type, open the schema in i2 Analyze Schema Designer and select it in the left-hand pane. You'll see its logical type along as well as its identifier, name, and description. When you specify default values during connector configuration, or you return entities and links to i2 Analyze from a connector, you must ensure that the property values you assign align with the logical types. The following descriptions provide examples of data that is valid for each of the supported logical types. SINGLE_LINE_STRING Single-line strings can be up to 250 bytes long, and can be further limited by the schema. \"This is a single-line string.\" MULTIPLE_LINE_STRING Multiple-line strings can be up to 32K bytes long, and can be further limited by the schema. \"This is a multiple-line string.\" DATE Date values must be in the ISO 6801 format \"YYYY-MM-DD\". Also, values must be in the range from 1753-01-01 to 9999-12-30. \"2021-11-30\" TIME Time values must be in the ISO 6801 format \"hh:mm:ss\". Seconds are optional, but hours and minutes are not. Greater precision than seconds is not supported. \"23:59:59\" DATE_AND_TIME Four pieces of information are required to specify a particular point in time: The date The time as it would appear on a clock The time zone in which the point in time is observed If the point in time occurs during the hour that is repeated when clocks are turned back to end Daylight Saving Time, whether to use the first or second occurence of this time. There are two ways to return values with the DATE_AND_TIME logical type. You can use an ISO 8601 string without a time zone; or a date-and-time JSON object that explicitly defines the time zone. When you use the ISO 8601 format, the value must be specified as a string with at least minute-precision and up to millisecond-precision. The time zone is taken from the timeZoneId that's defined in defaultValues in the connector configuration. (If no default is present, the time zone is set to UTC.) \"2021-12-30T23:59:59\" or \"2021-12-30T23:59:59.999\" Alternatively, a JSON object must contain three fields: localDateAndTime is a string that conforms to ISO 8601 as above, again with at least minute-precision and up to millisecond-precision. timeZoneId specifies the time zone in which the moment of time is observed. For a list of valid time zone identifiers, see the IANA Time Zone Database. isDST specifies which occurrence of the repeated hour to use if the point in time occurs when the clocks turn back at the end of Daylight Saving Time. To use the first, set it to true ; to use the second, set it to false . { \"localDateAndTime\": \"2021-12-30T23:59:59.999\", \"timeZoneId\": \"Europe/London\", \"isDST\": false } BOOLEAN Boolean values must be be either true or false . true or false INTEGER Integer values must be in the range from -2147483648 to 2147483647. 2147483647 DOUBLE Double values must be in the range from 4.94065645841246544e-324d to 1.79769313486231570e+308d. 4.94065645841246544e-324d DECIMAL Decimal values are strings that contain up to 18 digits before the decimal separator, and up to 4 digits after it. There can be a leading minus sign, but no exponent (e) notation. \"-123456789012345678.1234\" SELECTED_FROM Selected-from string values must match a permitted value that the schema or a form condition defines. \"This is a selected-from string.\" SUGGESTED_FROM Suggested-from string values must match a permitted value that a form condition defines when you use them as default values, but are otherwise unrestricted. \"This is a suggested-from string.\" GEOSPATIAL Geospatial values must be formatted as GeoJSON points, as described at https://datatracker.ietf.org/doc/html/rfc7946#section-3.1.2 . The first element in the coordinates array is longitude, and must be a decimal between -180.0 and 180.0. The second element is latitude and must be a decimal between -90.0 and 90.0. { \"type\": \"Point\", \"coordinates\": [1.0, 2.0] } GEOSPATIAL_AREA Important: Values of type GEOSPATIAL_AREA are valid for use in connector configuration, but not in the data that you return from a connector. Geospatial area values must be formatted as GeoJSON feature collections, as described at https://datatracker.ietf.org/doc/html/rfc7946#section-3.3 . Foreign members are allowed. The geometry must be of type Polygon or MultiPolygon , both of which must contain at least four coordinates, where the first coordinate matches the last. { \"type\": \"FeatureCollection\", \"features\": [ { \"type\": \"Feature\", \"geometry\": { \"type\": \"MultiPolygon\", \"coordinates\": [ [ [ [1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [1.0, 2.0] ] ] ] } }, { \"type\": \"Feature\", \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [1.0, 2.0] ] ] } } ] }"
  },
  "content/miscellaneous/deploy-i2-analyze-is-daod.html": {
    "href": "content/miscellaneous/deploy-i2-analyze-is-daod.html",
    "title": "Deploying i2 Analyze with the Information Store and the i2 Connect gateway",
    "keywords": "Deploying i2 Analyze with the Information Store and the i2 Connect gateway This guide walks you through how to deploy i2 Analyze with the Information Store and the i2 Connect gateway. To deploy i2 Analyze with the i2 Connect gateway, see this guide . Prerequisites Before you start, ensure that you have: installed i2 Analyze, installed either IBM Db2 or Microsoft SQL Server , and defined a schema to model the data. License acknowledgement Open the license_acknowledgement file in your i2 Analyze directory. Set the value to ACCEPT . It should now look like this: LIC_AGREEMENT = ACCEPT Configuration Create the configuration directory In your i2 Analyze directory, navigate to toolkit\\examples\\configurations\\information-store-daod-opal . Copy the configuration directory to the toolkit directory. This provides a starting point for a deployment that includes the Information Store and support for the i2 Connect gateway. Database management If you are using IBM Db2 as your chosen database management system, you can skip the following steps, otherwise: Download the Microsoft JDBC Driver 7.4 for SQL Server archive. Extract the contents of the download and navigate to sqljdbc_7.4\\enu . Copy the mssql-jdbc-7.4.1.jre8.jar file to the i2 Analyze toolkit\\configuration\\environment\\common\\jdbc-drivers directory. Copy the topology.xml file for SQL Server from the toolkit\\configuration\\examples\\topology\\sqlserver to the toolkit\\configuration\\environment directory, overwriting the existing topology.xml file in the destination directory. Specify the credentials for deployment Using a text editor, open the toolkit\\configuration\\environment\\credentials.properties file. Specify a user name and password to use for the database in the db.infostore.user-name and db.infostore.password properties. Specify a user name and password to use for the Solr indexes in the solr.user-name and solr.password properties. Enter the password to encrypt LTPA tokens in the ltpakeys.password property. Save and close the file. Command access control To gain access to certain features, including the ability to use Postman to reload the connector when making schema changes, you need to copy and modify some files in the i2 Analyze deployment: Navigate to the toolkit\\configuration\\examples\\security-schema directory and copy the file named example-command-access-control.xml . Navigate to the toolkit\\configuration\\fragments\\opal-services\\WEB-INF\\classes directory and paste the file from the previous step. Open the DiscoServerSettingsCommon.properties file and add the name of file you just copied to the CommandAccessControlResource field, including the .xml extension. Save and close the properties file. Configure the schema To test your schema and use it in Analyst's Notebook before deploying a connector, you can configure it as an Information Store schema. The steps below describe this process. Copy your schema and charting schemes to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to your schema files by setting the following properties: SchemaResource=schema-filename.xml ChartingSchemesResource=charting-schemes-filename.xml Configure the security schema All i2 Analyze deployments require a security schema, which defines the level of access users have to the data in the system. You can learn about the i2 Analyze security model in the product documentation but, for the purpose of this guide, follow the steps below to use an example security schema. Copy example-dynamic-security-schema.xml from toolkit\\configuration\\examples\\security-schema to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to the security schema by setting the following property: DynamicSecuritySchemaResource=example-dynamic-security-schema.xml Generate the default configuration For the purposes of this guide, only a basic configuration is required, so you can use the default. Open the toolkit\\scripts directory in a command prompt. To populate some of the mandatory settings with default values, run: setup -t generateDefaults In the i2analyze directory, navigate to the toolkit\\configuration\\environment directory and open the topology.xml file in a text editor. Replace every instance of the host-name tag with the value localhost . There will be four to change in total. This will be used to test your application in Postman. Deployment Open the toolkit\\scripts directory in a command prompt. To deploy i2 Analyze with the configuration you have just created, run setup -t deploy To add an example user whose name and password are both \"Jenny\", run: setup -t ensureExampleUserRegistry To start i2 Analyze, run setup -t start You can now connect Analyst's Notebook Premium to i2 Analyze. The output of the start command includes the URL to use for the i2 Analyze server. This will be the best time to test the schema you created in the previous step, or the existing one if you are using the example schema provided. You can open Analyst's Notebook and start to drag/drop entities and links onto your chart, testing different charting schemes and seeing what labels appear and how they are formatted. For these changes to take effect, you need to update the i2 Analyze connectors configuration and run the internal Reload command via Postman. First, navigate to i2analyze\\toolkit\\scripts in your console and run: setup -t updateConnectorsConfiguration Next you need to open Postman , select the i2 Analyze with InfoStore and Gateway environment and run the Form Based Login request. This will authenticate you as the default user in i2 Analyze, then you can run the Reload request which will configure your schema changes to the topology. (NOTE: You will need to perform a reload command for every iteration of your schema design, otherwise the changes will not take effect). You now have a running i2 Analyze deployment, if you are happy with your schema you can now deploy a connector . However, if there are changes you wish to make to the schema and associated charting schemes, you can run yourself through the schema design guide again."
  },
  "content/miscellaneous/i2-connect-architecture.html": {
    "href": "content/miscellaneous/i2-connect-architecture.html",
    "title": "System architecture with the i2 Connect gateway",
    "keywords": "System architecture with the i2 Connect gateway The simplest possible deployment of i2 Analyze that uses connectors to access an external data source has four parts. The client, in this case Analyst's Notebook Premium, communicates with a deployment of i2 Analyze that includes the i2 Connect gateway. The gateway manages at least one connector, which in turn exchanges data with an external source. System interactions The following diagram represents the deployment that is described in the introduction, and the interactions that take place between each part. The diagram also identifies five of the REST endpoints that connectors can implement and use: Configuration endpoint The gateway sends a request to the mandatory configuration endpoint to gather information about the services that the connector supports. All connectors have at least one service, and it is a service that implements the validate and acquire endpoints. Note: For connectors whose services present modified behavior to different users, the information is split between two endpoints: this one, and the user-specific configuration endpoint. Schema endpoint The schema endpoint is optional, and so is where you implement it. If a connector returns results whose types are not in the Information Store schema or a gateway schema, you can provide a connector schema that define those types. The configuration specifies whether and where a connector schema is available. Charting scheme endpoint The charting scheme endpoint is optional, and closely associated with the schema endpoint. A connector configuration that specifies a schema endpoint also specifies a charting scheme endpoint. Validate endpoint The validate endpoint is optional. If the configuration endpoint says that a particular service supports it, then the gateway sends a request to the validation endpoint immediately before a request to the acquire endpoint. The purpose of the validate endpoint is to test whether a request is reasonable before it is run. Acquire endpoint The acquire endpoint is mandatory for all services that provide synchronous querying. During a synchronous query, the gateway sends a request to the acquire endpoint that tells the service to query the external source. The service receives data back from the source and places it into the response that it returns to the gateway. Note: For services that provide asynchronous queries, the query resource endpoint is mandatory instead of the acquire endpoint. Interaction sequence The interaction between the i2 Connect gateway and the other parts of an i2 Analyze deployment takes place in three distinct phases, labeled 1, 2, and 3 in the diagram. When i2 Analyze starts up, the i2 Connect gateway sends a request to the configuration endpoint of every connector that is listed in the topology. It caches the information that it receives in response. If the configuration for any of the connectors specifies schema and charting scheme endpoints, the gateway sends requests to retrieve the connector schemas and charting schemes from those endpoints. Note: You can also force the gateway to repeat this process while i2 Analyze is running. The gateway implements the reload endpoint, whose POST method you can call after you change the configuration of a connector. When a client connects to i2 Analyze, the latter gets the cached information from the gateway and returns it to the client. The client can then present the queries that the services implement to users, and help users to provide valid parameters and seeds to those queries where appropriate. Note: For connectors that implement the user-specific configuration endpoint, i2 Analyze sends a request to it with information about the connecting user at this stage, to retrieve the remainder of the configuration information. When a user runs a synchronous query, i2 Analyze passes it to the gateway for processing. The gateway packages the query into a request to the acquire endpoint on the service in question, preceded by a request to the validate endpoint if the service supports it. On receiving the response from the acquire endpoint, the gateway converts the data that it contains into results that i2 Analyze can ultimately return to the client."
  },
  "content/miscellaneous/postman.html": {
    "href": "content/miscellaneous/postman.html",
    "title": "Postman",
    "keywords": "Postman You can use Postman collections to test the endpoints of i2 Analyze and your connector. Prerequisites Install the latest version of Postman Import the Postman environments from the postman/environments directory Import the Postman collections from the postman/collections directory To import the environments and collections: Click Import , or click File -> Import In the Import Folder tab, click Choose Folders Select the postman directory that you downloaded from this repository The Postman collections are displayed in the left pane and a populated dropdown list of environments near the top right of the window. Testing i2 Analyze To make sure that i2 Analyze is configured correctly, authenticate a user and test the API. Authentication Click on the environment dropdown menu at the top-right bar: If you have deployed i2 Analyze with the i2 Connect gateway , select the i2 Analyze with i2 Connect Gateway environment. If you have deployed i2 Analyze with the Information Store and the i2 Connect gateway , select the i2 Analyze with InfoStore and Gateway environment. Open the i2 Analyze collection. Open the Authentication folder. Run the Form Based Login request. This authenticates your Jenny user, and generates the session token to permit subsequent API requests. API After generating the token, test that you can use the i2 Analyze endpoints. Get all valid time zones In the i2 Analyze collection, open the Core folder. Run the Timezones request. You should see that i2 Analyze returns all valid time zones. Reload connectors configuration In the i2 Analyze collection, open the Gateway folder. If your connector has a service that defines semantic seed constraints, add the SEMANTIC_SEARCH capability to the request's query parameter. If the value is not present, information about that service is excluded from the response. Run the Reload request. You should see that i2 Analyze returns its connectors configuration. Reload live configuration Ensure you are authenticated as the Administrator user (Jenny). In the i2 Analyze collection, open the Admin folder. Run the Reload request. A 200 OK response indicates that the live configuration was loaded successfully. Testing a connector Before you test your connector, ensure that both i2 Analyze and your connector are running. In this example, the NYPD connector is tested. For more information, see Setting up and running the NYPD connector . From the environment dropdown menu, select the NYPD Connector environment. Open the Connector Services collection. Config and schema requests In the Connector Services collection, open the Config folder. Run the Config request. The response provided should be the full contents of the connector's config.json . The Schema and Charting Schemes requests should echo the contents of your connector schema and charting schemes XML files respectively. Acquire requests The Acquire folder contains requests that respond with entities and links according to how their respective endpoints were implemented. Using the NYPD connector example: All The All request synchronously returns all entities and links from the NYPD dataset as a JSON response. Search The Search request is a parameterized search accepts a JSON payload of specified conditions which filter the results returned. The values of each condition can be changed to imitate user input from the client for a condition field. Find Like This The Find Like This request is a seeded search that accepts a JSON payload of a single seed entity with a property that is used to filter the JSON response by matching against entities with a similar property. Expand The Expand request is a seeded search that accepts a JSON payload of a single seed entity used as a starting point to find other entities connected to it and the links that connect them. These entities and links are returned in the response in JSON format. Validate requests The Validate folder contains a single Search request that performs server-side validation on the payload of specified conditions to ensure input values are in the correct format. Async requests The Async folder contains requests for the asynchronous service. These requests only function as expected on the Async connector. For more information, see setting up and running the Async connector Acquire The Acquire request triggers an asynchronous query and returns a queryId . Using Postman tests (post-request logic), this queryId is automatically stored as an environment variable ( queryId ) to facilitate subsequent async requests. The request accepts a payload of parameters for simulating the asynchronous request; configuring the duration of time before succeeding and optionally mocking a failure. Status Using the queryId retrieved from the previous request, the Status retrieves the current status and additional information of the triggered asynchronous query as a JSON response. Results Using the same queryId from the async acquire, the Results retrieves the JSON response of entities and links from the asynchronous query as you would expect from a synchronous request. This request only works as expected after the status of the query is SUCCEEDED . Delete / Cancel Using the queryId , the Delete / Cancel request deletes an asynchronous query if its state is SUCCEEDED or FAILED . It cancels the running job if it was STARTED , causing it to be FAILED . Auth requests The Auth folder contains requests for the services which require authentication. These requests only function as expected on the Auth connector. For more information, see setting up and running the Auth connector Login There are two Login requests for the two example authentication configurations specified in the connector configuration of the Auth connector. One logs in using sample username and password credentials; the other logs in using a sample API key. The post-request logic stores the authToken which the connector responds with on a successful log-in. This authToken is used for subsequent requests. Acquire & Acquire Async These requests are just like the synchronous acquire and asynchronous acquire requests except that they include the generated authToken from the login requests in the Authorization header of their requests."
  },
  "content/miscellaneous/principal-propagation.html": {
    "href": "content/miscellaneous/principal-propagation.html",
    "title": "Custom HTTP headers",
    "keywords": "Custom HTTP headers Starting at version 4.3.3, i2 Analyze contains an extension point that enables you to return additional headers to any connectors from the i2 Connect gateway. The extension point requires you to write an implementation of IConnectorRequestModifier.java . The implementation must be packaged into a JAR file and deployed to i2 Analyze. The following example demonstrates how to use Maven to do this and how to configure i2 Analyze to use your implementation. Overview For i2 Analyze to use your implementation, complete the following tasks: Create and package your implementation of IConnectorRequestModifier.java Configure i2 Analyze to use your packaged implementation Redeploy and restart Liberty to update your deployment Prerequisites Java installed The principal propagation project Update the value of the <toolkitLocation> element in the principal-propagation/pom.xml file to reference the installation location of i2 Analyze. For example: <toolkitLocation>C:\\i2\\i2analyze</toolkitLocation> This location is used by Maven to locate the JAR files to pull into the local Maven repository and where to place the packaged JAR file that contains your implementation. To set up the Maven environment, open a terminal in the principal-propagation directory. 1. Initialize the Maven dependencies: mvnw initialize This pulls in JAR files from the i2 Analyze deployment toolkit into your local Maven repository. 2. Compile and install the project: mvnw clean install At this point, you could start editing the implementation in principal-propagation/src/main/java/com/i2group/example/PrincipalPropagation.java . For the example implementation, do not make any changes. Deploy the principal propagation implementation When the implementation is complete, package the class into a JAR file. Open a terminal in the principal-propagation directory and run the following command: mvnw package The Maven package command creates a JAR file from the PrincipalPropagation.java class and puts it in the toolkit/configuration/fragments/opal-services/WEB-INF/lib/ directory of the i2 Analyze toolkit specified in the <toolkitLocation> element. Configuring i2 Analyze For i2 Analyze to use the PrincipalPropagation.jar , it must be specified in the DiscoServerSettingsCommon.properties file. Add the following line to the end of the toolkit/configuration/fragments/opal-services/WEB-INF/classes/DiscoServerSettingsCommon.properties file: ConnectorRequestModifier=com.i2group.example.PrincipalPropagation Redeploy and restart Liberty setup -t stopLiberty setup -t deployLiberty setup -t startLiberty Any connectors receive an extra header named i2ExtensionHeader in responses from the i2 Connect gateway. For example: Host: localhost:9084, Connection: keep-alive, Content-Length: 92, User-Agent: Apache-HttpClient/4.5.6 (Java/1.8.0_232) Content-Type: application/json i2ExtensionHeader: Jenny Accept-Encoding: gzip,deflate Accept-Language: en-gb"
  },
  "content/miscellaneous/property-semantic-search.html": {
    "href": "content/miscellaneous/property-semantic-search.html",
    "title": "Semantic seeds",
    "keywords": "Semantic seeds The connector development walkthrough describes how to create a service that accepts seed records to drive the searches that it performs. In that example, the records that the service accepts are constrained by their item types. Starting from version 4.4.1 of i2 Analyze, you can specify semantic constraints instead of item type constraints, so that: A service can accept seed records of any item type, provided that they have properties with particular semantic types (or descendants of those types). Where a seed record has more than one property with the same (or a related) semantic type, you can base a search on the values of all such properties. This topic describes how to specify semantic constraints. For more information about the uses to which you can put seeded searches, see Supporting seeds . Example This example is based on the auth-connector example, but adds a service that uses a semantic seed constraint. The service uses the SPI to examine the seed that it receives in the request, and returns a response containing entities and links whose property values match the values in that seed. Add a new service The new service will query an external data source (just a text file in this example) for person records that have the same last name as the seed in the request. By using a semantic constraint, the service will accept a seed of any type that has a property with the \"Person Last Name\" semantic type. In addition, since the \"Maiden Name\" semantic type is a child of \"Person Last Name\", the service will also accept a seed with a property of that type. In the config.json file that defines the connector, you will need to add the new service to the services array. You will also need to add seedConstraints to define a seeded search, and within it semanticPropertyTypes to define the semantic types of properties that records must have in order to be used as seeds. The following code specifies a seed constraint that uses the \"Person Last Name\" semantic type. (The GUID guid62B01C18-2E64-46D5-B2FF-3C69B4F76FEB appears as the identifier of that semantic type in the i2 Semantic Type Library.) { \"services\": [ { \"id\": \"find-same-last-name\", \"name\": \"Auth Connector: Find Same Last Name\", \"description\": \"A service that finds records with the same last name as the seed.\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/find-same-last-name\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"semanticPropertyTypes\": { \"Name constraints\": [ \"guid62B01C18-2E64-46D5-B2FF-3C69B4F76FEB\" ] } } } } ] } Note: When you write your own services with semantic type constraints, you'll need to find the GUIDs of the semantic property types that you want to use. See Semantic type identifiers for more information. Implement the service endpoint The service will work by examining the property values of a selected record according to the defined constraints, and searching for data in the external source that has matching values in its properties. In code, you can access the semantic properties of the seed record in the request through the request.payload.seeds.semanticData object. The service that you added to config.json specifies an endpoint named /find-same-last-name that you can implement like this, in ConnectorController.java : @RequestMapping(method = RequestMethod.POST, value = \"/find-same-last-name\", consumes = APPLICATION_JSON_VALUE, produces = APPLICATION_JSON_VALUE) public ResponseEntity<?> findSameLastName( @Valid @RequestBody ConnectorRequest request) { return connectorDataService.findSameName(request.payload.seeds); } Access the seeds The findSameName() method needs to examine the seeds passed into the request according to the SPI , and return a response that contains entity and link records. The DaodRequest object that the service receives for a search that uses semantic seeds might look like the following example. { \"payload\": { \"seeds\": { \"entities\": [ { \"accessDimensionValues\": [], \"extensions\": [], \"label\": \"\", \"properties\": { \"PER1\": \"Sally\", \"PER2\": \"Armstrong\", \"PER6\": \"Hendricks\" }, \"seedId\": \"123\", \"sourceIds\": [], \"typeId\": \"Person\" } ], \"links\": [], \"semanticData\": { \"Name constraints\": [ { \"properties\": { \"guid62B01C18-2E64-46D5-B2FF-3C69B4F76FEB\": [ { \"logicalType\": \"SINGLE_LINE_STRING\", \"value\": \"Armstrong\" }, { \"logicalType\": \"SINGLE_LINE_STRING\", \"value\": \"Hendricks\", \"sourceSemanticTypeId\": \"guid699AD2D5-B30B-4508-9349-0E1AC1FF876C\" } ], \"seedId\": \"123\", \"isLink\": false } } ] } } } } In this example, a single seed with the identifier 123 was selected on the chart. That seed had values for the Last Name property (PER2) and the Maiden Name property (PER6). In the semantic data, the entry with the value Hendricks has a sourceSemanticTypeId value, which means that the containing property does not have the \"Person Last Name\" semantic type, but rather a descendant of that type - in this case, \"Maiden Name\". Find data based on the seed As with any service, how you use the seed data depends on what you want to achieve. To perform a search using this seed data, we need to filter the list of records in people.json according to whether they have values of \"Hendricks\" or \"Armstrong\" for any properties whose meaning is the same as \"last name\". In this case, the records in people.json have a property called surname , which we can compare with the contents of the seed data. Add this implementation of the findSameName() method to the ExternalConnectorDataService.java file: public ResponseEntity<?> findSameName(DaodSeeds seed) { final Set<Object> nameValues = new HashSet<>(); final List<SemanticSeed> nameConstraintSemanticProperties = seed.semanticData.get(\"Name constraints\"); for (SemanticSeed semanticData : nameConstraintSemanticProperties) { for (Set<SemanticProperty> constraintSemanticProperties : semanticData.properties.values()) { nameValues.addAll(constraintSemanticProperties.stream().map(constraint -> constraint.value).collect(Collectors.toList())); } } final List<Person> people = retrievePeopleData(); // Remove any people that do not match the name constraint values people.removeIf(p -> !nameValues.contains(p.surname)); final ConnectorResponse acquireResponse = marshalItemsFromResponse(people); return ResponseEntity.status(200).body(acquireResponse); } Only one of the people in people.json has the surname \"Armstrong\" or \"Hendricks\", and so for this example the response should just contain the following data: { \"entities\": [ { \"typeId\": \"Person\", \"id\": \"21bbe625-c8da-4461-a8a0-f7c89aa85889\", \"version\": 1, \"properties\": { \"PER2\" : \"Armstrong\", \"PER1\" : \"Bryce\", \"PER4\" : \"00453474\", \"PER3\" : \"1971-04-29\", \"PER5\" : \"1976-10-23T13:30:00.409\" } } ], \"links\": [] } Run the query To make it possible for users to run the new query, update the auth-connector 's configuration: setup -t updateConnectorsConfiguration Then, use the Admin Console to reload the updated configuration by clicking Reload gateway to reflect your changes in the topology. Note: If you want to reload the connector configuration through Postman, you must add the SEMANTIC_SEARCH capability to the request's query parameter. If the value is not present, information about this service is excluded from the response. Now, you can use the new query: Open the Analyst's Notebook Premium desktop client. Select a record on the chart that has a value for the \"Family Name\" or \"Maiden Name\" property (or for both). Click \"External Search\". Select the new \"Find Same Last Name\" query, and run it. The list of records that you see in the results should include all the \"people\" from the people.json file whose surname matches either the family name or the maiden name of the record that you originally selected."
  },
  "content/miscellaneous/semantic-type-identifiers.html": {
    "href": "content/miscellaneous/semantic-type-identifiers.html",
    "title": "Semantic type identifiers",
    "keywords": "Semantic type identifiers When you write a service that uses seeds with semantic property type constraints , the definition of that service requires the identifiers of the semantic property types that it uses. Semantic property types are defined alongside semantic entity and link types in the i2 Semantic Type Library. This topic describes how to find a suitable semantic property type and retrieve the GUID that identifies it. Browse the i2 Semantic Type Library: Open i2 Analyze Schema Designer without loading a schema file. The application creates an empty schema. Click New Entity Type . You don't actually need to create an entity type, but this step provides access to the semantic type library. On the new Entity Type tab, find the Semantic Type field, and click Select to display the Select Semantic Type dialog. Open the Property tab to view the list of semantic property types in the library. Find the most generic type that's meaningful for the searches that you want to enable. For example, if you're writing a service that works with phone numbers, it's likely that you'd use the parent Phone Number semantic property type rather than one of its child types (Cell Phone Number, Home Phone Number, and so on). Make a note of the name of the type, which you'll need for the next step. The Select Semantic Type dialog does not display the GUIDs of the semantic types. To find those identifiers, you can look in the XML file that contains the library. Look up the GUID of your chosen type in the i2 Semantic Type Library file: In the location where you installed i2 Analyze, navigate to the toolkit\\application\\stl directory. Open the file named stl.xml in an XML editor. Search the file for the name of the semantic property type that you want to use. For example, Person Last Name: <lcx:Property pGUID=\"guid62B01C18-2E64-46D5-B2FF-3C69B4F76FEB\" baseProperty=\"guid6F5BDE02-467B-4fef-8585-F55C13F7B592\"> <PropertyName>Person Last Name</PropertyName> <RelatedType tGUID=\"guid8A586959-9837-47DE-8DBF-BC7031F01545\"/> <Documentation> <lcx:Synonym>Family Name</lcx:Synonym> <lcx:Synonym>Surname</lcx:Synonym> <Description>A person's family name.</Description> </Documentation> </lcx:Property> Copy the GUID that appears as the value of pGUID attribute of the <lcx:Property> element, which is guid62B01C18-2E64-46D5-B2FF-3C69B4F76FEB in the example above."
  },
  "content/miscellaneous/source-identifiers.html": {
    "href": "content/miscellaneous/source-identifiers.html",
    "title": "Source identifiers",
    "keywords": "Source identifiers In all versions of i2 Analyze, the ETL pipeline requires you to provide origin identifiers for incoming records through the ingestion mappings that you write. Starting from i2 Analyze 4.3.5, developers of connectors for i2 Connect and plug-ins for i2 Analyst's Notebook Premium can explicitly provide source identifiers for the records that they create. Note: For high-level descriptions of source identifiers and origin identifiers (and the differences between them), see Identifiers in i2 Analyze records . When you attach recognizable identifiers to the records that you create in connector code, you enable services to perform operations based on those identifiers. You also enable i2 Analyze clients and servers to perform matching on records that have a shared source. The rules that govern the structure and contents of source identifiers are similar to - but not the same as - the rules for origin identifiers. Similar to their definitions, the rules for source identifiers are sometimes less restrictive than those for origin identifiers. The structure of a source identifier A source identifier contains a type and a key . If a source identifier is attached to a record that is subsequently uploaded to the Information Store, then the identifier is stored with that record in the database. As a result, there are limits on how much information types and keys can store. type The type of a source identifier allows the services in an i2 Analyze deployment to determine whether the source identifier is \"known\" to them - that is, that they can understand the key. The value of the type element does not have to be meaningful, but should be unique to your services so that you avoid clashes with any third-party services you might use. The length of the source identifier's type must not exceed 200 bytes, which is equivalent to 100 2-byte Unicode characters. The following types are reserved and must not be used (case-insensitive): OI.IS OI.DAOD OI.ANB Anything starting with i2. key The key of a source identifier is an array containing the information necessary to reference the data in its source. The pieces of information that you use to make up the key differ depending on the source of the data. The total length of the source identifier's key must not exceed 692 bytes, which is equivalent to 346 2-byte Unicode characters. The key is stored as a serialized JSON array, so additional characters appear alongside the actual key elements: two characters are required for the array braces, and two quotes are required for each element, with commas as separators between elements. In other words, a key with N elements requires 3N + 1 characters of overhead. For example, the total length of [\"a\",\"bc\",\"defg\"] is 17 characters, while [\"a,bc,defg\"] is 13 characters. Also, if some special characters are present in a key element, they must be escaped for storage. For example, \" becomes \\\" , which further increases the size of the key. Using source identifiers When a service receives a seed record, it can inspect the source identifiers to find out whether a known type is present. If it is, then the key can be used to retrieve or match information from the source system. When records are returned from external sources, the client can match them against other records by using the source identifiers (if source identifier matching is enabled in the match rules with enableSourceIdentifierMatching=\"true\" ). Also, if source identifier matching is configured in the system match rules, then source identifiers provided through i2 Connect or Analyst's Notebook Premium can be matched against the origin identifiers of records in the Information Store. In both cases, source identifier matches occur when record item types match and the type and keys of the identifiers are an exact (case-sensitive) match. Note: After upgrading to i2 Analyze 4.3.5 from a previous release, if you want to match against the origin identifiers of already ingested records, you must build a new match index. Limitations There is a limit on the number of unique source identifiers that you can add to a record in the Information Store. The default limit is 50, but you can modify it by adding MaxSourceIdentifiersPerRecord=N (where N is a positive integer) to the DiscoServerSettingsCommon.properties file. Origin identifiers are not stored in chart records. As a result, they are not present as source identifiers on seeds that are sent to connectors or Analyst's Notebook Premium plug-ins. Only those source identifiers that have been added to records through the same mechanisms are present. Connectors that specify source identifiers are API-compatible with older versions of the i2 Connect gateway and Analyst's Notebook Premium, but those products will not recognize new source identifiers as such. Rather, they will be treated as ordinary identifiers or ignored. To use source identifiers to their full potential, upgrade all products to the latest release at your earliest opportunity. Example This example is based on the NYPD connector. When the external data source is queried, source identifiers are built and attached as the identifier of the data for the record, rather than using the unique identifier from the dataset. This arrangement allows the find-like-this-complaint service to extract the relevant information and use it in the query parameters. Build a source identifier To build a source identifier, you must provide the values for its type and key fields: For the type, you need a unique value that you can use to identify your source identifiers. You can use the value to filter out any other source identifiers that are also attached to the seeds, such as system-generated source identifiers. For the key, populate an array with information that can be used to build the query in the service. Then, using the source identifier type and key, assign it to the entity's identifier property. In the following Java and Python examples, the type is assigned the value NYPD . The key is built using the values of the law_cat_cd , addr_pct_cd and cmplnt_num columns of the NYPD Complaint Data dataset. Java example In the ItemFactory.java file in the sample solution directory for the NYPD connector, update the createComplaint method with the following code: final EntityData complaint = new EntityData(); if (entry.offenceLevel != null && entry.precinctCode != 0) { final SourceId sourceId = new SourceId(); sourceId.type = \"NYPD\"; sourceId.key = Arrays.asList(entry.offenceLevel, String.valueOf(entry.precinctCode), entry.complaintNum); complaint.id = Collections.singletonMap(\"sourceId\", sourceId); } else { complaint.id = \"COMP\" + entry.complaintNum; } Python example In the classes.py file in the sample solution directory for the NYPD connector, define a global function to build the source identifiers: def generate_source_identifier(entry): return { 'sourceId': { 'type': 'NYPD', 'key': [entry.get('law_cat_cd'), entry.get('addr_pct_cd'), entry.get('cmplnt_num')] } } Then, on the id property in the Complaint class, replace the call to the method get_id(base, entry) with generate_source_identifier(entry) : class Complaint(Entity): \"\"\" A base class for Complaint entities. Attributes: entry (dict): One record from the external datasource. \"\"\" def __init__(self, entry): id = generate_source_identifier(entry) Update the find-like-this-complaint service To use the source identifiers that are now attached to the seeds, you need to extract and filter the sourceIds to find the ones with the type that you created. You can use the key of those source identifiers in your query parameters to the service. In the following Java and Python examples, the source identifiers are filtered down to those whose type property contains the value NYPD . Their key values are then used to populate the request parameters. Instead of the existing parameters, which searched for matching records with the same level of offense, the request is updated to search for matching records with the same level of offense within the same precinct . Java example In the ExternalConnectorDataService.java file in the sample solution directory for the NYPD connector, update the findLikeThisComplaint method with the following code: final List<SourceId> sourceIds = seed.sourceIds; // filter out system generated source ids final List<SourceId> nypdSourceIds = sourceIds .stream() .filter(sourceId -> sourceId.type.equals(\"NYPD\")) .collect(Collectors.toList()); if (nypdSourceIds.isEmpty()) { return new ConnectorResponse(); } final Map<String, Object> params = new HashMap<>(); params.put(\"limitValue\", 50); params.put(\"lawCategory\", nypdSourceIds.get(0).key.get(0)); // Level of offence params.put(\"precinctCode\", nypdSourceIds.get(0).key.get(1)); // Precinct code final String url = LIMIT_PARAM + \"&$where=law_cat_cd='{lawCategory}'&addr_pct_cd='{precinctCode}'\"; Python example In the service.py file in the sample solution directory for the NYPD connector, update the impl_find_like_this_complaint method with the following code: if seeds: sourceIds = seeds['entities'][0]['sourceIds'] nypdSourceIds = [] # filter out system generated source ids for sourceId in sourceIds: if sourceId['type'] == 'NYPD': nypdSourceIds.append(sourceId) if not nypdSourceIds: return response params = f\"&$where=law_cat_cd='{nypdSourceIds[0]['key'][0]}'&addr_pct_cd='{nypdSourceIds[0]['key'][1]}'\" records = query_external_datasource(params) response = marshal(records, type_ids['complaint'], False)"
  },
  "content/miscellaneous/source-references.html": {
    "href": "content/miscellaneous/source-references.html",
    "title": "Source references",
    "keywords": "Source references Source references can provide users with rich information about where the data they see in i2 Analyze records came from. By adding source references to the record data that a service returns, you enable extra functionality in the client, including the ability to associate and view images with your records. Source references provide up to five pieces of information about the source of an i2 Analyze record: The type of the source that data in the record came from The name of the source A description of the source The location of the source, which might be a URL An image of the source, which might also be an image for the record When you return source reference information from a service, only the name is mandatory. To add source references to the record data that your service returns to clients, you must arrange to include a sourceReference object as a peer of the properties object in your response from the acquire endpoint. In your code that generates responses from the acquire endpoint, add at least the following extra content: { ... \"properties\": { ... }, \"sourceReference\": { \"source\": { \"name\": \"source_name\" } }, ... } To give users the best experience, add a type and a description for your source that align with the definitions in the source reference schema for the deployment: ... \"sourceReference\": { \"source\": { \"name\": \"source_name\", \"type\": \"source_type\", \"description\": \"source_description\" } }, ... If the data in the external source has associated images, you can arrange for users to see an image for a record when they view it by adding an image field to the source object: \"source\": { ... \"image\": \"image_url\" } You can similarly include the location of the source in the source reference by adding a location field and setting it to either a URL or a text description. Completing these steps in your service code means that source references are present in the records that users retrieve and view. Source references are optional, so you can control whether to include them for all records, or just for records of particular types. Finally, source references have the same features in entity and link records, so you do not need to write different code for those two cases. Example The following JSON structure provides an example of an entity object that contains a source reference: { \"id\": \"123\", \"typeId\": \"ET1\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" }, \"sourceReference\": { \"source\": { \"name\": \"Source Dataset\", \"type\": \"Open source data\", \"description\": \"A source reference to the corresponding record from the dataset.\", \"location\": \"https://data.cityofnewyork.us/resource/7x9x-zpz6.json?$where=cmplnt_num=123456789\", \"image\": \"https://github.com/i2group/analyze-connect/blob/gh-pages/content/schemas/images/nypd-dataset-webpage.png?raw=true\" } } }"
  },
  "content/miscellaneous/spi-examples.html": {
    "href": "content/miscellaneous/spi-examples.html",
    "title": "i2 Connect SPI examples",
    "keywords": "i2 Connect SPI examples You can use the following example requests and responses to help understand how parameterized and seeded search services work. Example data Throughout, assume you are querying the following set of entities and links. The following are the type IDs corresponding to each of the entity, link and property types used in the example schema: Entity ID Complaint ET1 Location ET2 Person ET3 Link ID Located At LT1 Suspect Of LT2 Victim Of LT3 Properties For Entity ID Complaint Number Complaint PT1 Borough Name Location PT16 Age Group Person PT26 Race Person PT27 Gender Person PT28 This can be represented as a set of entities and links in JSON as follows: { \"entities\": [ { \"typeId\": \"ET1\", \"id\": \"complaint-1\", \"version\": 1, \"properties\": { \"PT1\": \"1\", } }, { \"typeId\": \"ET2\", \"id\": \"manhattan\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" } }, { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-B\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"<18\", \"PT27\": \"Black\" } }, { \"typeId\": \"ET1\", \"id\": \"complaint-2\", \"version\": 1, \"properties\": { \"PT1\": \"2\", } }, { \"typeId\": \"ET3\", \"id\": \"person-C\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-D\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" } }, { \"typeId\": \"ET1\", \"id\": \"complaint-3\", \"version\": 1, \"properties\": { \"PT1\": \"3\", } }, { \"typeId\": \"ET2\", \"id\": \"bronx\", \"version\": 1, \"properties\": { \"PT16\": \"BRONX\" } }, { \"typeId\": \"ET3\", \"id\": \"person-E\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-F\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"45-64\", \"PT27\": \"Black\" } } ], \"links\": [ { \"typeId\": \"LT1\", \"id\": \"located-at-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"manhattan\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-A\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-B\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT1\", \"id\": \"located-at-2\", \"version\": 1, \"fromEndId\": \"complaint-2\", \"toEndId\": \"bronx\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-2\", \"version\": 1, \"fromEndId\": \"complaint-2\", \"toEndId\": \"person-C\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-2\", \"version\": 1, \"fromEndId\": \"complaint-2\", \"toEndId\": \"person-D\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT1\", \"id\": \"located-at-3\", \"version\": 1, \"fromEndId\": \"complaint-3\", \"toEndId\": \"bronx\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-3\", \"version\": 1, \"fromEndId\": \"complaint-3\", \"toEndId\": \"person-E\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-3\", \"version\": 1, \"fromEndId\": \"complaint-3\", \"toEndId\": \"person-F\", \"linkDirection\": \"WITH\", } ] } Parameterized search To implement a service to search for people by age group, you can define a clientConfig in config.json like the following: { \"id\": \"age-search-form\", \"type\": \"FORM\", \"config\": { \"sections\": [ { \"conditions\": [ { \"id\": \"age-group-search-term\", \"label\": \"Age Group\", \"logicalType\": \"SINGLE_LINE_STRING\", \"mandatory\": true } ] } ] } } The id should be a unique identifier for this clientConfig . Then the services you define can use this form by supplying this id in the service's clientConfigId field. The config contains sections and each section contains a JSON object defining a condition field depicted below: The condition id is used as a reference for the value in the request. The label is the field title as shown in [1] above. The logicalType defines the accepted data type of the request value entry [2] . The mandatory field specifies whether a value is required for the field. Empty mandatory fields are highlighted red and shown a warning message [3] . A DaodRequest issued by i2 Analyze when a user runs this search might look like this: { \"payload\": { \"conditions\": [ { \"id\": \"age-group-search\", \"logicalType\": \"SINGLE_LINE_STRING\", \"value\": \"18-24\" } ], \"seeds\": {} } } The request searches for Person entities where the Age Group property is equal to \"18-24\". In the implementation of the parameterized search service, you would filter through the data and find entities which satisfy the request requirements, i.e. have typeId equal to \"ET3\" and have the Age Group property PT26 equal to \"18-24\" . The response, from the example request above, would look like this: { \"entities\": [ { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-E\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } } ], \"links\": [] } Seeded search Seeded searches take as input a set of entities and links that a user already has on their chart and use this information when finding results. The types of seeded search operations that the following examples cover are: \"Find like this\", where a user is able to select a single entity and search for other entities of the same type with similar properties. \"Expand\", where a user can select an entity on their chart and search for all other entities that are connected to it by a link, and all those entities and links will be returned. Note: The requests in these examples do not include data from semantic seeds. For an example of a request that does contain such data, see Semantic seeds . Find like this A DaodRequest received by the connector for a \"find like this\" search on the example data could look something like the following: { \"payload\": { \"conditions\": {}, \"seeds\": { \"entities\": [ { \"accessDimensionValues\": [], \"extensions\": [], \"label\": \"\", \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" }, \"seedId\": \"d8ee0564-57bb-40ed-9409-79f8d13497a5\", \"sourceIds\": [ { \"itemTypeId\": \"ET3\", \"key\": [\"nypd-connector\", \"ET3\", \"person-D\"], \"type\": \"OI.DAOD\" } ], \"typeId\": \"ET3\" } ], \"links\": [], \"allItemTypes\": [] } } } You can deduce which of the entities the DaodSeedEntity in this request corresponds to by looking at the key in the sourceIds field. The third element of this list gives us the ID we have assigned the entity in our connector, \"person-D\" . You also have its type ID \"ET3\" , so it is a Person entity. Have a look at the data above and find this entity. To perform a \"find like this\" search using this seed entity, you need only use its properties. We can filter through our list of entities for those which have typeId equal to \"ET3\" (are Person entities) and have the properties: PT26 equal to \"F\" , i.e. they are female; PT27 equal to \"White\" , i.e. they are a white female; and PT28 equal to \"<18\" , i.e. they are a white female under 18 years of age. After excluding the seed entity itself, you would return the following: { \"entities\": [ { \"typeId\": \"ET3\", \"id\": \"person-C\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" } } ], \"links\": [] } Expand A DaodRequest received by the connector for an \"expand\" search might look like this: { \"payload\": { \"conditions\": {}, \"seeds\": { \"entities\": [ { \"accessDimensionValues\": [], \"extensions\": [], \"label\": \"\", \"properties\": { \"PT1\": \"1\", }, \"seedId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"sourceIds\": [ { \"itemTypeId\": \"ET1\", \"key\": [\"nypd-connector\", \"ET1\", \"complaint-1\"], \"type\": \"OI.DAOD\" } ], \"typeId\": \"ET1\" } ], \"links\": [], \"allItemTypes\": [] } } } Again, you can deduce which of our entities the DaodSeedEntity corresponds to by looking at the sourceIds . The id of the entity in question is \"complaint-1\" and it has typeID equal to \"ET1\" , so it is a complaint. Look at the example data above and find which entity you are expanding. What would you expect an Expand operation to return? To perform an Expand operation with this entity as the seed, you need to: Find all links connected to the corresponding entity. This means going through all the links and finding those with a fromEndId or a toEndId equal to the id of the entity, \"person-D\" . Find all entities at the other end of these links. This can be done by using the fromEndId s and toEndId s of the links found in step 1 - just use the end ID that does not correspond to the seed entity. If returning these entities and links as-is, along with the entity corresponding to the seed, you would respond with: { \"entities\": [ { \"typeId\": \"ET1\", \"id\": \"complaint-1\", \"version\": 1, \"properties\": { \"PT1\": \"1\", } }, { \"typeId\": \"ET2\", \"id\": \"manhattan\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" } }, { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-B\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"<18\", \"PT27\": \"Black\" } } ], \"links\": [ { \"typeId\": \"LT1\", \"id\": \"located-at-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"manhattan\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-A\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-B\", \"linkDirection\": \"WITH\", } ] } When copying these results to a chart, the seed entity would be duplicated along with all its links and connected entities that may already be on the chart which would all be connected to the duplicate. Depending on how you want the service to function, you might prefer to have the returned items connected to the entity that you selected on the chart rather than to a duplicate. In this case, you need to change all id , fromEndId and toEndId fields that refer to the ID of the seed entity (in this case \"complaint-1\") to seedId of the DaodSeedEntity in the request, i.e. \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\" . In this case, you would return the following response: { \"entities\": [ { \"typeId\": \"ET2\", \"id\": \"manhattan\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" } }, { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-B\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"<18\", \"PT27\": \"Black\" } } ], \"links\": [ { \"typeId\": \"LT1\", \"id\": \"located-at-1\", \"version\": 1, \"fromEndId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"toEndId\": \"manhattan\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-1\", \"version\": 1, \"fromEndId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"toEndId\": \"person-A\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-1\", \"version\": 1, \"fromEndId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"toEndId\": \"person-B\", \"linkDirection\": \"WITH\", } ] }"
  },
  "content/miscellaneous/spi-version.html": {
    "href": "content/miscellaneous/spi-version.html",
    "title": "SPI version negotiation",
    "keywords": "SPI version negotiation Starting with version 4.4.0 of i2 Analyze, your connectors can negotiate with the gateway to establish a common understanding of which version of the i2 Connect SPI to use. By default, the gateway assumes that your connector will use version 1.0 of the SPI. When 1.0 is the correct version for your connector, you don't need to do anything. If you rely on a later version of the SPI, you can both determine what versions the server supports, and respond with the version that you want to use. For more information on the differences between SPI versions, refer to the SPI changelog . Server-supported SPI versions All HTTP requests to your connector from i2 Analyze version 4.4.0 and above contain a header named I2-Spi-Versions . The value of the header is a comma-separated list of the SPI versions that the server supports. Connector specification of SPI version To specify the SPI version that it wants to use, a connector can add a field named version to its response from the configuration endpoint. For example: { \"version\": \"1.0\", \"defaultValues\": { \"timeZoneId\": \"Europe/London\", ... }, \"services\": [ ... ] } Java example The following code is taken from ConnectorController.java in the sample solution code for the NYPD connector. In implementing the configuration endpoint, it extracts the supported SPI versions from the HTTP headers, and outputs some diagnostic information: private static final int CONNECTOR_MAJOR_VERSION = 1; private static final int CONNECTOR_MINOR_VERSION = 0; /** * Defines the /config endpoint which acquires the connector configuration data. * * @return The config.json file. */ @RequestMapping(method = RequestMethod.GET, value = \"/config\", produces = APPLICATION_JSON_VALUE) public Resource config(@RequestHeader(value = \"I2-Spi-Versions\", required = false) List<String> gatewaySupportedVersions) { System.out.println(\"Gateway supported versions: \" + gatewaySupportedVersions); for (String supportedVersion: gatewaySupportedVersions) { final String[] supportedVersionParts = supportedVersion.split(\"\\\\.\"); final double supportedMajorVersion = Double.parseDouble(supportedVersionParts[0]); final double supportedMinorVersion = Double.parseDouble(supportedVersionParts[1]); final String connectorVersion = CONNECTOR_MAJOR_VERSION + \".\" + CONNECTOR_MINOR_VERSION; if ((CONNECTOR_MAJOR_VERSION == supportedMajorVersion) && (CONNECTOR_MINOR_VERSION <= supportedMinorVersion)) { System.out.print(\"The gateway supports connector version \" + connectorVersion + \"\\n\"); } else { System.out.print(\"The gateway does not support connector version \" + connectorVersion + \"\\n\"); } } return configResource; } Python example The following code is taken from controller.py in the sample solution code for the NYPD connector. In implementing the configuration endpoint, it extracts the supported SPI versions from the HTTP headers, and outputs some diagnostic information: connector_major_version = '1' connector_minor_version = '0' @controller.route('/config') def config(): gateway_supported_versions = request.headers['I2-Spi-Versions'] print('Gateway supported versions: ' + gateway_supported_versions) for supported_version in gateway_supported_versions.split(','): supported_version_parts = supported_version.split('.') supported_major_version = supported_version_parts[0] supported_minor_version = supported_version_parts[1] connector_version = connector_major_version + '.' + connector_minor_version if ((connector_major_version == supported_major_version) and (connector_minor_version <= supported_minor_version)): print('The gateway supports connector version ' + connector_version) else: print('The gateway does not support connector version ' + connector_version) return send_from_directory('static', 'config.json')"
  },
  "content/miscellaneous/troubleshoot.html": {
    "href": "content/miscellaneous/troubleshoot.html",
    "title": "Troubleshooting guide",
    "keywords": "Troubleshooting guide This is a list of common issues you may face during the development of your connector and how to solve them. They are categorized by the stage of development in which they may arise. If you come across a problem that is not covered below, please raise an issue here . Where to check for errors Log files The i2 Analyze server logs can be found in the i2analyze\\deploy\\wlp\\usr\\servers\\opal-server\\logs\\ directory. console.log is the main server log file. opal-services-daod\\i2_Analysis_Repository.log is where errors with your connector configuration and/or data will appear. Connector configuration errors \"Failed to retrieve configuration information for the connector with identifier '<CONNECTOR_ID>' from the URL '<URL>'\" You will see this in the output when running setup -t start if i2 Analyze fails to retrieve your connector's configuration from its config endpoint. Check that the base URL printed here is correct. If not, update the connector base url given in the i2 Analyze topology.xml file (in the i2analyze\\toolkit\\configuration\\environment directory). If the URL is correct, then you need to check the implementation of your config endpoint and that you have assigned to it the correct URL path. This should be the base URL suffixed with \"/config\". \"The service [...] specifies an unknown semantic property type identifier [...]. The identifier must be present in the core semantic type library, or in the semantic types that connector or gateway schemas define, or in the Information Store schema (if present).\" You will see this in the output when running setup -t start if your connector includes a service that specifies semantic seed constraints, and the server detects a problem with the semantic type identifiers that the service uses. During connector development, the message usually means that you made a mistake when you added the identifier to the service configuration - a copy-and-paste error, or something similar. The detail in the message will suggest the solution. If you see the message during deployment of a connector to a production system, it is more likely that the version of the i2 Semantic Type Library on the i2 Analyze server is older than the version that was used during development. In this situation, you must update the server with the latest version of the i2 Semantic Type Library. \"Some queries are not configured correctly. Contact your system administrator.\" You will see this in the External Searches window if there is a problem with your connector configuration. Clicking DETAILS will provide more information. Problems might include: No acquireUrl defined for a service: make sure you define one in the service definition The application is communicating with the connector through a protocol that is insecure. You will see this because i2 Analyze and your connector communicate via HTTP. In a production environment, you should secure this connection by using HTTPS. This is beyond the scope of this guide, but is covered in the i2 product documentation . Errors running connector services \"Failed to open the selected query. Contact your system administrator\" If you see the above error message when trying to run a query defined by your connector via External Searches in Analysts's Notebook Premium, there are a number of potential causes. To understand what went wrong: Look at the log file: i2analyze\\deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services-daod\\i2_Analysis_Repository.log You should see at the bottom a more detailed description of the error that occurred. You will most likely see an error message starting with the following: d484-4e04-97d5-86e771fed434 - Validation error while retrieving a record. - The service with identifier '<SERVICE_NAME>' on the connector with identifier '<CONNECTOR_ID>' returned invalid data Below this line will be more specific information about why the query failed. Examples and their solutions are given below. \"The schema does not contain an entity/a link/a property with identifier '<TYPE_ID>'\" This means you have supplied an entity or link with a typeId that does not exist in your schema. Check that the typeId you supplied is not a typo. \"The value '<PROPERTY_VALUE>' is not valid for the property type with identifier '<TYPE_ID>'. The value '<PROPERTY_VALUE>' is incompatible with the '<LOGICAL_TYPE>' data type.\" This means that the property value you supplied is not in the correct format, which is defined by the logical type of the property. This is especially common when dealing with DATE , TIME and DATETIME properties. Check that you have assigned an appropriate logical type for this property and see the data model examples to understand the required format for each logical type."
  },
  "content/schemas/connector-schema.html": {
    "href": "content/schemas/connector-schema.html",
    "title": "Connector schemas",
    "keywords": "Connector schemas A connector schema is a type of i2 Analyze schema that is provided by a connector that defines some or all of the item types the connector can return. Connector schemas allow for connectors to be added to, or removed from, i2 Analyze deployments more easily without the need to change the existing Information Store or gateway schemas. They also make it easier for connectors to be shared and used by multiple deployments. To create a connector schema, use i2 Analyze Schema Designer, which provides an interface for creating the XML file containing the schema and, optionally, an additional XML file containing the charting schemes. To learn more about how to develop i2 Analyze schemas, see Designing an i2 Analyze schema . Configuring a connector schema A connector schema is provided by the connector itself. To supply a schema, a connector must provide: An endpoint that returns the schema in its XML form from a GET request. The URL for the endpoint. The URL is provided in the schemaUrl field of its configuration, and it must be relative to the connector's base URL. A connector schema can be supplemented with a charting scheme. To supply a charting scheme, a connector must provide: An endpoint that returns the charting scheme in its XML form from a GET request. The URL for the endpoint. The URL is provided in the chartingSchemesUrl field of its configuration, and it must be relative to the connector's base URL. For example, if a connector's base URL is http://exampleconnector.com:3700 , the schema and charting schemes endpoints might be available at the following URLs: http://exampleconnector.com:3700/schema http://exampleconnector.com:3700/charting-schemes Then, you add the schemaUrl and chartingSchemesUrl fields to the object that is returned from the configuration endpoint. For example: { \"schemaUrl\": \"/schema\", \"chartingSchemesUrl\": \"/charting-schemes\", \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ ... ] } Connector schema short names All connector schemas have a short name that is displayed to analysts in i2 Analyst's Notebook Premium when they interact with entity and link types from that schema. The short name of a connector schema can be configured in two ways: The connector can specify the short name of the configuration that is returned from its configuration endpoint. You can specify the short name of the schema when defining the connector in the i2 Analyze topology ( topology.xml ). The short name that is supplied in the topology takes precedence over the one supplied in the connector configuration. If a short name is not specified in either location, then the identifier of the connector in the topology is used. Connector configuration To specify the short name in the return value from the configuration endpoint, use the schemaShortName field of the ConnectorConfig object. For example: { \"schemaUrl\": \"/schema\", \"chartingSchemesUrl\": \"/chartingschemes\", \"schemaShortName\": \"Social Media\", \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ ... ] } For more information about the ConnectorConfig object, see the i2 Connect gateway REST SPI . i2 Analyze topology To specify the short name in the i2 Analyze topology, use the schema-short-name attribute of the <connector> element. For example: <connector id=\"example-connector\" name=\"Example Connector\" base-url=\"http://localhost:3700/\" schema-short-name=\"Social Media\"/>"
  },
  "content/schemas/gateway-schema.html": {
    "href": "content/schemas/gateway-schema.html",
    "title": "Gateway schemas",
    "keywords": "Gateway schemas A gateway schema is a type of i2 Analyze schema that defines the item types that can be used by any number of connectors. A gateway schema allows multiple connectors to return the same item types, which makes them useful in situations where multiple connectors return similar data. Gateway schemas can be used in i2 Analyze deployments that include the i2 Connect gateway. Gateway schemas are optional, and there is no limit to the number of gateway schemas that can be deployed. A connector can return item types from one gateway schema only. To create a gateway schema, use i2 Analyze Schema Designer, which provides an interface for creating the XML file containing the schema and, optionally, an additional XML file containing the charting schemes. To learn more about how to develop i2 Analyze schemas, see Designing an i2 Analyze schema . Gateway schema short names All gateway schemas have a short name that is displayed to analysts in i2 Analyst's Notebook Premium when they interact with entity and link types from that schema. The short name must be unique within the i2 Analyze deployment. You specify the short name when you configure the gateway schema. For a connector to return item types defined in a gateway schema, the schema must be assigned to the connector using its short name. Configuring a gateway schema To configure a gateway schema, you must have a schema XML file and provide a short name for it. You can also provide a charting schemes XML file. To configure a gateway schema: Copy your schema and charting schemes files to the configuration/fragments/common/WEB-INF/classes directory in your i2 Analyze configuration. Open the ApolloServerSettingsMandatory.properties file in the same directory, and add the following properties Gateway.<SCHEMA_SHORT_NAME>.SchemaResource=<SCHEMA_XML_FILE_NAME> Gateway.<SCHEMA_SHORT_NAME>.ChartingSchemesResource=<CHARTING_SCHEMES_XML_FILE_NAME> For example, to configure a gateway schema that defines item types relating to telecommunications data: The schema file is telecom-schema.xml The charting schemes file is telecom-schema-charting-schemes.xml The short name is Telecom Add the following to ApolloServerSettingsMandatory.properties : Gateway.Telecom.SchemaResource=telecom-schema.xml Gateway.Telecom.ChartingSchemesResource=telecom-schema-charting-schemes.xml Assigning a gateway schema to a connector For a connector to be able to return item types defined in a gateway schema, you must assign the gateway schema to it using its short name. You can do so in two ways: The connector can specify the short name of the gateway schema it uses in the configuration that is returned from its configuration endpoint. You can specify the short name of the gateway schema that the connector uses when defining the connector in the i2 Analyze topology ( topology.xml ). The setting in the topology takes precedence over the setting in the connector configuration. Connector configuration In the return value from the configuration endpoint, a gateway schema short name can be specified using the gatewaySchema field of the ConnectorConfig object. For example: { \"gatewaySchema\": \"Telecom\", \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ ... ] } For more information about the ConnectorConfig object, see the i2 Connect gateway REST SPI . i2 Analyze topology To assign a gateway schema to a connector in the i2 Analyze topology, specify the gateway schema short name in the <connector> element using the gateway-schema attribute. For example: <connector id=\"example-connector\" name=\"Example Connector\" base-url=\"http://localhost:3700/\" gateway-schema=\"Telecom\"/>"
  },
  "content/schemas/schema-fragments.html": {
    "href": "content/schemas/schema-fragments.html",
    "title": "Migrating from schema fragments",
    "keywords": "Migrating from schema fragments Before the introduction of connector and gateway schemas in i2 Analyze 4.3.3, schema fragments were supported on deployments with only the i2 Connect gateway. Schema fragments are a mechanism used to extend an i2 Analyze schema to incorporate new types from external data sources. They are combined with the deployed schema on server start-up. Similarly to the way connector schemas are retrieved now, schema fragments are provided via endpoints defined in the connector configuration so that they are made available to the server. Updating the schema fragments Schema fragments have a slightly different syntax to that of full schemas. Because of that, you will first need to create a new schema using the same entity and link types from your schema fragment. The easiest way to do this would be through the use of the i2 Analyze Schema Designer . Migrating to a connector schema Configuring a connector schema is very similar to configuring schema fragments. Supplying the schemaUrl (and, optionally, the chartingSchemesUrl ) endpoints in the connector configuration works in the same fashion. Just like with schema fragments, it is easiest, but not mandatory, to implement these endpoints on the server which hosts the connector that uses them. As such, the URLs can be either absolute or relative to the connector's base URL. Using short names An important distinction to note is the presence of schema short names , names which are displayed to analysts when viewing entity and link types so that they can identify which schema an item type belongs to. If you do not want the schema short name to default to the ID of the connector, it is necessary to explicitly specify the short name in either the connector configuration or the i2 Analyze topology . Migrating to a gateway schema Migrating from using schema fragments to using a gateway schema requires more changes. Carry out the following instructions: Remove schema fragment configuration First, remove the schemaUrl (and chartingSchemesUrl if it exists) definitions from the connector configuration. Leaving these properties in will have the connector believe that the specified endpoints are for retrieving a connector schema. Configure the gateway schema Follow the steps to configure a gateway schema . Unlike short names for connector schemas, all gateway schemas must have a short name specified. Assign a connector to a gateway schema Follow the steps to assign a gateway schema to a connector . This will enable your connector to accept seeds and return item types defined in the gateway schema. Using the same schema short name you specified for the gateway, it is necessary to specify the same in the connector's configuration or the i2 Analyze topology ."
  },
  "content/schemas/schema-types.html": {
    "href": "content/schemas/schema-types.html",
    "title": "Schema types",
    "keywords": "Schema types The data that you retrieve from an external source through the i2 Connect gateway must have entity and link types that are defined somewhere in the i2 Analyze deployment. If none of the existing schemas contains types that correspond with the external data, then you must extend or write a new schema as part of connector development. There are two ways to add types to an existing deployment of i2 Analyze: Add the new entity, link, and property types directly to one of the deployed schemas in the usual way. Additive changes to schemas are always allowed. Create a gateway or connector schema (along with an accompanying charting scheme), and define the new item types in that schema. Choosing which approach to use depends on your aims: Adding types to the Information Store schema makes them immediately and permanently available throughout the i2 Analyze deployment. Users can upload records that they retrieve from the external source to the Information Store without any further intervention. Especially in a deployment that does not include the Information Store, adding types to an existing gateway schema has some of the same benefits. The new types appear at familiar locations in the Analyst's Notebook Premium user interface, and they become available to future connectors. A key reason for creating a gateway schema to contain your new types is when you are creating a library of connectors that provide access to a particular source. The types in the gateway schema can be shared by all the connectors, and you can provide the gateway schema alongside the connectors if you need to distribute them. Creating a connector schema provides the highest level of flexibility. You can add the connector with its schema to any i2 Analyze deployment that includes the i2 Connect gateway. And you can make modifications to the connector schema without affecting the other schemas. But by its nature, this approach is also the least well integrated with the rest of the deployment. If you decide to extend one of the deployed schemas, you can do so in the same way that you would add types for any other reason: Open the XML file that contains the deployed schema in Schema Designer. Add entity and link types (or property types) to the schema that correspond to data from the external source. Add configuration for new and modified entity and link types to the charting scheme. Update the deployment with the modified schema and charting scheme. For more information, see the Schema Designer user guide, and the documentation on modifying the Information Store schema and modifying gateway schemas. If you decide to create a schema, then you still use Schema Designer, but the process is different: In Schema Designer, select File > New Schema . Edit the schema and its charting scheme to add types for the data from the external source, and then save and close the XML files. To deploy a gateway schema, make it (and its charting scheme) available to the deployment toolkit, and then follow the instructions in Adding, removing, modifying gateway schemas . To deploy a connector schema, arrange for it (and its charting scheme) to be available from endpoints on the server that hosts the connector. To complete this approach, you must tell the i2 Connect gateway where to find the endpoints that you created. You provide that information by adding to your implementation of the configuration endpoint."
  },
  "content/schemas/type-location.html": {
    "href": "content/schemas/type-location.html",
    "title": "Type location",
    "keywords": "Type location The type location of an item type used by a connector indicates where - between the Information Store schema, the gateway schema used by the connector, or its own connector schema - the item type is defined. With the introduction of gateway and connector schemas, it has become important for i2 Analyze to determine the origin of item types. It is possible for two schemas to have item types with the same identifier, in which case the gateway will infer the type location by looking in the following order: The connector's own schema The gateway schema used by the connector The Information Store schema A connector can explicitly specify the type locations for item types to avoid ambiguity and ensure that the desired item type from the appropriate schema is used. Default type locations You can specify the default type location for all entities and links returned from the connector in the defaultValues field of the connector configuration as follows: { \"defaultValues\": { \"entityTypeLocation\": \"<LOCATION>\", \"linkTypeLocation\": \"<LOCATION>\" }, \"services\": { /* ... */ } } Legal values for <LOCATION> are as follows: INFOSTORE - The Information Store schema. GATEWAY - The gateway schema used by the connector. CONNECTOR - The connector's own schema. Type locations for results A service's resultItemTypeIds property specifies the item types expected to be returned from the service. It also provides type location overrides for specific item types using the following structure: { \"<LOCATION>\": [\"TYPEID1\", \"TYPEID2\", /* ... */ ] } Again, <LOCATION> can be INFOSTORE , GATEWAY , or CONNECTOR . Since a service can use item types from all three type locations, all three type locations can be specified. For example, a service that returns records with entity type ET1 from the Information Store schema, ET2 from the gateway schema, and ET3 from a connector schema, will look like the following: { \"services\": [ { /* ... */ \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"LT1\"], \"GATEWAY\": [\"ET2\", \"LT2\"], \"CONNECTOR\": [\"ET3\", \"LT3\"] } } ] } Specifying type locations in this way overrides the defaultValues set for entityTypeLocation and linkTypeLocation . Type locations for seed constraints It is also possible to specify the type locations of seed records by including typeLocation attributes for each allowed item type of a service's seed constraint definition. This is shown as follows: { \"services\": [ { /* ... */ \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"LT1\"], \"GATEWAY\": [\"ET2\", \"LT2\"], \"CONNECTOR\": [\"ET3\", \"LT3\"] }, \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"ET1\", \"typeLocation\": \"INFOSTORE\", \"min\": 1, \"max\": 1 }, { \"id\": \"ET3\", \"typeLocation\": \"CONNECTOR\", \"min\": 1, \"max\": 1 } ] } } } ] }"
  },
  "content/spi-versions/1.0/changes.html": {
    "href": "content/spi-versions/1.0/changes.html",
    "title": "Changelog for i2 Analyze REST SPI 1.0",
    "keywords": "Changelog for i2 Analyze REST SPI 1.0 Version 1.0 is the first public release of the i2 Analyze REST SPI. As such, all endpoints and schemas are new at this version of the SPI. Added The following endpoints are new and fully supported at this version of the i2 Analyze REST SPI: Endpoints for retrieving the configuration information from each connector Endpoints for running synchronous queries Endpoints for running asynchronous queries An endpoint to authenticate users with third-party services An endpoint for validating a request Modified N/A Removed N/A"
  },
  "content/spi-versions/1.1/changes.html": {
    "href": "content/spi-versions/1.1/changes.html",
    "title": "Changelog for i2 Analyze REST SPI 1.1",
    "keywords": "Changelog for i2 Analyze REST SPI 1.1 Version 1.1 contains minor changes to the i2 Analyze REST SPI. Added N/A Modified The following endpoints were modified at this version of the i2 Analyze REST SPI: /ASYNC_SERVICE_URL A new semanticData field is added to the DaodSeeds request. This provides functionality to use the data from the semantic property types that were specified as seeds in the request. The optional queryId field in the AsyncQueryResponse response is now mandatory. /ASYNC_SERVICE_URL/{queryId} The optional message and type fields in the AsyncQuerySubstatus response are now mandatory. /CONFIGURATION_URL A service that accepts seed records can now be constrained by semantic types . Semantic constraints can be specified using the new ConnectorSeedSemanticPropertyTypes object. /USER_CONFIGURATION_URL A service that accepts seed records can now be constrained by semantic types . Semantic constraints can be specified using the new ConnectorSeedSemanticPropertyTypes object. /SYNC_SERVICE_URL A new semanticData field is added to the DaodSeeds request. This provides functionality to use the data from the semantic property types that were specified as seeds in the request. /VALIDATE_URL A new semanticData field is added to the DaodSeeds request. This provides functionality to use the data from the semantic property types that were specified as seeds in the request. Removed N/A"
  },
  "content/walkthrough/1-schema-design-guide.html": {
    "href": "content/walkthrough/1-schema-design-guide.html",
    "title": "Designing an i2 Analyze schema",
    "keywords": "Designing an i2 Analyze schema i2 Analyze schemas are at the core of any i2 Analyze deployment. They define the types of entities, links, and properties that users can view and analyze in Analyst's Notebook Premium. This section guides you through the process of developing a schema that aligns with the data the connector will retrieve. The process of developing a schema is iterative. Schema design requires fine tuning, especially if you are unfamiliar with the data set that you are creating the schema for. After you deploy i2 Analyze, you will be able to experiment with and test the schema you create. You can refer back to this document to repeat the process until you have a schema that you believe represents the NYPD Complaint Dataset. If you already understand the schema development process, you can use the example schema from this repository. Prerequisites Before you start, ensure that you have installed Analyst's Notebook Premium with the i2 Analyze Schema Designer. The i2 Analyze schema There are three integral components of an i2 Analyze schema: Entity types . These represent real-world concepts such as person, location, bank account, or event (\"meeting\", for example). Link types . A link type describes a relationship between two entity types, such as a person's ownership of a vehicle or a transaction between two bank accounts. Link types can specify exactly which entity types can appear at their ends, or they can represent a general association between entities of any types. A link between entities on a chart is represented by a line between them, which will be an arrow if the link is directed. Property types . Entities and links have properties, which store information about the object, event, or relationship. For example, a person might have a name, date of birth, and eye color; while a meeting might have a date, a time, and a duration. The property types of an entity or link type define exactly what properties the records of that type can have. Getting to know the data The data that your connector will retrieve comes from the NYPD Complaint Dataset . Before you build your schema, you should explore the data and decide how to model it as entities, links, and properties. You can also consult the data API and look at the raw data if you want to. Creating the schema Open i2 Analyze Schema Designer. From the File menu, click New Schema . Adding entity types You need to create entity types that can represent different objects. These types will relate to the supplied NYPD Complaint Dataset, and you can decide what they will be. One example of an entity type used in the sample schema is a Complaint . You can follow the instructions below to simulate creating the example schema with this entity type, or you can create your own types. From the Insert menu, click New Entity Type . Enter a name and a description for the entity type. (For example, Name: Complaint, Description: \"A complaint regarding an incident that occurred between some individuals\".) Select an icon to represent entities of this type in Analyst's Notebook Premium. (For example, a notepad.) You can create any entity types you like, provided that they align with the data. For example, you could have entity types to represent people and locations. Look at the example schema for inspiration if you need to. Adding property types to entity types For the entity types that you created, decide which fields in the data provide information that can be stored in properties. To continue with the Complaint example, some property types that might be suitable include: Complaint Number Complaint Start Date Complaint End Date Level of Offense Offense Description These property types are valuable because they allow for additional analysis to occur in Analyst's Notebook Premium. They provide the opportunity to conduct live formatting on the data, as well as to create different charting schemes for when data needs to be displayed in a different fashion. This is explained in more detail in Creating a charting scheme , below. To add a property type to an entity type: Select the entity type in the navigation tree. From the Insert menu, click New Property Type . Enter a name and a description for the new property type. (For example, Name: Complaint Number, Description: \"Persistent ID for each complaint\".) Assign a logical type that describes the type of data that properties of this type will contain. (For example, Integer .) Select the Is Mandatory checkbox if you want to enforce that all entities of this type have a property of this type. Adding link types Decide how the entity types you have created can be related. An example of a link type in the supplied schema is Located At. You can follow the instructions below to simulate creating the example schema with this link type, or you can create your own link types: From the Insert menu, click New Link Type . Enter a name and a description for the link type. (For example, Name: Located At, Description: \"Where the incident was located\".) Links often make sense only between certain entity types. To choose which entities can be connected by a link of this type, open the Link Ends tab and select the appropriate From End Types and To End Types . (For example, From End: Complaint, To End: Location.) Define as many link types as you like, ensuring that they align with the data. For example, a person might be linked to a complaint if they are listed as the victim in the complaint. You might therefore have a Victim In link type that connects Person entities to Complaint entities. Similarly, you might define a a Suspect In link type. Adding property types to link types For the link types that you have created, decide if there are fields in the data set that can provide properties for them. The example schema contains no links with properties, and you can complete the demonstration without them, but they might be useful for the connectors that you make after following this guide. To add a property type to a link type: Select the link type in the navigation tree. From the Insert menu, click New Property Type . Enter a name and a description for the new property type. Assign a logical type that describes the type of data that properties of this type will contain. For example, Single Line String or Integer . Select the Is Mandatory checkbox if you want to enforce that all links have a property of this type. Adding semantic types Entity types, link types, and property types can all have semantic types that provide a connection between the model that your schema represents, and its meaning in the real world. For example, you might have entity types in your schema called Car and Truck. By giving them both the semantic type \"Vehicle\", you create an association between Car records and Truck records - that they are the same kind of thing - that would not otherwise be present in the model. Similarly, a Person might have an SSN property type, while a Car (or a Truck) might have a License Plate property type. Both of those are fundamentally identifiers, and you can represent that by giving them both an \"Identifier\" semantic type. Adding semantic types to the entity, link, and property types in your schema improves the analysis that client applications can perform. You can also take this semantic information into account in the connectors that you create. i2 provides a standard library of semantic types that you can assign to the types in your schema: In Schema Designer, for each type, find the Semantic Type field and click Select to open the Select Semantic Type dialog. Use the tabs in the dialog to browse the library and understand the relationships between the semantic types that it contains. When you've found a semantic type that fits your entity, link, or property type, click Assign to add it to the schema. For more information about semantic types and the i2 Semantic Type Library, see the section on Assigning semantic types in the Schema Designer documentation. Creating a charting scheme When you have defined all the entity, link, and property types you want to model, you must create a charting scheme to determine how items that contain your entity and link records appear in Analyst's Notebook Premium. For example, the charting scheme determines how labels on the chart are generated from properties of entities and links. To create a charting scheme: From the File menu, click Edit Charting Schemes . Expand the first charting scheme in the left panel to reveal Entity Types and Link Types . Expand Entity Types and Link Types to reveal the entity and link types that you created earlier. Right-click Properties and select Insert Chart Item Property Type\" . From the dropdown, select Label . In the Label tab, set the text to be displayed with entities of this type in Analyst's Notebook Premium to distinguish them on the chart. You can use the values of properties assigned to a record to construct your label. To use a property value, click the downward arrow on the Insert button and select the property type of your choice. In the example charting scheme, the Complaint Number is used in the label of Complaint entities. You can prefix the value of a property with a space, a newline, or some custom text. The prefix can clarify what data is represented in the label. For example, a Complaint Number might be prefixed with the text \"Complaint \". This is displayed in Schema Designer as follows: A Complaint with the Complaint Number 667574574 will then be displayed in Analyst's Notebook Premium like this: You can also add a suffix in the same way, which can be useful if you use multiple property values in the label. For example, you might display values on separate lines, or with spaces between them. For example, Person entities in the example are shown as follows: You can also set a default value to use when propert values are not set. In this case, the default value is \"NO PROPERTIES\". This range of options allows you to customize your charting scheme in a way that best represents your data. To combine multiple properties into a single label, follow the above process again. You can add more charting schemes to visualize the data in different ways. The supplied example schema contains two charting schemes - one with labels (detailed version) and one without labels (simplified version). To create further charting schemes, repeat the above instructions with different (or no) labels. A group of entities and links with a detailed charting scheme might look like this: This charting scheme provides a substantial amount of information to the analyst, allowing them to see details of their entities and links at a glance. However, there might be times when this much information is overwhelming, especially in a chart with lots of records. For such cases, you might use a simpler charting scheme that just gives an overview of data in the records. A group of entities and links with a simplified charting scheme might look like this: When you have finished making your changes, click OK to close the charting scheme editor. See the i2 product documentation for information about adding property types that are not used in the example schema. Link summarizations Sometimes, multiple links exist between the same two entities on a chart in Analyst's Notebook Premium. You can use the charting scheme to choose how to represent these links. The different link summarizations that you can use are: Link Option Description Single Link All links of the selected type between the same two entities are combined into a single link. This option is useful if you are producing a summary chart and do not want to show all the detail of the data between entities. Directed All links of the selected type in the same direction between the same two entities are combined. This option is useful if you are charting information such as telephone calls or transactions. Multiple Each link of the selected type between the same two entities is charted separately. This option is useful if you want to show all the detail. If there are many links, you might make the chart cluttered or hard to read. Flow All links of the selected type between the same two entities are combined into a single, directed link. The direction (or flow) is determined from a property that you specify. For example, if there are several financial transactions between two bank accounts, this option is useful to determine the direction that the aggregate amount of the money is flowing. The example schema uses link multiplicity (Multiple) to show all the data flowing to and from entities. If you want to change this setting, this is the time to do so! You are likely to use other settings in your own connectors. Live Formatting in Analyst's Notebook Premium Live formatting in Analyst's Notebook Premium is a type of Conditional Formatting that changes the appearance of chart items, by applying conditional formatting specifications in real time. This allows analysts to format their charts to their own specifications. The default appearance of chart items is defined by your schema, but Live Formatting extends this to provide for filtering and quick, real time insight. An example of this functionality in action with the example schema is with the Sex property of the Person entity. Live Formatting allows for quick, visual confirmation of whether a Person is a Male or a Female (pink for Male, blue for Female). This example can be seen as follows. If you wish to know more about Live Formatting, including information regarding its setup, how to define a specification, and formatting system messages, you can visit the i2 product documentation . Saving the schema From the File menu, click Save Schema . Choose a location to save your schema, then click Save . The schema and charting schemes will both be saved in your chose location as XML files. Now that you have defined your schema, you need to deploy i2 Analyze ."
  },
  "content/walkthrough/2-deploy-i2-analyze.html": {
    "href": "content/walkthrough/2-deploy-i2-analyze.html",
    "title": "Deploying i2 Analyze with the i2 Connect gateway",
    "keywords": "Deploying i2 Analyze with the i2 Connect gateway This guide walks you through how to deploy i2 Analyze with the i2 Connect gateway only. To deploy i2 Analyze with the Information Store and the i2 Connect gateway, follow this guide. Prerequisites Before you start, ensure that you have: installed i2 Analyze, and defined a schema to model the data. License acknowledgement Open the license_acknowledgement file in your i2 Analyze directory. Set the value to ACCEPT . It should now look like this: LIC_AGREEMENT = ACCEPT Configuration Create the configuration directory In your i2 Analyze directory, navigate to toolkit\\examples\\configurations\\daod-opal . Copy the configuration directory to the toolkit directory. This provides a starting point for a deployment that includes only the i2 Connect gateway. Specify the credentials for deployment Using a text editor, open the toolkit\\configuration\\environment\\credentials.properties file. Specify a user name and password to use for the Solr indexes in the solr.user-name and solr.password properties. Enter the password to encrypt LTPA tokens in the ltpakeys.password property. Save and close the file. Command access control To gain access to certain features, including the ability to use Postman to reload the connector when making schema changes, you need to copy and modify some files in the i2 Analyze deployment: Navigate to the toolkit\\configuration\\examples\\security-schema directory and copy the file named example-command-access-control.xml . Navigate to the toolkit\\configuration\\fragments\\opal-services\\WEB-INF\\classes directory and paste the file from the previous step. Open the DiscoServerSettingsCommon.properties file and add the name of file you just copied to the CommandAccessControlResource field, including the .xml extension. Save and close the properties file. Configure the schema To test your schema and use it in Analyst's Notebook before deploying a connector, you can configure it as a gateway schema. The steps below describe this process. Copy your schema and charting schemes to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. In the same directory, open the ApolloServerSettingsMandatory.properties file. By default, the gateway schema and charting scheme properties look like the following: Gateway.External.SchemaResource= Gateway.External.ChartingSchemesResource= Note: The default schema short name is External . For this example, we will change that short name to NYPD-Complaints . You may choose to use a different short name as you wish. The schema short name will appear in the labels of items on a chart in Analyst's Notebook Premium. Update the schema and charting scheme properties to point to your schema files as follows: Gateway.NYPD-Complaints.SchemaResource=schema-filename.xml Gateway.NYPD-Complaints.ChartingSchemesResource=charting-schemes-filename.xml Configure the security schema All i2 Analyze deployments require a security schema, which defines the level of access users have to the data in the system. You can learn about the i2 Analyze security model in the product documentation but, for the purpose of this guide, follow the steps below to use an example security schema. Copy example-dynamic-security-schema.xml from toolkit\\configuration\\examples\\security-schema to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to the security schema by setting the following property: DynamicSecuritySchemaResource=example-dynamic-security-schema.xml Generate the default configuration For the purposes of this guide, only a basic configuration is required, so you can use the default. Open the toolkit\\scripts directory in a command prompt. To configure the environment with default property values, run: setup -t generateDefaults In the i2analyze directory, navigate to the toolkit\\configuration\\environment directory and open the topology.xml file in a text editor. Replace every instance of your machine's IP address in the host-name attribute with localhost . There will be three to change in total. This will be used to test your application in Postman. Deployment Open the toolkit\\scripts directory in a command prompt. To deploy i2 Analyze with the configuration you have just created, run setup -t deploy To add an example user whose name and password are both \"Jenny\", run: setup -t ensureExampleUserRegistry To start i2 Analyze, run setup -t start You can now connect Analyst's Notebook Premium to i2 Analyze. The output of the start command includes the URL to use for the i2 Analyze server. This will be the best time to test the schema you created in the previous step, or the existing one if you are using the example schema provided. You can open Analyst's Notebook and start to drag/drop entities and links onto your chart, testing different charting schemes and seeing what labels appear and how they are formatted. For these changes to take effect, you need to update the i2 Analyze connectors configuration and run the internal Reload command via Postman. First, navigate to i2analyze\\toolkit\\scripts in your console and run: setup -t updateConnectorsConfiguration Next you need to open Postman , select the i2 Analyze with i2 Connect Gateway environment and run the Form Based Login request. This will authenticate you as the default user in i2 Analyze, then you can run the Reload request which will configure your schema changes to the topology. (NOTE: You will need to perform a reload command for every iteration of your schema design, otherwise the changes will not take effect). You now have a running i2 Analyze deployment, if you are happy with your schema you can now deploy a connector . However, if there are changes you wish to make to the schema and associated charting schemes, you can run yourself through the schema design guide again."
  },
  "content/walkthrough/3-deploy-connector.html": {
    "href": "content/walkthrough/3-deploy-connector.html",
    "title": "Deploying a connector",
    "keywords": "Deploying a connector In this task, you deploy a minimal connector with a configuration endpoint that defines just one service. This is the minimum required for any connector to interact with the i2 Connect gateway. If you have any issues during this task, the troubleshooting guide might be helpful. Prerequisites Before you start, ensure that you have: deployed i2 Analyze with the i2 Connect gateway, and defined a schema to model the data, and installed Analyst's Notebook Premium and connected it to your i2 Analyze deployment Adding the connector to the i2 Analyze topology This is how to configure the i2 Analyze server so that it knows about your connector. Add the connector to the topology.xml In the i2analyze directory, navigate to the toolkit\\configuration\\environment directory and open the topology.xml file in a text editor. In the opal-services-daod <war> element, if it is not already there, add a <connector-ids> element. Inside that, add a <connector-id> element with a value attribute that matches the unique identifier of your connector. For example: <wars> <war ... name=\"opal-services-daod\" ... > ... <connector-ids> <connector-id value=\"nypd-connector\"/> </connector-ids> ... </war> </wars> Inside the <topology> element, add a <connectors> element. In here, add a <connector> element with id , name , and base-url attributes. Choose whatever id and name you like, but set the base-url to http://localhost:9081/ . For example: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9081/\" name=\"NYPD Connector\" id=\"nypd-connector\"/> </connectors> </ns1:topology> Note: If the schema is configured as a gateway schema, also add a gateway-schema attribute and set it to the schema short name. Reload the i2 Analyze configuration For these changes to take effect, you need to update the i2 Analyze connectors configuration and use the Admin Console to reload it. Navigate to i2analyze\\toolkit\\scripts in your console and run: setup -t updateConnectorsConfiguration Open a web browser and navigate to <i2-Analyze-URL>/admin#/connectors , where <i2-Analyze-URL> is the URL used to access your i2 Analyze deployment. For example, http://localhost:9082/opaldaod/admin#/connectors . For more information about the Admin Console, refer to i2 Analyze Server Admin Console . If you are prompted to log in, enter the credentials for your default user. If you added an example user, both username and password will be Jenny . Click Reload gateway to enact your changes to the topology. Verify the connector configuration To confirm that you successfully added a connector to the topology, you can view the status of your connectors using the Admin Console. Although you don't yet have a running connector, you can test that you have configured i2 Analyze so that it can find the one you will soon create. In the Connectors section, you can see a list of connectors that you deployed to your i2 Analyze server, and their respective statuses. For the nypd-connector , there is an error with a message saying that i2 Analyze failed to retrieve configuration information for your connector, as shown below. This is because the connector doesn't exist yet! Unsecured protocol warning message You might also see the following warning message in the Admin Console: \"Configuration error for the connector with identifier '<CONNECTOR-ID>': The application is communicating with the connector through a protocol that is not secure.\" In a development environment, you can ignore this warning. It appears because i2 Analyze and your connector are communicating via HTTP rather than HTTPS. In a production environment, you should secure this connection using HTTPS. For information about configuring the connection to use HTTPS, see Client authenticated Secure Sockets Layer with the i2 Connect gateway . Deploying the connector Now that i2 Analyze is expecting a connector, you had better create one! For the purposes of this guide, an example Spring Boot application is provided. Set up the example starting point Java Open the example starting code from the connector/nypd/java/stage1/nypd-connector folder. Copy the nypd-connector directory to wherever you would like to work, and open it in VSCode or the IDE of your choice. Have a look at the contents: Resources: config.json and application.properties Code: a simple Spring Boot application The config.json file tells Analyst's Notebook what services are available and how to execute them. For more information on providing this configuration, see Implementing config . The application.properties file defines the server port. In later walkthroughs, it also defines the resource URL and the API token required to connect to the resource. Start the connector. See running example connectors in Java . Python Open the example starting code from the connector/nypd/python/stage1/nypd-connector folder. Copy the nypd-connector directory to wherever you would like to work, and open it in VSCode or the IDE of your choice. Have a look at the contents: Resources: config.json Code: a simple Flask application. The config.json file tells Analyst's Notebook what services are available and how to execute them. For more information on providing this configuration, see Implementing config . Start the connector through the command line . Use Postman to investigate the config endpoint The supplied starting code has a config endpoint that is ready to use: In Postman , call the GET method of the config endpoint and look at the output. It matches the contents of the config.json file that you set up earlier. Update the i2 Analyze connectors configuration Now that you have a connector running, you need to tell i2 Analyze to reload its connector configurations. In the Admin Console, click Reload gateway to configure your changes. You can see that the \"Failed to retrieve configuration\" error has been replaced by two other errors. Investigate in Analyst's Notebook Premium Before you fix these issues, take a look at what happens when you try to use your connector from Analyst's Notebook Premium. Open Analyst's Notebook Premium and log in to the i2 Analyze server. In the top ribbon, click the External Searches button. Notice the banner at the top of the resulting window that says \"Some queries are not configured correctly\". This is because there are errors in your configuration, as shown in the Admin Console. Adding a service To enable the use of the connector, you must resolve all the errors displayed in the connector messages. One of the errors is caused by the connector not having any services. The next step is therefore to define and implement a service ."
  },
  "content/walkthrough/4-add-service.html": {
    "href": "content/walkthrough/4-add-service.html",
    "title": "Adding a service",
    "keywords": "Adding a service In this task, you add a simple service with the minimum configuration required for your connector to be valid. For the moment, the service returns to i2 Analyze some dummy data that you create. Later on, it will retrieve real data from the NYPD Complaint Dataset. Remember to consult the troubleshooting guide if you face any issues. Prerequisites Before starting, ensure that you have: Deployed i2 Analyze with the i2 Connect gateway Configured a basic connector Define the service To make i2 Analyze aware that your service exists, you must define it in the connector configuration returned by the config endpoint. This means editing the config.json file in the resources directory. Add a services array At the top level of the configuration, add a services array: { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [] } Add a service In the services array, define a service object. The mandatory fields to provide are the following. Field Description id The unique identifier of the service name The name of the service acquireUrl This specifies the URL for i2 Analyze to use to get data synchronously from the service. If this is present, then async must not be present. For more information about the async option, see Async Connector . clientConfigType This specifies the type of interface a user will interact with when using the service, and takes one of the three following values: NONE - there is no interface required, the service simply runs and returns whatever data it retrieves FORM - the user fills in a form to provide the service with extra information that it needs to run. This is used to provide text-search services, for example. CUSTOM - specifies that a custom client configuration will be used In the case of FORM and CUSTOM client configurations, the specific configuration to be used must be specified by the additional field clientConfigId and there must be a client configuration with the given ID defined in the connector configuration. This will be covered later. It is also common and recommended to add a description field. This appears to users in Analyst's Notebook Premium, so that it can be used to give more information about what the service does. You should have something like the following. { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"nypd-service\", \"name\": \"NYPD Connector: Get all\", \"description\": \"A service that retrieves all data.\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/all\" } ] } After defining the service, you must restart your connector. Refer to the following language guides for the steps to take: Java Python Update the connector's configuration In order for i2 Analyze to recognize these changes, you must reload the connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. If this does not add the relevant changes to Analyst's Notebook, redeploy: setup -t stop setup -t deploy setup -t start View the service in Analyst's Notebook Premium Now that you have defined a service, you can try to use it. Open Analyst's Notebook Premium and log in to the i2 Analyze server. If you were already logged in, log out and then back in again. Open External Searches. The service you have defined should appear. Try to run it. Although the service is defined, since it has not yet been implemented, you should see a red banner appear with the message \"The application failed to perform the search operation.\" To see the full error, look in the i2_Analysis_Repository.log found in deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services within the i2analyze directory. You should see a 404 (Not Found) Error. This is because an endpoint for the acquireUrl defined in the config.json has not been implemented. Implement the service Now that i2 Analyze knows to expect the service, you had better create it. Look at the SPI The SPI you need to implement is documented here . Think about the relevant objects you will need to implement in order to build a service that returns a fixed set of entities and links that you will create. Add an acquire endpoint i2 Analyze knows the acquire URL for this service. Now you need to add an endpoint for it in the connector. Java A template has been provided to get started with; see the stage2/nypd-connector directory. This includes: an example configuration config.json with a service defined some changes to ConnectorController a new class ConnectorDataService some REST transport classes. Apply these changes to your code, either manually, or by copying the files over directly. You may need to change the path of the new method in ConnectorController to match the acquireUrl of your service. You need not copy the example config.json if you have defined your service correctly. You can also just use the directory provided as it is setup to be used straight away. Look at how the endpoint is defined in the ConnectorController class and the code that produces the response that is returned to i2 Analyze. For information about the acquire endpoint, see Implementing acquire . Python A template has been provided to get started with; see the stage2/nypd-connector directory. This includes: an example configuration config.json with a service defined some additions to controller.py a new file for service functions, service.py a new file classes.py containing some REST transport classes. Apply these changes to your code, either manually, or by copying the files over directly. You may need to change the path of the new method in controller.py to match the acquireUrl of your service. You need not copy the example config.json if you have defined your service correctly. You can also just use the directory provided as it is setup to be used straight away. Look at how the endpoint is defined in the controller.py class and the code that produces the response that is returned to i2 Analyze. For information about the acquire endpoint, see Implementing acquire . Test the service in Analyst's Notebook Premium Redeploy the connector. Depending on how you are running the connector, this may be done automatically for you when changes are made. Re-run the service in Analyst's Notebook Premium via the External Searches window. You should no longer get a 404 Error, but the search will return nothing. Return some data Java Make changes so that the service returns some entities and links - just create some dummy data for now. You will need to complete the implementation of the LinkData class provided. Test your changes in Analyst's Notebook in the same way. If you don't see what you expect to, or you come across an error, investigate the i2_Analysis_Repository.log in the deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services directory. Python Make changes so that the service returns some entities and links - just create some dummy data for now. You will need to complete the implementation of the Link class in classes.py . Ensure you are importing the classes into service.py . Test your changes in Analyst's Notebook in the same way. If you don't see what you expect to, or you come across an error, investigate the i2_Analysis_Repository.log in the deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services directory. It is vital that you return property values in the format expected, which is defined by the logical type of each property. See the data model examples for examples of each supported logical type. Querying data from an external source Now that you have a basic connector with a working simple service, you can make it more useful by returning real data ."
  },
  "content/walkthrough/5-connect-to-eds.html": {
    "href": "content/walkthrough/5-connect-to-eds.html",
    "title": "Connect to an external datasource",
    "keywords": "Connect to an external datasource Here, you will connect to the NYPD Complaint Dataset as your external datasource and marshal the data into entities, links, and properties so that you can return results which can be displayed in Analyst's Notebook Premium. Again, use the troubleshooting guide if you need to. Create a Socrata app token You need an app token that will allow you to make unlimited requests to Socrata's API (within reason). If you don't use an app token, the APIs will throttle by IP address. Visit this link to register your account. After you log in, navigate to https://data.cityofnewyork.us/profile/edit/developer_settings . On subsequent visits, you can reach this page again by following these steps: Click your name in the top right of the header bar, and then select Your Profile . Click the pencil icon next to the Your Profile heading. In the side panel, click Developer Settings . At the bottom of the page, click Create New App Token , specify your own \"Application Name\" and \"Description\", and save. If you leave the site for any reason, you can always retrieve your app token again by logging into your account again. Query the external datasource Retrieve the raw data Java Look at the version of code in the stage3/nypd-connector directory. There are changes to ConnectorController and the application.properties file. Apply these to your code manually or copy over these two files. If you're copying them, you may need to change the paths of the endpoints defined in ConnectorController . In application.properties shown below, specify the NYPD Complaint Dataset API resource for the socrata.url key as the URL in the comment and your Socrata API Token for the socrata.api.token key. server.port=9081 # Resource URL, for example https://data.cityofnewyork.us/resource/7x9x-zpz6.json socrata.url= # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= You need to implement the ExternalConnectorDataService and SocrataResponseData classes so that they retrieve data from the NYPD Complaint Dataset and use it to create entities and links to be returned to i2 Analyze. It should not be necessary to modify the example SocrataClient . The dataset can be queried using SoQL (Socrata Query Language). To do this, you must construct a URL with specified parameters (if necessary) to retrieve the data. By default, a $limit parameter has been set to the value of 1 to restrict the number of records retrieved. It's best to keep this value small to reduce the response time of each request until you are more comfortable with SoQL. final Map<String, Object> params = new HashMap<>(); params.put(\"limitValue\", 1); // Only returning 1 entity for the moment. increase when ready final String url = \"?$limit={limitValue}\"; // Make the request and map the whole response body as a string so that you can // see what is returned // TODO: Remove this since it's just for debugging System.out.println(socrataClient.get(url, String.class, params)); Python Look at the version of code in the stage3/nypd-connector directory. Changes have been made to controller.py and there is now an additional resource file: application.yml . Apply these to your code manually or copy over these two files. If you're copying them, you may need to change the paths of the endpoints defined in controller.py . In application.yml shown below, specify the NYPD Complaint Dataset API resource for the socrata.url key as the URL in the comment and your Socrata API Token for the socrata.token key. socrata: url: https://data.cityofnewyork.us/resource/7x9x-zpz6.json token: # Replace with Socrata API token You need to implement the query_external_datasource function in service.py so that it retrieves data from the NYPD Complaint Dataset and uses it to create entities and links to be returned to i2 Analyze. The dataset can be queried using SoQL (Socrata Query Language). To do this, you must construct a URL with specified parameters (if necessary) to retrieve the data. By default, a $limit parameter has been set to the value of 1 to restrict the number of records retrieved. It's best to keep this value small to reduce the response time of each request until you are more comfortable with SoQL. with open('static/application.yml') as yml_file: config = yaml.safe_load(yml_file) base_url = config['socrata']['url'] api_token = config['socrata']['token'] limit = 1 request_url = f\"{base_url}?$limit={limit}\" x = requests.get(request_url, headers = { 'X-App-Token': api_token }) (Optional) Verify the data It's worth testing that you are successfully querying the data and returning results. Print the returned value to the console and check that it matches with the data you see when you make a request to the acquire endpoint via Postman. Marshal the data to objects To make it easier to create entities and links using the data retrieved, you can create a class to represent a single row of the dataset. This will have a field for each of the columns of the data. You can then write a function that serializes the incoming data into a collection of these objects. Note that in Java, there exists a library which makes this process much easier: jackson-annotations . You might want to add source references to the entities and links that are returned by your connector. This allows users trace the source of the data represented by those entities and links. For information on adding source references, see here . (Optional) Verify your marshalling function Test that you are successfully marshalling the data. You should be able to assert against the properties of your object to verify the expected and actual results are equal. Extract entities and links from objects You can create entities and links from the objects that represent rows of the dataset and define their properties using the relevant fields. Implement this extraction; deriving entities from each record as well as establishing links between them. Create a response object with a list of entities and links to be returned. You need to take care not to duplicate entities in the list. Also take care again to assign property values in the correct format. Refer to the data model examples again if you need. (Optional) Validate your return response Verify that the response returned from your function is valid and is as expected. View results in Analyst's Notebook Premium You should now be able to log into Analyst's Notebook Premium and run your query. If there are any errors, you may want to check that your schema is in the right shape, that your data is clean and that there are no missing values. Next steps Next, you can configure your own parameterized search ."
  },
  "content/walkthrough/6-parameterised-search.html": {
    "href": "content/walkthrough/6-parameterised-search.html",
    "title": "Parameterized search",
    "keywords": "Parameterized search A parameterized search passes defined conditions that you can use to drive your searches. For more general information about enabling parameterized searches, see Supporting parameters . If you have any problems during this task, remember to consult the troubleshooting guide . Configuration Add a new service You will need to add a service for parameterized searches to the services array in the config.json . For this service, you need to set the clientConfigType . In this example, you will set the value to be FORM so that you can specify your conditions via fields in a form that will be shown in Analyst's Notebook Premium. { ... \"services\": [ { \"id\": \"nypd-search-service\", \"name\": \"NYPD Connector: Search\", \"description\": \"A service for conditional searches.\", \"clientConfigType\": \"FORM\", \"clientConfigId\": \"searchForm\", \"acquireUrl\": \"/search\" } ] } Define search fields You will need to define the search fields in a clientConfigs array at the root level of your config.json . For example: { \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ { \"id\": \"searchForm\", \"config\": { \"sections\": [ { \"conditions\": [ { \"id\": \"searchTerm\", \"label\": \"made-up-field (e.g. Complaint Number)\", \"mandatory\": false, \"logicalType\": \"SINGLE_LINE_STRING\" } ] } ] } } ] } For more information on client configuration, refer to the development workflow documentation . Test search fields Check that your fields work in Analyst's Notebook Premium, you need to tell i2 Analyze to reload it's connectors' configurations, run: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. You can then: Log out and log back into Analyst's Notebook Premium to see the configuration changes and your newly-defined service. Try running it. You should receive an error as your condition fields have been defined but not yet implemented. Implementation It's time to implement these conditions. Add an acquire endpoint for your service Java i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage4/nypd-connector directory provided. This includes: An example config.json with a parameterized search service and search fields defined. This is just a template in case you have not already defined a new service Changes to the ConnectorController class Some extra REST transport classes Apply these changes to your code, either manually, or by copying the relevant files. If you are copying the files, you may need to change the path of the new method in ConnectorController . Look at how the endpoint is defined in the ConnectorController class and think about how you should implement this service. Python i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage4/nypd-connector directory provided. This includes: An example config.json with a parameterized search service and search fields defined. This is just a template in case you have not already defined a new service Changes to the controller.py file Some extra REST transport classes Apply these changes to your code, either manually, or by copying the relevant files. If you are copying the files, you may need to change the path of the new method in controller.py . Look at how the endpoint is defined in the controller.py class and think about how you should implement this service. Access conditions You will need to parse the conditions passed in the request according to the SPI and return a response containing entities and links. For an example of how your connector may receive requests, and the responses it may return, you can view some SPI examples here . You will need to create basic POJOs to parse the request and access the condition information. To do that please refer to the DaodRequest model in the SPI. The list of conditions can be accessed via request.payload.conditions . Filter data by conditions When you have a list of conditions, you can use their id and value fields to determine which of the entities retrieved from the datasource match the parameters given by the user in the form. Run your query Update the connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Open Analyst's Notebook Premium. Click on \"External Search\". Select your parameterized search service. Provide a value to the condition field and click \"Run\". You should now see a resulting list of entities that satisfy your condition. Next steps Now you can implement seeded searches ."
  },
  "content/walkthrough/7-seeded-search.html": {
    "href": "content/walkthrough/7-seeded-search.html",
    "title": "Seeded search",
    "keywords": "Seeded search A seeded search passes information from an entity on the chart to the connector so that you can use that information to drive your searches. In order to pass an entity as a seed, users can select an entity in Analyst's Notebook Premium and open \"External Search\". Again, if you face any issues during this task, remember to consult the troubleshooting guide . Configuration You need to configure a service to allow for seeded searches. Add a new service You will need to add a new service for seeded searches to the services array in the config.json . You will also need to add seedConstraints which define the entities allowed by the seeded search. For example: { \"services\": [ { \"id\": \"nypd-find-like-this-complaint-service\", \"name\": \"NYPD Connector: Find like this complaint\", \"description\": \"A service that finds a similar complaint.\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/find-like-this-complaint\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\", \"min\": 1, \"max\": 1 } ] } } }, { \"id\": \"nypd-expand-service\", \"name\": \"NYPD Connector: Expand\", \"description\": \"A service that executes an expand operation on a seed.\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/expand\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\" }, { \"id\": \"made-up-schema-type-id (e.g. ET1)\" } ] } } } ] } Check service in Analyst's Notebook Premium Check that your service now appears in the list of defined services in Analyst's Notebook Premium. Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Log out and log back into Analyst's Notebook Premium to see the configuration changes and your newly-defined service. (If the service does not show up, try unchecking the Hide queries whose requirements are not met box). Try running it by pre-clicking on an entity. You should receive an error as your seeded search has been defined but not yet implemented. Implementation It's time to implement these conditions. Add an acquire endpoint for your service Java i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage5/nypd-connector folder provided. This includes: An example config.json with a seeded search service defined. This is just a template in case you have not already defined a new service. Changes to ConnectorController Some new REST transport classes Apply the code changes, either manually or by copying the relevant files over. You may need to change the path of the new method in ConnectorController if copying the files completely. Notice how the endpoint is defined in the ConnectorController class. Python i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage5/nypd-connector folder provided. This includes: An example config.json with a seeded search service defined. This is just a template in case you have not already defined a new service. Changes to controller.py Some new REST transport classes Apply the code changes, either manually or by copying the relevant files over. You may need to change the path of the new method in controller.py if copying the files completely. Notice how the endpoint is defined in the controller.py class. Access seeds You will need to manipulate a seed passed into the request according to the SPI and return a response containing entities and links. You will also need to create basic POJOs to parse the request and access the seed information. Filter data based on seed How you will use the seed depends on what you are trying to achieve. Find Like This seeded search A Find Like This query looks at the property values of a selected record and searches for data in the external source that has the same or similar property values. For this service you will need to filter out your entities based on the matching properties of the seed entity that can be accessed via request.payload.seeds.entities.get(0) . Note: Do not return the entity that was passed as the seed . Expand seeded search An Expand query takes an entity as a seed and returns a list of entities and links that are connected to the seed. For this service you will need to find all links connected to the seed entity that can be accessed via request.payload.seeds.entities.get(0) . Then you will need to find all entities connected to these links. Finally, you need to make sure that the link is pointing to the seedId . To do that, you will need to change toEndId or fromEndId to the seedId that can be accessed via request.payload.seeds.entities.get(0).sourceIds.get(0).key.get(2) . For more information about these and other uses for seeded searches, see Supporting seeds . Run your query Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Open Analyst's Notebook Premium. Select an entity on the chart. Click on \"External Search\". Click on your seeded search service to run it. You should now see a resulting list of entities which are connected to the entity you initially selected. Next steps Next, you can combine what you've learned from these past two sections to implement seeded parameterized searches ."
  },
  "content/walkthrough/8-seeded-parameterised-search.html": {
    "href": "content/walkthrough/8-seeded-parameterised-search.html",
    "title": "Seeded parameterized search",
    "keywords": "Seeded parameterized search Simply put, a seeded parameterized search is a combination of seeded and parameterized searches. This type of search passes information from an entity on the chart to the connector together with conditions that are used to drive searches. Should you have any problems during this task, please consult the troubleshooting guide . Configuration You need to configure a service to allow for seeded parameterized searches. Add a new service You will need to set the clientConfigType and clientConfigId values similar to configuring parameterized search configuration. You will also need to add seedConstraints to define constraints on the seed similar to configuring seeded search. For example: { \"services\": [ ... { \"id\": \"nypd-expand-with-conditions\", \"name\": \"NYPD Connector: Expand with conditions\", \"description\": \"A service that executes an expand operation on a seed, with conditions.\", \"clientConfigType\": \"FORM\", \"clientConfigId\": \"expandForm\", \"acquireUrl\": \"/expand-with-conditions\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\" }, { \"id\": \"made-up-schema-type-id (e.g. ET1)\" } ] } } } ] } Optionally, you can also define the type location for the item type with the field typeLocation . This can be used to determine in which schema the item type resides. Possible value are 'CONNECTOR', 'GATEWAY' and 'INFOSTORE'. The itemTypes object will look like this: { \"seedTypes\": { ... \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\", \"typeLocation\": \"CONNECTOR\" } ] } } You will also need to provide a clientConfig with the id set in the service configuration. It should look something like this: { ... \"clientConfigs\": [ { \"id\": \"expandForm\", \"config\": { \"sections\": [ { \"conditions\": [ { \"id\": \"made-up-id (e.g. searchBorough)\", \"label\": \"made-up-field (e.g. Borough)\", \"logicalType\": \"SINGLE_LINE_STRING\" } ] } ] } } ] } You can change these conditions to relate to your own schema and what you want to search for. Check the service in Analyst's Notebook Premium Let's check that your service now appears in the list of defined services in Analyst's Notebook Premium. Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Log out and log back into Analyst's Notebook Premium to see the configuration changes and our newly-defined service. Try running it. You should receive an error as your seeded parameterized search has been defined but not yet implemented. Implementation It's time to implement the seeded parameterized search. Add an acquire endpoint for your service In the same fashion as the other services you have defined, add an acquire endpoint for this service in your controller file. Access conditions and seeds You will need to manipulate the seeds and conditions passed in the request according to the SPI and return a response containing entities and links. You will need to create basic POJOs to parse the request and access the condition and seed information contained in request.payload . Filter data based conditions and seeds An Expand With Conditions query takes an entity as a seed and returns a list of entities and links that are connected to the seed that satisfy the list of conditions provided by the user. For this service you will need to find all links connected to the seed entity that also satisfy your conditions . In the example, only links that had been created after the date provided by the user were return. Then you will need to find all entities connected to these links. Finally, you need to make sure that the link is pointing to the seedId . To do that, you will need to change toEndId or fromEndId to the seedId that can be accessed via request.payload.seeds.entities.get(0).sourceIds.get(0).key.get(2) . Run your query Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Open Analyst's Notebook. Select an entity on the chart. Click on \"External Search\". Click on your seeded parameterized search service. Provide a value to the condition field and click \"Run\". You should now see a resulting list of entities which are connected to the entity you initially selected and also satisfy the conditions you defined. Next steps Now that you've completed this, you can look into validating your requests ."
  },
  "content/walkthrough/9-validation.html": {
    "href": "content/walkthrough/9-validation.html",
    "title": "Validation",
    "keywords": "Validation You can validate requests at the gateway to ensure they are in the correct form before sending them to the respective service. For some more general information about when to validate requests, see Supporting validation . You can also consult the troubleshooting guide if needed during this task. Client-side validation Validation can be performed on the client for simple input checks before sending a request to the gateway, such as ensuring the presence of values for mandatory fields, or verifying that input values are in the correct format. Client-side validation is configured via the connector's configuration in the same way as parameterized searches. Below is an example configuration for validating that input values for the 'Offence' field start with two letters and two numbers: { \"conditions\": [ { \"id\": \"searchTerm\", \"label\": \"Offence\", \"logicalType\": \"SINGLE_LINE_STRING\", \"mandatory\": false, \"extraStringValidation\": { \"regex\": \"[A-Za-z]{2}\\\\d{2}\", \"message\": \"Case number must start with 2 letters then 2 numbers\" } } ] } The mandatory property in the config.json file specifies whether a field is required or not. The extraStringValidation property property allows regex validation be performed with custom error messages to be sent back to the client when a value does not comply with the rule. You can find out more on accepted condition properties by looking at the /CONFIGURATION_URL endpoint definition in the i2 Connect gateway REST SPI . Server-side validation Validation can be performed on the server for more complex inputs. For example, if there are 3 input fields and you require at least one to be set but they are otherwise optional. In your connector, you can check that the user has defined at least one condition in the request. In another case, if you have two date fields and want to support searching a range of dates, you can validate that the start date is before the end date. 1. Add the validation endpoint to the connector configuration In your connector configuration, add the validateUrl property after the existing acquireUrl property as shown in the snippet below. Set the value to the endpoint where you will implement your server-side validation logic. { \"id\": \"nypd-search-service\", \"name\": \"NYPD Connector: Search\", \"description\": \"A service for conditional searches.\", \"clientConfigType\": \"FORM\", \"clientConfigId\": \"searchForm\", \"acquireUrl\": \"/search\", \"validateUrl\": \"/search/validate\" } 2. Implement the validation endpoint In your code, implement the server-side logic for the validate endpoint using the conditions in the request. The payload that the endpoint receives in the request contains all the same condition and seed information that the acquire endpoint receives. If validation succeeds according to your logic, return a 200 response code. The body of the response must either be an empty object or an object containing an errorMessage with a null value. For example: { \"errorMessage\": null } If it fails, return an object containing an errorMessage with your error message: { \"errorMessage\": \"This is the error message displayed.\" } When the i2 Connect gateway receives a non-null errorMessage , it does not subsequently send a request to the acquire endpoint. More information can be found looking at the /VALIDATE_URL endpoint definition in the i2 Connect gateway REST SPI . 3. Reload the i2 Connect gateway Instruct the i2 Connect gateway to reload the configuration. Test that your new validation has the correct behavior."
  },
  "content/workflow/acquire.html": {
    "href": "content/workflow/acquire.html",
    "title": "Implementing acquire",
    "keywords": "Implementing acquire In a connector, an acquire endpoint must perform two tasks in succession. Its roles are to retrieve data from an external source, and to return that data to the i2 Connect gateway in a form that the latter can use to create i2 Analyze records. When a user runs a synchronous query against an external data source through their client application, the i2 Connect gateway makes a request to the acquire endpoint of the relevant service. The request always has a payload that can contain information (often, parameter values or seed records) that the user provided to the query. In general, your implementation of an acquire endpoint uses the contents of the payload to refine the commands that it sends to the external source. If the service that owns the endpoint specified a clientConfigType of FORM , then the identifiers of conditions in the form match the identifiers of conditions in the payload. If the service specified seedConstraints , then the payload contains the property values of seed records. For a query with no parameters and no seeds, the payload is empty. You can focus your development effort on making sure that the response from the acquire endpoint meets the requirements of the i2 Connect gateway. Note: It is not possible to follow the same procedure for every different kind of external data source. The following steps do not apply in all cases. Rather, they describe tasks that you are likely to require. In i2 Analyze Schema Designer, open the schema file that contains the definitions of the i2 Analyze entity, link, and property types that data from the external source must align with. Make a note of the identifiers and logical types of the property types of each of the entity and link types in the retrieved data. Make sure that you know which property types are mandatory. Run the command to retrieve data from the external data source: If data retrieval from the external source succeeds, store the data in a structure that you can manipulate easily, and continue with the next step in this procedure. If data retrieval from the external source fails, add the optional errorMessage field to the response from the endpoint. The errorMessage field contains a string that users of the connector see when a problem occurs. When the field is present (and not empty), the operation fails and any data in the arrays of entities and links is ignored. If the data from the external source contains information about links between entities, but the links in the data have no natural identifiers, you must manufacture those identifiers. This problem is similar to the one that you face if you prepare data for ingestion into the Information Store. Create the arrays of entity and link data objects that must appear in the response from the acquire endpoint. Populate the arrays according to the descriptions in the SPI documentation, taking care to ensure that you provide values for all mandatory properties."
  },
  "content/workflow/authentication.html": {
    "href": "content/workflow/authentication.html",
    "title": "Supporting authentication",
    "keywords": "Supporting authentication Many of the data sources that users might want to query through a connector require authentication. For example, some sources require a username-password combination, while others need the caller to provide an API key. The i2 Connect gateway supports asking users for credentials on behalf of an i2 Connect service, and management of authenticated connections during a user session. To enable authentication, a connector must specify in its configuration which of its services require authentication. If a user makes a request for an authenticated service without previously providing credentials, the gateway's response prompts the client to display a login dialog. When the user provides valid credentials, the gateway caches a token in memory that allows further requests to succeed. Configuring a service to require authentication is similar to configuring it to take parameters. In the response from the connector's configuration endpoint, you define a form that prompts the user for credentials. To support the interactions that the i2 Connect gateway expects, you must implement a login method on he connector that validates the user's credentials. You must also implement the token-handling behavior on any affected service endpoint. Note: Although both can function as ways of controlling access to i2 Connect services, supporting authentication is orthogonal to supporting user-specific configuration. Typically, user-specific configuration affects the availability (or functionality) of a service according to rules set by the i2 Analyze server, while authentication controls users' ability to use a service according to rules set by the service itself. In your response from the configuration (or user configuration) endpoint, write the service definition so that it contains an authConfigId . This identifier links to an authentication configuration elsewhere in the response. In some circumstances, you might use the same authentication configuration for more than one service. Add an authConfigs array to the response, and within it an authentication configuration object whose id matches the identifier that you specified in Step 1. This object contains all the information that the i2 Connect gateway needs to get credentials from the user and call an authentication endpoint on the connector. For example: { \"id\": \"authConfig1\", \"loginUrl\": \"/login/userpass\", \"form\": { \"description\": \"This service requires a username and a password.\", \"fields\": [ { \"id\": \"username\", \"label\": \"Username\", \"type\": \"text\" }, { \"id\": \"password\", \"label\": \"Password\", \"type\": \"password\" } ] } } Create the endpoint at the loginUrl that is defined in the authentication configuration. The syntax of this POST method is straightforward: it receives the credentials that the user provides, and (on success) responds with a token for subsequent requests to use. The detail of generating the token depends on the nature of the service. For example, you might be passing the credentials to an external provider, or implementing the authentication yourself in the service code. If authentication fails, the endpoint must instead respond with an object that complies with RFC 7807. For an example of doing so, see the file named ExternalConnectorDataService.java in the connector/auth sample project. Adapt any endpoint that requires authentication (for example, the acquire, delete, or results endpoints) to use the token mechanism. The implementations should expect requests to include an authorization header containing a token that was originally returned by the authentication endpoint, in the form Authorization: Bearer token . If the token is valid, processing of the request should proceed normally. If the token is not valid, the endpoint must respond in the same way as the authentication endpoint. The user is then required to re-authenticate with the service. Restart the i2 Analyze server or instruct the i2 Connect gateway to reload the configuration. Connect to the server with a client that supports external searches, and verify that the connector and its services behave in the way you expect."
  },
  "content/workflow/config.html": {
    "href": "content/workflow/config.html",
    "title": "Implementing config",
    "keywords": "Implementing config The configuration endpoint of a connector must respond to requests from the i2 Connect gateway with information about the schemas that the connector uses and the services that it provides. In some more complex connectors, the configuration endpoint can indicate in its response that more information is available from other endpoints. Note: Starting with version 4.4.0 of i2 Analyze, a connector can tell the gateway what version of the SPI it needs to use. If the response from your configuration endpoint does not indicate the version, the gateway assumes that the connector uses version 1.0 of the SPI. For more details, see SPI version negotiation . When the i2 Analyze server starts, the i2 Connect gateway sends a request to the configuration endpoint. The response that your connector sends back provides information to the gateway that includes a list of what services are available. i2 Analyze caches this information and provides it to clients when they connect. Note: Some connectors support user-specific configuration , where different users see different views of the services that the connector exposes. In those circumstances, a static list of services is not appropriate. Instead, the configuration endpoint provides the location of the user configuration endpoint , which provides per-user information about the connector's services. The simplest possible service does not behave differently for different users, does not support seeds, and does not require users to provide parameters. As a result, it does not require the client to display user input controls. In that case, the structure that you return from the configuration endpoint must contain only two objects: The defaultValues object. Within the defaultValues object, you must specify the time zone for the i2 Connect gateway to apply to any retrieved value that does not specify its own time zone. There are also some optional settings: You can specify a direction for any retrieved link that does not specify its own direction. You can specify the identifiers and locations of the entity and link types that the i2 Connect gateway applies to records if a service does not indicate the types of data that it can retrieve. You can provide a default setting for whether the data identifiers that the connector returns are persistent from one call to the next. Note: If you do not indicate whether these identifiers are persistent here or in the services array, the gateway assumes that the identifiers are not persistent. The services array. The services array must contain information for at least one synchronous or asynchronous service. The information must include a unique identifier and a name for the service. It must also specify whether the service requires a client UI, and the URL of the acquire or queriesResource endpoint. First, construct the defaultValues object: Determine the time zone that is most likely to apply to any temporal data in the source that the services in this connector query. Find the identifier of the time zone in the IANA Time Zone Database, and arrange for the response from the endpoint to include the identifier in its defaultValues object. For example: { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", ... }, \"services\": [ ... ] } Note: You can also retrieve a list of supported time zones from the GET method on the i2 Analyze server's /api/v1/core/temporal/timezones REST endpoint. If the source contains only a few types, and if you intend the connector eventually to have many services that run different queries, then consider adding an entityTypeId and a linkTypeId to the defaultValues object. It is more common to specify what types of record a query might retrieve on a per-service than on a connector-wide basis. However, if you do not supply default types here, then every service must supply types individually. The gateway searches for the types that you supply here in the connector schema, the gateway schema, and the Information Store schema. To specify exactly where the types are defined, you can add entityTypeLocation and linkTypeLocation properties. If you know that the source will always attach the same identifier to the same piece of retrieved information, add the resultIdsPersistent field to the defaultValues object and set its value to true . The effect of the field depends on your deployment of i2 Analyze: If the deployment contains only the i2 Connect gateway, then setting \"resultIdsPersistent\": true prevents duplication when the same record is returned twice from the same connector. Each time the record is sent to a chart, it replaces the existing version. In other scenarios (if the deployment also contains the Information Store, or if you set \"resultIdsPersistent\": false ), records can be duplicated on the chart unless you configure source identifier matching in the match rules. Note: The resultIdsPersistent field is also valid at the per-service level, where its value overrides any connector-wide setting. Then, add an object to the services array: To the services array, add a service object that has an id and a name . It is common to include a description and populate the resultItemTypeIds array as well, although neither is mandatory. For example: { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"nypd-service\", \"name\": \"NYPD Connector: Get all\", \"description\": \"A service that retrieves all data\", \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"ET2\", \"ET3\", \"LT1\", \"LT2\", \"LT3\"] }, ... } ] } For a service whose query does not allow callers to provide parameters, you can set the clientConfigType to NONE . { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"nypd-service\", \"name\": \"NYPD Connector: Get all\", \"description\": \"A service that retrieves all data\", \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"ET2\", \"ET3\", \"LT1\", \"LT2\", \"LT3\"] }, \"clientConfigType\": \"NONE\", ... } ] } If you later add parameters to the query, you can allow users to specify them by changing the value to FORM and providing the identifier of a client configuration in clientConfigId . If you later add support for seeds, you must add a seedConstraints object to the service. Finally, for a synchronous service, set the acquireUrl of the service to the URL where you intend to host the acquire endpoint. { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"nypd-service\", \"name\": \"NYPD Connector: Get all\", \"description\": \"A service that retrieves all data\", \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"ET2\", \"ET3\", \"LT1\", \"LT2\", \"LT3\"] }, \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/all\" } ] } This JSON structure represents (almost) the simplest response that you can send from the configuration endpoint of a connector. It contains the definition of one simple service. In order for the i2 Connect gateway to retrieve that definition, you must add details of the connector to the i2 Analyze deployment topology."
  },
  "content/workflow/creating.html": {
    "href": "content/workflow/creating.html",
    "title": "Development workflow",
    "keywords": "Development workflow To create a connector, you need a data source to query, and a server that supports REST endpoints to query it from. On that foundation, you can build the functionality that takes requests from the i2 Connect gateway, retrieves data from the source, and returns it to the gateway in a shape that is ready for conversion to i2 Analyze records. The easiest approach to writing a connector is to start by transferring data from the external source to the i2 Analyze server - and then to the client - with as few complications as possible. When you have that mechanism, you can use it as a base from which to develop the more complex services that you want to implement."
  },
  "content/workflow/parameters.html": {
    "href": "content/workflow/parameters.html",
    "title": "Supporting parameters",
    "keywords": "Supporting parameters In nearly all real situations, users want to be able to customize the queries that they can run against an external data source. To make that possible, you must configure the service to present parameters to its users. In your implementation of the acquire endpoint, you can act on the values that they specify. To support parameters in a service, you add a set of conditions for which users can supply values. In a connector that supports user-specific configuration, you can even arrange for the same service to support different parameters, depending on who is using it. For example, in a query for people, you might allow users to search for particular names or characteristics. In a \"find path\" seeded query, you might allow users to specify how long the path can be - and you might vary the limit according to the user's group membership. Note: A significant difference between visual query and external searches is that in the latter, conditions are not bound to property types. A condition in a service can be anything that you can process usefully in your implementation. To add parameters to a service, you add a client configuration to its definition that describes how each condition is presented to users. The REST SPI for the configuration endpoint describes the structure of clientConfig objects, which look like this outline: { \"id\": \"\", \"config\": { \"sections\": [ { \"title\": \"\", \"conditions\": [ { \"id\": \"\", \"label\": \"\", \"description\": \"\", \"mandatory\": false, \"logicalType\": \"\" } ] } ] } } The client configuration is responsible for the appearance of the conditions that users see, and the restrictions on the values they can provide. As well as controlling the types of conditions, and specifying whether supplying a value is mandatory, you can add further validation that is appropriate to their type. For more information about this kind of validation, see the REST SPI documentation for the configuration endpoint. When a user opens a query that supports parameters, they see a form that displays your conditions. When they provide values and run the query, your implementation of the acquire endpoint receives a payload that contains those values. You can write the implementation to act on those values in any way that makes sense for your data source. Like many other aspects of developing a connector, supporting parameters means changing your implementations of the configuration (or user configuration) and acquire endpoints. In your response from the configuration endpoint, write or modify the service definition so that its clientConfigType is \"FORM\", and add a clientConfigId . This identifier links to a client configuration elsewhere in the response. In some circumstances, it might be appropriate to use the same client configuration for more than one service. Add a clientConfigs array to the response, and within it a client configuration object whose id matches the identifier that you specified in Step 1. Add your conditions to the client configuration. To begin with, you might consider leaving out validation checks in the interests of getting a working implementation quickly. Add code to your implementation of the acquire endpoint that unpacks the condition values from the payload. You can then use those values to affect the query that you perform against the external source. Restart the i2 Analyze server or instruct the i2 Connect gateway to reload the configuration. You can now test the code and make sure that it does what you expect. Return to the response from the configuration endpoint, edit the condition descriptions, and add client-side validation to improve the user experience. Restart or reload again, and ensure that the validation has your intended effect. The validation that you can specify in the response from the configuration endpoint is performed by the client, and applies to values in isolation. If your service (and your users) might benefit from some more complex validation, consider adding a validate endpoint."
  },
  "content/workflow/query.html": {
    "href": "content/workflow/query.html",
    "title": "Writing a query",
    "keywords": "Writing a query All connectors contain at least one service that retrieves information from an external source. By starting with a service that runs a simple query, you can focus on aspects such as the REST endpoint implementations and data structures that are common to all services. As you create a connector to an external source, writing a query that does not require parameters makes early testing and development easier. Not only does it simplify the connector, but also it simplifies running the query from the client. No fixed set of steps for retrieving data from external data sources exists, but some of your choices can make later tasks easier. Whatever the source, your initial aim is to generate a small set of representative data. If the source allows it, begin by writing a tightly bound query. Or, if your source comprises one or more text files, try to abbreviate them. At the start of the process, large volumes of data can be distracting. If you can, make the query retrieve data for entities and links. Creating the data structures that represent linked entities is an important part of connector development. Try to generate a data set that has most of the types that the external source contains. No matter how complex your queries eventually become, the code to process data into the right shape for i2 Analyze is unlikely to change. When you implement an acquire endpoint, you can call the query that you developed here directly from that code. Alternatively, during development, you might decide to run the query and save the results in a file."
  },
  "content/workflow/seeds.html": {
    "href": "content/workflow/seeds.html",
    "title": "Supporting seeds",
    "keywords": "Supporting seeds If you configure a connector's services to enable it, users can run queries with seed records that enable or change their functionality. Your implementations of the acquire endpoint receive identity and property data from selected records that you can use to drive operations against the external data source. The behavior of a query that supports parameters is modified by values that users specify when they run it. The behavior of a query that supports seeds is modified by the records that a user selects. Seeded queries generally fall into one of a handful of categories: A \"find like this\" query looks at the property values of a selected record and searches for data in the external source that has the same or similar property values. A \"get latest\" query uses the identifying information of a selected record to look for the same data in the external source, with the aim of updating the record. An \"expand\" query uses the identifying information of a selected record to search for data in the external source that is connected to that record. A \"find path\" query receives the identifying information for exactly two selected records and searches for a path that connects those records in the external source. Seed constraints To indicate that a particular service supports seeds, you add a seedConstraints object to its definition in your response from the configuration (or user configuration) endpoint. An empty object indicates that the service supports any number of entity seeds. To change this behavior, and for more control over the constraints, you can populate the object. The REST SPI documentation for the configuration endpoint describes its structure: \"seedConstraints\": { \"connectorIds\": [\"\"], \"min\": 0, \"max\": 0, \"seedTypes\": { \"allowedTypes\": \"\", \"itemTypes\": [ { \"id\": \"\", \"min\": 0, \"max\": 0, \"typeLocation\": \"\" } ] } } A service can specify which records it accepts as seeds by restricting the connectors they came from. It can also set boundaries on the number of seed records it accepts, regardless of their type. A service can specify whether entity or link records are allowed as seeds (entity records are the default), and then restrict the range to a subset of that group. It can also set boundaries on the number of seeds, on a per-type basis. If you configure the constraints so that requests must contain at least one seed of each permitted item type, then the service requires seeds. Users cannot run the service without an appropriate selection. Otherwise, your implementation of the acquire endpoint must support requests that might or might not contain seed information. Semantic seed constraints Alternatively, instead of specifying which item types records must have in order to be seeds, a service can specify the semantic types of properties for which a seed record must have a value. In that case, the SPI and the seed constraints look a little bit different: \"seedConstraints\": { \"min\": 0, \"max\": 0, \"seedTypes\": { \"semanticPropertyTypes\": { \"semanticPropertyTypeLabel\": [\"\"] } } } If you configure semantic seeds, then each seed must satisfy the full set of at least one of the semantic property type constraints. Users cannot run the service without an appropriate selection. Note: When you specify a semantic property type as a seed constraint, the service will also accept records with values for descendants of that semantic property type as seeds. In most cases, it's better to use the most generic semantic type that is reasonable to use as a constraint. When the i2 Connect gateway calls your acquire endpoint to perform a seeded query, it includes a payload that contains identifiers and property values for all seeds. For information about the structure of the payload, see the REST SPI documentation for the request parameter of the acquire endpoint. Creating a seeded query In outline, the procedure for supporting seeded queries in one of your services is to start by adding seed constraints to its configuration. Clients then interpret the configuration and present the queries to users. When users run a query, your implementation of the acquire endpoint receives seeds that you can use to guide your response. Decide what kind of seeded operation you want to perform, and its implications for the service. For an \"expand\" query, for example, you might accept a fairly large number of entities of any type as seeds. For \"find like this\", the seed is more likely to be a single record of a specific type, or with a value for a specific semantic property type. Add a seedConstraints object that reflects the requirements of the query, to the response from the configuration (or user configuration) endpoint. Implement the acquire endpoint for your service. If at least one of the constraints does not specify a minimum record count, the endpoint might be called with or without seeds. The REST SPI documentation for the acquire endpoint describes the structure of the seed data. Important: For \"get latest\" and \"expand\" queries, it is common for seed records also to appear in the results. In that case, you must ensure that the id of the outgoing record matches the seedId of the incoming record. In an \"expand\" query, for example, you can identify a seed as being one end of a link in the response by setting the fromEndId of the link to the seedId . Restart the i2 Analyze server or instruct the i2 Connect gateway to reload the configuration. Then, connect to the server with a client, log in as a user who can see the query, and ensure that the service is working properly."
  },
  "content/workflow/testing.html": {
    "href": "content/workflow/testing.html",
    "title": "Modifying and testing",
    "keywords": "Modifying and testing A connector to an external data source is defined by the information that the i2 Connect gateway retrieves from its configuration endpoints. To add a connector, or when you modify an existing one, you must arrange for the gateway to reload the information for all the connectors in the topology. A connector that has only a configuration endpoint and an acquire endpoint is simple, but entirely functional. When you have a connector in this state, you can verify that many of the most important mechanisms are working correctly. In general, to enable faster test and development cycles, add new connectors to a running instance of i2 Analyze as soon as you can. The i2 Connect gateway maintains a cache of configuration information that it provides to clients on request. Depending on the stage of development of your connector, you can use different approaches to make the gateway update its cache: If you add or edit a <connector> element in the topology file, then you must redeploy and restart the i2 Analyze server. If you modify a connector so that the response from its configuration endpoint changes - when you add a validate endpoint, for example - you do not need to redeploy the server. Instead, you can either run the restartLiberty task from the deployment toolkit, or use the i2 Connect gateway's reload endpoint. Note: For a connector that supports user-specific configuration, the i2 Connect gateway maintains per-user caches of configuration information. Using the reload endpoint clears the caches for all users. The per-user caches are not re-created until a user makes a new request to the connector. Regardless of how you update the gateway's cache of connector configuration information, the effect for clients and their users is the same. When a client next asks for a list of the services that it can use, any changes that you made are reflected in the list. To run restartLiberty , you can use the same technique as for any other deployment toolkit task. This section describes how to use the reload endpoint, and then explains some ways to diagnose problems with a new or updated connector. The reload endpoint supports the POST method, which you can call through a command-line tool such as postman or curl , or through the i2 Analyze Server Admin Console . To trigger the reload endpoint, you can use one of the following options: Curl The endpoint requires authentication, which means that you must log in to the server and retrieve a cookie before you can POST to reload. The following steps describe how to use curl to reload the connectors: If the curl utility is not available on your server, download it from the project website at https://curl.haxx.se/download.html . Open a command window and use curl to log in to the i2 Analyze server and retrieve an authorization cookie: curl -i --cookie-jar cookie.txt -d j_username=user_name -d j_password=password http://host_name:host_port/context_root/j_security_check This command connects to the specified i2 Analyze server as the specified user, retrieves the authentication cookie, and saves it to a local file named cookie.txt . Use curl a second time to call the POST method on the reload endpoint: curl -i --cookie cookie.txt -X POST http://host_name:host_port/context_root/api/v1/connectors/reload This command causes the i2 Connect gateway to reload configuration information from all the connectors that the topology included when it was last deployed. Clients receive updated information about the connectors when they next connect to i2 Analyze. Postman To use Postman to reload the connectors, follow the steps described here . i2 Analyze Server Admin Console The following steps describe how to use the Admin Console to reload the connectors: Open a web browser and navigate to <i2-Analyze-URL>/admin#/connectors , where <i2-Analyze-URL> is the URL used to access your i2 Analyze deployment. For example, http://localhost:9082/opaldaod/admin#/connectors . If you are prompted to log in, enter the credentials for your default user. If you added an example user, both username and password will be Jenny . Click Reload gateway to reload your connectors. For more information about the Admin Console, refer to i2 Analyze Server Admin Console . After you reload the connectors (or restart the server), click Preview services in the Admin Console to check that users will see your changes in their lists of queries. If you do not see your changes, different symptoms imply different causes: If a query does not appear in the list at all, then the problem might lie with either the implementation of the configuration endpoint, or the <connector> element in the topology.xml file. If the connector supports user-specific configuration, there might be a problem with the user configuration endpoint, or you might be logged in as a user whose configuration does not list the query in question. If a query is in the list but marked as unavailable, then the cause of the problem is either the implementation of the acquire endpoint, or the specification of that endpoint in the response from the configuration endpoint. If the displayed information about a query is faulty or incomplete, look again at the definition of the corresponding service in the response from the configuration or user configuration endpoint. Provided that the queries due to new or modified connectors appear correctly, you can open a client, run them, and view the results that they return. After you test that your connector meets your requirements, you might choose to secure the connector in your deployment. You can control user access to the connector, and secure the connection between the connector and gateway."
  },
  "content/workflow/validation.html": {
    "href": "content/workflow/validation.html",
    "title": "Supporting validation",
    "keywords": "Supporting validation The support in the i2 Connect gateway for queries that take parameters includes the ability to perform relatively simple, client-side validation on the values that users supply. In cases where running a query might fail due to a set of values that are mutually incompatible, you can write an endpoint to perform server-side validation. When you write a service definition that includes conditions, you have the option (and sometimes the duty) to include logic that validates the values that users supply for those conditions. For example, you might insist that a string is shorter than a certain length, or that its contents match a particular pattern. If you ask the user to select from a discrete set of values, then you must provide the values for them to select from. However, there are other kinds of validation that the mechanism for defining conditions does not support. In particular, you cannot use it to validate values that are reasonable in isolation, but faulty in combination. (For example, dates of birth and death might both contain reasonable values, but it would make no sense to search for individuals where the latter is earlier than the former.) If you have an implementation of the acquire endpoint for which it might be useful to perform this kind of validation, you can write a validate endpoint. When your configuration specifies a validate endpoint for a service, the gateway uses it before the acquire endpoint, and passes the same payload. If you decide that validation fails, the request to the acquire endpoint does not happen, and the user sees an error message that you specify. To add a validate endpoint to a service: In your response from the configuration (or user configuration) endpoint, add the validateUrl setting alongside the existing acquireUrl setting. Set its value to the location of the implementation. Implement the rules for the validate endpoint in a way that is consistent with its definition in the REST SPI documentation. The payload that the endpoint receives in the request contains all the same seed and parameter information as the acquire endpoint receives. If validation succeeds according to your rules, return an empty response. If it fails, set the response to a simple object that contains an errorMessage : { \"errorMessage\": \"\" } When the i2 Connect gateway receives a response that is not empty, it does not then send a request to the acquire endpoint. Restart the i2 Analyze server or instruct the i2 Connect gateway to reload the configuration. Test that your new validation has the correct behavior."
  },
  "index.html": {
    "href": "index.html",
    "title": "i2 Connect example connectors",
    "keywords": "i2 Connect example connectors To create a connector, you need a data source to query, and a server that supports REST endpoints to query it from. On that foundation, you can build the functionality that takes requests from the i2 Connect gateway, retrieves data from the source, and returns it to the gateway in a shape that is ready for conversion to i2 Analyze records. Developing a connector walkthrough The documentation for understanding, deploying and configuring both i2 Analyze and an example connector is divided into the following sections: Building an i2 Analyze schema and working with the i2 Connect gateway Deploying i2 Analyze Deploying a connector Creating a service Querying an external data source Implementing parameterized searches Implementing seeded searches Implementing seeded, parameterized searches Validating requests Further materials for connector development: Asynchronous services Authentication User-specific configuration Custom HTTP headers Semantic seeds Environment setup The workshop requires the following tools and technologies: i2 Analyze 4.4.1 : The server that hosts the i2 Connect gateway. i2 Analyst's Notebook Premium 9.4.1 : The client that provides the user interface for interacting with your connector. The client displays the resulting entity and link records with their properties. i2 Analyze Schema Designer : The tool that enables you to design and create your i2 Analyze schema. During installation of Analyst's Notebook Premium, ensure that you also install Schema Designer. Microsoft Visual Studio Code : The recommended IDE for developing your connector. You can use any IDE you like, but the Visual Studio Code Spring Boot Dashboard plugin can handle running and redeploying the connector for you. Download it here . Postman : An API development tool that you can use to create and execute requests against REST endpoints. Download Postman here . An application framework dependent on the language you are writing the connector with. Java (Spring Boot) : The connector is written in Java with Spring Boot. Spring Boot is a Spring framework with an embedded Tomcat server that makes it easy to spin up a web application. No installation is required. Python (Flask) : The Python connector is written and run on a Flask application. Flask is a micro web framework which requires no particular tools or libraries. No manual installation other than the specified setup instructions is required as the Pipfile contains it as a dependency. Additional resources To help you to understand some of the tools and technologies you will use, here are a few resources that contain more information about them: i2 Analyze data model examples Example requests and responses Using Postman Running the example connectors: Java Python Source identifiers Semantic type identifiers Source references SPI version negotiation Troubleshooting Schemas and schema types The documentation for understanding and configuring connector schemas, gateway schemas, type locations, item type mappings, and item type security is divided into the following sections: Connector schemas Gateway schemas Type location Migrating from schema fragments The example connectors used are: NYPD Connector KCPD Connector ERI Connector SPI version information New versions of the REST SPI are always released as part of new i2 Analyze releases. Documentation for the latest version of the SPI is available through the REST SPI link at the top of the page. SPI version  i2 Analyze version  1.0 1.1 4.4.0   4.4.1   SPI changelog 1.0 changes 1.1 changes"
  }
}