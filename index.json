{
  "content/example-connectors/connector-async.html": {
    "href": "content/example-connectors/connector-async.html",
    "title": "Async Connector",
    "keywords": "Async Connector The Async connector demonstrates the ability for a connector service to query data asynchronously. Asynchronous queries allow you to run multiple queries concurrently, and allow long running queries to persist after the user logs out. The example connector connects to a JSON file that is populated with sample data (people and their friends) and marshals the data into entities, links and properties. There are a number of fields in the dataset: Field Name Type id String forename String surname String dob String ssn String issuedDateAndTime String friends Array of Strings image String Data model The Async schema is modelled on the fields of the people.json data source which can be found in async\\async-connector\\src\\main\\resources . Each entry in the people object is represented by a Person entity and a Friends With link to their friends alongside the properties that are extracted from each field. The schema (and charting schemes) for the Async connector are in the schema directory of this repository. Entity: Person Represents a person in the people object. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Property Type Logical Type Derived From First Name SINGLE_LINE_STRING forename Last Name SINGLE_LINE_STRING surname Year of Birth DATE dob Social Security Number SINGLE_LINE_STRING ssn SSN Issued Date and Time DATE_AND_TIME issuedDateAndTime Link Establishes a connection between two Person entities. Link Type Link Ends Friends With Person --> Person Adding an async service 1. Adding the async service For a general understanding about how to add a service, see Adding a service . To define a service as asynchronous, the service object must be updated. Field Description async Indicates that the service must be called asynchronously, and provides configuration settings. If this is present, then acquireUrl must not be present. Asynchronous configuration Field Description queriesResource This specifies the URL for i2 Analyze to use to get data asynchronously from the service. pollingIntervalInSeconds (Optional) The recommended interval at which clients should poll asynchronous endpoints for changes. The connector configuration matches the example below when it contains an async service. The full configuration of the example asynchronous connector is in connector\\async\\async-connector\\src\\main\\resources\\config.json . { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"sample-async-service\", \"name\": \"Sample Connector: Async\", \"description\": \"A sample service that runs the sevice asynchronously\", \"async\": { \"queriesResource\": \"/async\", \"pollingIntervalInSeconds\": 1 }, \"clientConfigType\": \"NONE\", } ], \"clientConfigs\": [] } 2. Implementing the acquire endpoint This endpoint starts an asynchronous query to fetch data from the service in the specified query resource. If the service in question also specifies the validateUrl property, then the gateway uses the validateUrl endpoint before it uses the queriesResource endpoint. This service calls the POST method on the {queriesResource} endpoint and takes in a request parameter. It is a payload that the service can interpret to modify its behaviour. The service can have one of three types of client configuration: 'NONE' - the payload never contains conditions. 'FORM' - it contains conditions that have a fixed structure. 'CUSTOM' - the contents of the conditions object are free-form. The response must contain a query ID that the client can use in subsequent requests to retrieve further information. A sample solution has been written in Java and can be found in the asyncAcquireService() method in ConnectorController with it being implemented in asyncAcquire() in ExternalConnectorDataService . 3. Implementing the status endpoint This endpoint fetches the status of the specified asynchronous query from the specified queries resource. After a client starts an asynchronous query, it uses polling to determine its progress. The client polls the i2 Connect gateway, and the gateway calls through to the connector to retrieve its status. This service calls the GET method on the {queriesResource}/{queryId} endpoint. It queries that endpoint using the provided queryId to identify the current state of the query. The response must contain one of three state values that indicate the status of the query: 'STARTED', 'SUCCEEDED', or 'FAILED'. The response can also contain a series of substatuses that report progress from the underlying data source. The four valid types of substatus are 'INFORMATION', 'WARNING', 'ERROR', and 'SUCCESS'. A sample solution has been written in Java and can be found in the asyncStatusService() method in ConnectorController with it being implemented in asyncStatus() in ExternalConnectorDataService . 4. Implementing a result endpoint This endpoint fetches the results of the specified asynchronous query from the specified queries resource. Clients can attempt to retrieve results only from an asynchronous query whose state is 'SUCCEEDED'. An attempt to fetch results from a query in any other state must fail. This service calls the GET method on the {queriesResource}/{queryId}/results endpoint. It queries that endpoint using the provided queryId to fetch the results of the successful query. The structure of the response is identical to the response from a synchronous query. A sample solution has been written in Java and can be found in the asyncResultsService() method in ConnectorController with it being implemented in asyncResults() in ExternalConnectorDataService . 5. Implementing a delete endpoint This endpoint deletes the specified asynchronous query from the specified queries resource when it is no longer needed. The i2 Connect gateway calls the DELETE method when a query has succeeded, been cancelled, or failed. A connector can respond to the call by cleaning up any resources associated with processing the query. This service calls the DELETE method on the {queriesResource}/{queryId} endpoint. It queries that endpoint using the provided queryId to cancel the query and delete the queryId . The response must be returned with the queryId removed. A sample solution has been written in Java and can be found in the asyncDeleteService() method in ConnectorController with it being implemented in asyncDelete() in ExternalConnectorDataService . Setup These instructions are for setting up and running the Async connector. The solution uses a client configuration of type FORM to demonstrate the use of an async query. The duration condition is used to demonstrate the time taken for a long running query to be executed. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the Async connector: <wars> <war ... name=\"opal-services-daod\" ... > ... <connector-ids> <connector-id value=\"async-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9085\" name=\"Async Connector\" id=\"async-connector\"/> </connectors> </ns1:topology> Ensure that you are using the same port as specified in application.properties ( 9085 by default) and that the value of the id attribute is the same as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the Async schema as a connector schema or a gateway schema. Connector schema By default, your connector is configured as a connector schema. This is because schemaUrl , chartingSchemesUrl and schemaShortName are defined in the Async connector's config.json in async\\async-connector\\src\\main\\resources . Additionally, ensure the following is true: The connector's config.json does not contain a gatewaySchema property. The Async <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. For more information, see configuring a connector schema . Gateway schema If you want to set up the Async schema as a gateway schema, follow the guidelines for configuring a gateway schema using the Async schema and charting scheme in the schema directory of this repository. Additionally, ensure the following is true: The connector's config.json in async\\async-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default. If they are present, remove them. The Async <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. Run the Async connector To run the connector, navigate to connector\\async\\async-connector in your terminal and run the application using the following command: mvnw spring-boot:run 4. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-eri.html": {
    "href": "content/example-connectors/connector-eri.html",
    "title": "Emergency Response Incidents (ERI) Connector",
    "keywords": "Emergency Response Incidents (ERI) Connector The ERI connector connects to the Emergency Response Incidents Dataset and marshals the data into entities, links and properties. The dataset contains types and locations of incidents which the Office of Emergency Management of New York City have responded to. There are seven fields in the dataset: Column Name Type Incident Type Plain Text Location Plain Text Borough Plain Text Creation Date Date & Time Closed Date Date & Time Latitude Number Longitude Number Data model The ERI schema models the Emergency Response Incidents Dataset by using all seven fields. Each row of data can be represented by two entities (Incident and Location) and a single link between them (Located At) alongside properties extracted from each field. The schema (and charting schemes) for the ERI connector can be found in the schema directory of this repository. Entity: Incident Represents a reported incident. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Property Type Logical Type Derived From Incident Type SELECTED_FROM \\ * Incident Type Incident Subtype SINGLE_LINE_STRING Incident Type Creation Date DATE Creation Date Creation Time TIME Creation Date Closed Date DATE Closed Date Closed Time TIME Closed Date \\ * The possible values for Incident Type are: Administration, Aviation, Fire, HazMat, Law Enforcement, Marine, Medical, Rescue, Structural, Transportation, Utility, Weather and Other . Entity: Location Represents the location at which the incident occurred. Property Type Logical Type Derived From Borough SINGLE_LINE_STRING Borough Address SINGLE_LINE_STRING Location Coordinates GEOSPATIAL Longitude & Latitude Link Associates an incident with the location at which it is reported to have occurred. Link Type Link Ends Located At Incident -> Location Setup These instructions are for setting up and running the ERI connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology: In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the ERI connector: <wars> <war ... > ... <connector-ids> <connector-id value=\"eri-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9084\" name=\"ERI Connector\" id=\"eri-connector\"/> </connectors> </ns1:topology> Ensure that you're using the same port as specified in application.properties ( 9084 by default) and that the value of the id attribute is the name as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the ERI schema as a connector schema or a gateway schema. Connector schema By default, your connector is configured to use a connector schema. This is due to the presence of the schemaUrl , chartingSchemesUrl and schemaShortName in the ERI connector's config.json in eri\\eri-connector\\src\\main\\resources . Additionally, ensure the following: The connector's config.json does not contain a gatewaySchema property; The ERI <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. See for more information on configuring a connector schema . Gateway schema If you want to set up the ERI schema as a gateway schema, follow the guidelines for configuring a gateway schema using the ERI schema and charting scheme found in the schema directory of this repository. Additionally, ensure the following is true: The connector's config.json in eri\\eri-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default; if they are present, remove them. The ERI <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. Acquire Socrata token In order to query the external datasource, a Socrata app token is required. If you do not already have a Socrata app token, you will need to generate one. Instructions on how to generate this token can be found here . In the ERI connector's application.properties file at connector\\eri\\eri-connector\\src\\main\\resources , add your token. server.port=9084 socrata.url=https://data.cityofnewyork.us/resource/pasr-j7fb.json # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= 4. Run the ERI connector To run the connector, navigate to connector\\eri\\eri-connector in your terminal and run the application using the following command: mvnw spring-boot:run For more information about running this repository's Java connectors, see running example connectors in Java . 5. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-kcpd.html": {
    "href": "content/example-connectors/connector-kcpd.html",
    "title": "Kansas City Police Department (KCPD) Connector",
    "keywords": "Kansas City Police Department (KCPD) Connector The KCPD connector connects to the KCPD Crime 2020 Dataset as the external datasource and marshals the data into entities, links and properties. This dataset contains reports on the criminal incidents which the Kansas City Police Department of Missouri have recorded. There are a number of fields in the dataset: Column Name Type Report_No Plain Text Reported_Date Date & Time Reported_Time Plain Text From_Date Date & Time From_Time Plain Text To_Date Date & Time To_Time Plain Text Offense Plain Text IBRS Plain Text Description Plain Text Beat Plain Text Address Plain Text City Plain Text Zip Code Plain Text Rep_Dist Plain Text Area Plain Text DVFlag Plain Text Involvement Plain Text Race Plain Text Sex Plain Text Age Number Firearm Used Flag Checkbox Location Location Data model The KCPD schema models the KCPD Crime 2020 Dataset using its relevant fields. Each row of data can be represented by three entities (Report, Person and Location) and a number of appropriate links between them alongside properties extracted from each field. The schema (and charting schemes) for the KCPD connector can be found in the schema directory of this repository. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Entity: Report Represents a report about a crime. Property Type Logical Type Derived From Report Number SINGLE_LINE_STRING Report_No Report Date DATE Reported_Date From Date DATE From_Date To Date DATE To_Date From Time TIME From_Time To Time TIME To_Time Offense SINGLE_LINE_STRING Offense Offense Description SINGLE_LINE_STRING Description Domestic Violence BOOLEAN DVFlag Entity: Person Represents a person somehow involved in a reported crime. Property Type Logical Type Derived From Race SINGLE_LINE_STRING Race Sex SUGGESTED_FROM Sex Age SUGGESTED_FROM Age Entity: Location Represents a location at which a reported crime occurred. Property Type Logical Type Derived From City SINGLE_LINE_STRING City Address SINGLE_LINE_STRING Address Zip Code INTEGER Zip Code Coordinates GEOSPATIAL Location Links Establishes some connection between a Report, a Location and a Person. The KCPD Dataset's Involvement field is used to determine how a Person is linked to a Report. Link Type Link Ends Located At Report -> Location Suspect Of Person -> Report Victim Of Person -> Report Complicit In Person -> Report Arrested Person -> Report Charged Person -> Report Setup These instructions are for setting up and running the KCPD connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway or deploying i2 Analyze with the Information Store and the i2 Connect gateway and have not previously done so, you must do so now. 1. Add connector to topology: In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the KCPD connector: <wars> <war ... > ... <connector-ids> <connector-id value=\"kcpd-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> element to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9083\" name=\"KCPD Connector\" id=\"kcpd-connector\"/> </connectors> </ns1:topology> Ensure that you're using the same port as specified in application.properties ( 9083 by default) and that the value of the id attribute is the name as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the KCPD schema as a connector schema or a gateway schema. Connector schema By default, your connector will be configured to use a connector schema. This is due to the presence of the schemaUrl , chartingSchemesUrl and schemaShortName in the KCPD connector's config.json in kcpd\\kcpd-connector\\src\\main\\resources . Additionally, ensure the following: The connector's config.json does not contain a gatewaySchema property; The KCPD <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. See for more information on configuring a connector schema . Gateway schema If you want to set up the KCPD schema as a gateway schema, follow the guidelines for configuring a gateway schema using the KCPD schema and charting scheme found in the schema directory of this repository. Additionally, ensure the following: The connector's config.json in kcpd\\kcpd-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default; if they are present, remove them. The KCPD <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. 3. Acquire Socrata token In order to query the external datasource, a Socrata app token is required. If you do not already have a Socrata app token, you will need to generate one. Instructions on how to generate this token can be found here . In the KCPD connector's application.properties file at connector\\kcpd\\kcpd-connector\\src\\main\\resources , add your token. server.port=9083 socrata.url=https://data.kcmo.org/resource/vsgj-uufz.json # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= 3. Run the KCPD connector To run the connector, navigate to connector\\kcpd\\kcpd-connector in your terminal and run the application using the following command: mvnw spring-boot:run For more information on running this repository's Java connectors, see running example connectors in Java . 4. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/connector-nypd.html": {
    "href": "content/example-connectors/connector-nypd.html",
    "title": "New York Police Department (NYPD) Connector",
    "keywords": "New York Police Department (NYPD) Connector The NYPD connector connects to the NYPD Complaint Data and marshals the data into entities, links and properties. The dataset contains information on the felony, misdemeanor, and violation crimes reported to the New York City Police Department. There are a number of fields in the dataset, of which the following were used: Column Name Type CMPLNT_NUM (Complaint Number) Number ADDR_PCT_CD (Precinct Code) Number BORO_NM (Borough Name) Plain Text CMPLNT_FR_DT (Complaint From Date) Date & Time CMPLNT_FR_TM (Complaint From Time) Plain Text CMPLNT_TO_DT (Complaint To Date) Date & Time CMPLNT_TO_TM (Complaint To Time) Plain Text CRM_ATPT_CPTD_CD (Crime Atempted-Completed Code) Plain Text HADEVELOPT (Housing Development) Plain Text JURISDICTION_CODE (Jurisdiction Code) Number JURIS_DESC (Jurisdiction Description) Plain Text KY_CD (Key Code) Number LAW_CAT_CD (Law Category Code) Plain Text LOC_OF_OCCUR_DESC (Location of Occurence Description) Plain Text OFNS_DESC (Offense Description) Plain Text PARKS_NM (Park Name) Plain Text PATROL_BORO (Patrol Borough) Plain Text PD_CD (PD Code) Number PD_DESC (PD Description) Plain Text PREM_TYPE_DESC (Premise Type Description) Plain Text RPT_DT (Report Date) Date & Time STATION_NAME (Station Name) Plain Text SUSP_AGE_GROUP (Suspect's Age Group) Plain Text SUSP_RACE (Suspect's Race) Plain Text SUSP_SEX (Suspect's Sex) Plain Text TRANSIT_DISTRICT (Transit District) Number VIC_AGE_GROUP (Victim's Age Group) Plain Text VIC_RACE (Victim's Race) Plain Text VIC_SEX (Victim's Sex) Plain Text Latitude Number Longitude Number Data model The NYPD schema models the NYPD Complaint Dataset by using its relevant fields. Each row of data can be represented by three entities (Complaint, Person and Location) and a number of appropriate links between them alongside properties extracted from each field. The schema (and charting schemes) for the NYPD connector can be found in the schema directory of this repository. Where Property Type is the name of the schema property, Logical Type is the property's data type, and Derived From is the field of the external dataset where the property is derived from. Entity: Complaint Represents a crime complaint. Property Type Logical Type Derived From Complaint Number SINGLE_LINE_STRING CMPLNT_NUM Complaint Start Date DATE CMPLNT_FR_DT Complaint End Date DATE CMPLNT_TO_DT Complaint Start Time TIME CMPLNT_FR_TM Complaint End Time TIME CMPLNT_TO_TM Crime Status SUGGESTED_FROM CRM_ATPT_CPTD_CD Jurisdiction Code INTEGER JURISDICTION_CODE Jurisdiction Description SINGLE_LINE_STRING JURIS_DESC Offence Classification Code SINGLE_LINE_STRING KY_CD Level of Offence SUGGESTED_FROM LAW_CAT_CD Offence Description SINGLE_LINE_STRING OFNS_DESC Internal Classification Code INTEGER PD_CD Classification Description SINGLE_LINE_STRING PD_DESC Event Date DATE RPT_DT Location of Occurrence SUGGESTED_FROM LOC_OF_OCCUR_DESC Entity: Location Represents the location of a reported crime. Property Type Logical Type Derived From Precinct Code INTEGER ADDR_PCT_CD Borough Name SINGLE_LINE_STRING BORO_NM Housing Development SINGLE_LINE_STRING HADEVELOPT Park Name SINGLE_LINE_STRING PARKS_NM Patrol Borough SINGLE_LINE_STRING PATROL_BORO Premises Description SINGLE_LINE_STRING PREM_TYP_DESC Station Name SINGLE_LINE_STRING STATION_NAME Transit District INTEGER TRANSIT_DISTRICT Coordinates GEOSPATIAL Latitude & Longitude Entity: Person Represents a person somehow involved in a reported crime. Since each record of the NYPD Complaint Dataset has information on both victims and suspects, two Person entities are created from a single record. Property Type Logical Type Derived From Age Group SUGGESTED_FROM SUSP_AGE_GROUP or VIC_AGE_GROUP Race SINGLE_LINE_STRING SUSP_RACE or VIC_RACE Sex SUGGESTED_FROM SUSP_SEX or VIC_SEX Links Establishes some connection between a Complaint, a Location and a Person. Link Type Link Ends Located At Complaint -> Location Suspect Of Person -> Complaint Victim Of Person -> Complaint Setup These instructions are for setting up and running the NYPD connector. If you are not familiar with deploying i2 Analyze with the i2 Connect gateway and have not previously done so, you must do so now. <!-- TODO: Doc review this line--> If you want to configure your connector to use an Information Store schema, you must deploy i2 Analyze with the Information Store and the i2 Connect gateway . 1. Add connector to topology: In your topology.xml file in toolkit\\configuration\\environment , add a new <connector-id> element for the NYPD connector: <wars> <war ... > ... <connector-ids> <connector-id value=\"nypd-connector\"/> </connector-ids> ... </war> </wars> Additionally, add a new <connector> to the topology: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9081\" name=\"NYPD Connector\" id=\"nypd-connector\"/> </connectors> </ns1:topology> Ensure that you're using the same port as specified in application.properties ( 9081 by default) and that the value of the id attribute is the name as the value attribute of its corresponding <connector-id> . 2. Configure the schema Choose whether you want to configure the NYPD schema as a connector schema, a gateway schema, or an Information Store schema. Connector schema By default, your connector will be configured as a connector schema. This is due to the presence of the schemaUrl , chartingSchemesUrl and schemaShortName in the NYPD connector's config.json in nypd\\nypd-connector\\src\\main\\resources . Additionally, ensure the following: The connector's config.json does not contain a gatewaySchema property; The NYPD <connector> element in your i2 Analyze topology does not contain a gateway-schema attribute. See for more information on configuring a connector schema . Gateway schema If you want to set up the NYPD schema as a gateway schema, follow the guidelines for configuring a gateway schema using the NYPD schema and charting scheme found in the schema directory of this repository. Additionally, ensure the following: The connector's config.json in nypd\\nypd-connector\\src\\main\\resources does not contain the schemaUrl , chartingSchemesUrl and schemaShortName properties. These properties exist on the configuration by default; if they are present, remove them. The NYPD <connector> element in your i2 Analyze topology does not contain a schema-short-name attribute. <!-- TODO: Doc review this section--> Information Store schema If you want to set up the NYPD schema as an Information Store schema: Copy the NYPD schema and charting scheme found in the schema directory of this repository to the configuration/fragments/common/WEB-INF/classes directory in your i2 Analyze configuration. Open the ApolloServerSettingsMandatory.properties file in the same directory, and update the following properties to point to the relevant files SchemaResource=nypd-complaint-data-schema.xml ChartingSchemesResource=nypd-complaint-data-schema-charting-schemes.xml Additionally, ensure the following: The connector's config.json in nypd\\nypd-connector\\src\\main\\resources does not contain the following properties that exist by default; if they are present, remove them: schemaUrl chartingSchemesUrl schemaShortName gatewaySchema The NYPD <connector> element in your i2 Analyze topology does not contain the following attributes: schema-short-name gateway-schema 3. Acquire Socrata token In order to query the external datasource, a Socrata app token is required. If you do not already have a Socrata app token, you will need to generate one. Instructions on how to generate this token can be found here . In the NYPD connector's application.properties file at connector\\nypd\\nypd-connector\\src\\main\\resources , add your token. server.port=9081 socrata.url=https://data.cityofnewyork.us/resource/7x9x-zpz6.json # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= 3. Run the NYPD connector To run the connector, navigate to connector\\nypd\\nypd-connector in your terminal and run the application using the following command: mvnw spring-boot:run For more information on running this repository's Java connectors, see running example connectors in Java . 4. Deploy and start i2 Analyze Deploy and start the Liberty server. setup -t deployLiberty setup -t startLiberty"
  },
  "content/example-connectors/run-java.html": {
    "href": "content/example-connectors/run-java.html",
    "title": "Running a Java connector via the command-line",
    "keywords": "Running a Java connector via the command-line The example Java connector is a simple Spring Boot application. Here's how to run it via the command-line. (You also have the option to run it directly in VSCode.) In the project directory (where the mvnw file is), use the commands below to get your connector up-and-running. There are two ways to go about this. We recommend using the run command, for the reasons detailed below. Run Use the following command to start the connector and run it until you press Ctrl-C . With the Spring Boot Devtools plugin, which we are using in this project, code and resource changes are detected automatically while using this command, and the application is automatically restarted. This is incredibly useful when making changes, since you can see the results almost instantaneously. mvnw spring-boot:run Start and stop To start the connector application and return to the command prompt, use: mvnw spring-boot:start You will then need to stop the connector application manually using mvnw spring-boot:stop Running the connector in VS Code To run your connector directly from VS Code, ensure you have installed the Spring Boot Extension Pack for VSCode, either via the Extensions menu in VSCode, or using this link Then, with your project directory open in VS Code: There should be a Spring Boot Dashboard panel at the bottom, where the \"demo\" app should appear. Right-click on the 'demo' app and click Run. The resulting output in the debug console should print the URL for the connector The app automatically restarts whenever files are updated, which helps speed up the development/test cycle. The screenshot below shows the Spring Boot Dashboard panel with the \"demo\" app appearing where you should expect to see it."
  },
  "content/example-connectors/run-node.html": {
    "href": "content/example-connectors/run-node.html",
    "title": "Running the connector via the command-line",
    "keywords": "Running the connector via the command-line To run your connector from the command-line: Open the command-line. Navigate to the root directory of the project. Install all dependencies for the project by running: npm install Start the connector by running: npm start To stop the connector press CTRL + C ."
  },
  "content/example-connectors/run-python.html": {
    "href": "content/example-connectors/run-python.html",
    "title": "Running the Python connector",
    "keywords": "Running the Python connector The example Python connector is a simple Flask application built on Python 3.7.4. It is imperative that you have Python installed and added to PATH . Older versions of Python may not work. In the project directory (where the Pipfile is), use the commands below to install dependencies and get your connector up-and-running. Installation If you're running Windows, you can install all required dependencies and create a virtual environment by running this setup batch script in the nypd-connector working directory: ./setup.bat Alternatively, run this sequence of the instructions (that the above batch script performs): Install pipenv , the virtual environment and package manager. pip install pipenv Activate your virtual environment. pipenv --python [python-version] Synchronize the environments dependencies with that of the Pipfile. pipenv sync You'll need to do this each time you're in a new Python working directory to ensure dependencies are installed and that your virtual environment is activated. Starting and stopping the application To start the connector application, run the following command in the working directory: pipenv run python app.py Simply press Ctrl+C to stop the application."
  },
  "content/miscellaneous/data-model.html": {
    "href": "content/miscellaneous/data-model.html",
    "title": "The i2 Analyze data model",
    "keywords": "The i2 Analyze data model All the properties defined in a schema, of both entites and links, have what is called a logical type. This defines the kind of data that a property represents and the format of it. Some properties, for example, respresent numerical data, whereas others represent textual data. Furthermore, of those properties representing numerical data, some will be integer-valued, whereas others will have floating-point values. We use logical types to make these distinctions, and there are fifteen of them: SINGLE_LINE_STRING MULTIPLE_LINE_STRING DATE TIME DATE_AND_TIME BOOLEAN INTEGER DOUBLE DECIMAL DOCUMENT* XML* PICTURE* SELECTED_FROM SUGGESTED_FROM GEOSPATIAL The i2 Connect gateway **does not * support the returning of properties with logical type XML, DOCUMENT or PICTURE. To see the logical type of a property, open the schema in IBM i2 Analyze Schema Designer and click on a property in the left-hand pane. You should see its logical type, along with its ID, name, and description. When returning entities and links to i2 Analyze from a connector, we must ensure that the property values we assign align with the logical type of those properties. For example, the i2 Analyze server would complain if we tried to assign the value \"fourty-two\" to a property representing the age of a person, that has logical type INTEGER. \"fourty-two\" is a string and should perhaps be changed to 42. See examples below of valid data for each of the valid logical types. Examples Logical Type Example(s) SINGLE_LINE_STRING \"Up to 250 bytes of UTF-8 characters\" MULTIPLE_LINE_STRING \"Up to 32700 bytes of UTF-8 characters\" DATE \"2019-11-18\" TIME \"16:34:11\" or \"16:34\" . Seconds are optional, but hours and minutes are not. Greater precision than seconds is not supported. DATE_AND_TIME Four pieces of information are required to specify a particular point in time: The date The time as it would appear on a clock The time zone in which the point in time is observed Daylight Saving Time: If the point in time occurs during the hour that is repeated when clocks are turned back to end Daylight Saving Time, whether to use the first or second occurence of this time. There are two ways of providing this information. Provide just the date and time, in which case the time zone is taken to be the default time zone defined in the connector configuration. This must be specfied as a string conforming to ISO 8601 with at least minute-precision and up to millisecond-precision. For example: \"2019-10-29T14:23:55\" \"2019-10-29T14:23:55.1\" \"2019-10-29T14:23:55.123\" Provide all four pieces of information in the form {\"localDateAndTime\": \"2019-10-29T14:23:55.487\", \"timeZoneId\": \"Australia/Perth\", \"isDST\": true} , where: localDateAndTime is a timestamp string conforming to ISO 8601 as above, again with at least minute-precision and up to millisecond-precision. timeZoneId specifies the time zone in which the moment of time is observed. For a list of valid time zone IDs, see the IANA Time Zone Database . isDST is used to specify which occurence of the repeated hour to use if the point in time occurs when the clocks turn back at the end of Daylight Saving Time. To use the first, set it to true ; to use the second, set it to false . BOOLEAN true or false INTEGER 42 , -9812 DOUBLE -213.89763 , 0.00083 DECIMAL 99.9999 (maximum of four digits after decimal point) SELECTED_FROM \"Red\" , \"Green\" , or \"Blue\" if choosing from a set of colors, for example. SUGGESTED_FROM \"Blue-ish green\" , if you need to be more specific than the suggested options. GEOSPATIAL {\"type\": \"Point\", \"coordinates\": [11.1, 12.2]} , where the coordinates are latitude and longitude."
  },
  "content/miscellaneous/deploy-i2-analyze-is-daod.html": {
    "href": "content/miscellaneous/deploy-i2-analyze-is-daod.html",
    "title": "Deploying i2 Analyze with the Information Store and the i2 Connect gateway",
    "keywords": "Deploying i2 Analyze with the Information Store and the i2 Connect gateway <!-- TODO: Doc review--> This guide walks you through how to deploy IBM i2 Analyze with the Information Store and the i2 Connect gateway. To deploy IBM i2 Analyze with the i2 Connect gateway, see this guide . Prerequisites Before you start, ensure that you have: installed i2 Analyze, installed either IBM Db2 or Microsoft SQL Server , and defined a schema to model the data. License acknowledgement Open the license_acknowledgement file in your i2 Analyze directory. Set the value to ACCEPT . It should now look like this: LIC_AGREEMENT = ACCEPT Configuration Create the configuration directory In your i2 Analyze directory, navigate to toolkit\\examples\\configurations\\information-store-daod-opal . Copy the configuration directory to the toolkit directory. This provides a starting point for a deployment that includes the Information Store and support for the i2 Connect gateway. Database management If you are using IBM Db2 as your chosen database management system, you can skip the following steps, otherwise: Download the Microsoft JDBC Driver 7.4 for SQL Server archive. Extract the contents of the download and navigate to sqljdbc_7.4\\enu . Copy the mssql-jdbc-7.4.1.jre8.jar file to the i2 Analyze toolkit\\configuration\\environment\\common\\jdbc-drivers directory. Copy the topology.xml file for SQL Server from the toolkit\\configuration\\examples\\topology\\sqlserver to the toolkit\\configuration\\environment directory, overwriting the existing topology.xml file in the destination directory. Specify the credentials for deployment Using a text editor, open the toolkit\\configuration\\environment\\credentials.properties file. Specify a user name and password to use for the database in the db.infostore.user-name and db.infostore.password properties. Specify a user name and password to use for the Solr indexes in the solr.user-name and solr.password properties. Enter the password to encrypt LTPA tokens in the ltpakeys.password property. Save and close the file. Command access control To gain access to certain features, including the ability to use Postman to reload the connector when making schema changes, you need to copy and modify some files in the i2 Analyze deployment: Navigate to the toolkit\\configuration\\examples\\security-schema directory and copy the file named example-command-access-control.xml . Navigate to the toolkit\\configuration\\fragments\\opal-services\\WEB-INF\\classes directory and paste the file from the previous step. Open the DiscoServerSettingsCommon.properties file and add the name of file you just copied to the CommandAccessControlResource field, including the .xml extension. Save and close the properties file. Configure the schema To test your schema and use it in Analyst's Notebook before deploying a connector, you can configure it as an Information Store schema. The steps below describe this process. Copy your schema and charting schemes to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to your schema files by setting the following properties: SchemaResource=schema-filename.xml ChartingSchemesResource=charting-schemes-filename.xml Configure the security schema All i2 Analyze deployments require a security schema, which defines the level of access users have to the data in the system. You can learn about the i2 Analyze security model in the Knowledge Center but, for the purpose of this guide, follow the steps below to use an example security schema. Copy example-dynamic-security-schema.xml from toolkit\\configuration\\examples\\security-schema to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to the security schema by setting the following property: DynamicSecuritySchemaResource=example-dynamic-security-schema.xml Generate the default configuration For the purposes of this guide, only a basic configuration is required, so you can use the default. Open the toolkit\\scripts directory in a command prompt. To populate some of the mandatory settings with default values, run: setup -t generateDefaults In the i2analyze directory, navigate to the toolkit\\configuration\\environment directory and open the topology.xml file in a text editor. Replace every instance of the host-name tag with the value localhost . There will be four to change in total. This will be used to test your application in Postman. Deployment Open the toolkit\\scripts directory in a command prompt. To deploy i2 Analyze with the configuration you have just created, run setup -t deploy To add an example user whose name and password are both \"Jenny\", run: setup -t ensureExampleUserRegistry To start i2 Analyze, run setup -t start You can now connect Analyst's Notebook Premium to i2 Analyze. The output of the start command includes the URL to use for the i2 Analyze server. This will be the best time to test the schema you created in the previous step, or the existing one if you are using the example schema provided. You can open Analyst's Notebook and start to drag/drop entities and links onto your chart, testing different charting schemes and seeing what labels appear and how they are formatted. For these changes to take effect, you need to update the i2 Analyze connectors configuration and run the internal Reload command via Postman. First, navigate to i2analyze\\toolkit\\scripts in your console and run: setup -t updateConnectorsConfiguration Next you need to open Postman , select the i2 Analyze with InfoStore and Gateway environment and run the Form Based Login request. This will authenticate you as the default user in i2 Analyze, then you can run the Reload request which will configure your schema changes to the topology. (NOTE: You will need to perform a reload command for every iteration of your schema design, otherwise the changes will not take effect). You now have a running i2 Analyze deployment, if you are happy with your schema you can now deploy a connector . However, if there are changes you wish to make to the schema and associated charting schemes, you can run yourself through the schema design guide again."
  },
  "content/miscellaneous/postman.html": {
    "href": "content/miscellaneous/postman.html",
    "title": "Postman",
    "keywords": "Postman You can use Postman collections to test the endpoints of i2 Analyze and your connector. Prerequisites Install the latest version of Postman Import the Postman environments from the postman/environments directory Import the Postman collections from the postman/collections directory To import the environments and collections: Click Import , or click File -> Import In the Import Folder tab, click Choose Folders Select the postman directory that you downloaded from this repository The Postman collections are displayed in the left pane and a populated dropdown list of environments near the top right of the window. Testing i2 Analyze To make sure that i2 Analyze is configured correctly, authenticate a user and test the API. Authentication <!-- TODO: Doc review --> Click on the environment dropdown menu at the top-right bar: If you have deployed i2 Analyze with the i2 Connect gateway , select the i2 Analyze with i2 Connect Gateway environment. If you have deployed i2 Analyze with the Information Store and the i2 Connect gateway , select the i2 Analyze with InfoStore and Gateway environment. Open the i2 Analyze collection. Open the Authentication folder. Run the Form Based Login request. This authenticates your Jenny user, and generates the session token to permit subsequent API requests. API After generating the token, test that you can use the i2 Analyze endpoints. Get all valid timezones In the i2 Analyze collection, open the Core folder. Run the Timezones request. You should see that i2 Analyze returns all valid timezones. Get all connectors In the i2 Analyze collection, open the Connectors folder. Run the Connectors request. You should see that i2 Analyze returns a JSON response containing a list of all configured connectors. Reload connectors configuration In the i2 Analyze collection, open the Gateway folder. Run the Reload request. You should see that i2 Analyze returns its connectors configuration. Testing a connector Before you test your connector, ensure that both i2 Analyze and your connector are running. In this example, the NYPD connector is tested. For more information, see Setting up and running the NYPD connector . From the environment dropdown menu, select the NYPD Connector environment. Open the Connector Services collection. Config and schema endpoints In the Connector Services collection, open the Config folder. Run the Config request. The response provided should be the full contents of the connector's config.json . The Schema and Charting Schemes requests should echo the contents of your connector schema and charting schemes XML files respectively. Acquire endpoints The Acquire folder contains requests that respond with entities and links according to how their respective endpoints were implemented. Using the NYPD connector example: All The All request synchronously returns all entities and links from the NYPD dataset as a JSON response. Search The Search request is a parameterized search accepts a JSON payload of specified conditions which filter the results returned. The values of each condition can be changed to imitate user input from the client for a condition field. Find Like This The Find Like This request is a seeded search that accepts a JSON payload of a single seed entity with a property that is used to filter the JSON response by matching against entities with a similar property. Expand The Expand request is a seeded search that accepts a JSON payload of a single seed entity used as a starting point to find other entities connected to it and the links that connect them. These entities and links are returned in the response in JSON format. Validate The Validate folder contains a single Search request that performs server-side validation on the payload of specified conditions to ensure input values are in the correct format. Async The Async folder contains requests for the asynchronous service. These requests only function as expected on our Async connector. For more information, see setting up and running the Async connector Acquire The Acquire request triggers an asynchronous query and returns a queryId . Using Postman tests (post-request logic), this queryId is automatically stored as an environment variable ( QUERY_ID ) to facilitate subsequent async requests. The request accepts a payload of parameters for simulating the asynchronous request; configuring the duration of time before succeeding and optionally mocking a failure. Status Using the QUERY_ID retrieved from the previous request, the Status retrieves the current status and additional information of the triggered asynchronous query as a JSON response. Results Using the same QUERY_ID from the async acquire, the Results retrieves the JSON response of entities and links from the asynchronous query as you would expect from a synchronous request. This request only works as expected after the status of the query is SUCCEEDED . Delete / Cancel Using the QUERY_ID , the Delete / Cancel request deletes an asynchronous query if its state is SUCCEEDED or FAILED . It cancels the running job if it was STARTED , causing it to be FAILED ."
  },
  "content/miscellaneous/principal-propagation.html": {
    "href": "content/miscellaneous/principal-propagation.html",
    "title": "Principal Propagation",
    "keywords": "Principal Propagation i2 Analyze contains an extension point that enables you to return additional headers to any connectors from the i2 Connect gateway. The extension point requires you to write an implementation of IConnectorRequestModifier.java . The implementation must be packaged into a JAR file and deployed to i2 Analyze. The following example demonstrates how to use Maven to do this and how to configure i2Analyze to use your implementation. Overview For i2 Analyze to use your implementation, complete the following tasks: Create and package your implementation of IConnectorRequestModifier.java Configure i2 Analyze to use your packaged implementation Redeploy and restart Liberty to update your deployment Prerequisites Java installed The principal propagation project Update the value of the <toolkitLocation> element in the principal-propagation/pom.xml file to reference the installation location of i2Analyze. For example: <toolkitLocation>C:\\IBM\\i2analyze</toolkitLocation> This location is used by Maven to locate the JAR files to pull into the local Maven repository and where to place the packaged JAR file that contains your implementation. To set up the Maven environment, open a terminal in the principal-propagation directory. 1. Initialize the Maven dependencies: mvnw initialize This pulls in JAR files from the i2 Analyze deployment toolkit into your local Maven repository. 2. Compile and install the project: mvnw clean install At this point, you could start editing the implementation in principal-propagation/src/main/java/com/i2group/example/PrincipalPropagation.java . For the example implementation, do not make any changes. Deploy the principal propagation implementation When the implementation is complete, package the class into a JAR file. Open a terminal in the principal-propagation directory and run the following command: mvnw package The Maven package command creates a JAR file from the PrincipalPropagation.java class and puts it in the toolkit/configuration/fragments/opal-services/WEB-INF/lib/ directory of the i2 Analyze toolkit specified in the <toolkitLocation> element. Configuring i2Analyze For i2Analyze to use the PrincipalPropagation.jar , it must be specified in the DiscoServerSettingsCommon.properties file. Add the following line to the end of the toolkit/configuration/fragments/opal-services/WEB-INF/classes/DiscoServerSettingsCommon.properties file: ConnectorRequestModifier=com.i2group.example.PrincipalPropagation Redeploy and restart Liberty setup -t stopLiberty setup -t deployLiberty setup -t startLiberty Any connectors receive an extra header named i2ExtensionHeader in responses from the i2 Connect gateway. For example: Host: localhost:9084, Connection: keep-alive, Content-Length: 92, User-Agent: Apache-HttpClient/4.5.6 (Java/1.8.0_232) Content-Type: application/json i2ExtensionHeader: Jenny Accept-Encoding: gzip,deflate Accept-Language: en-gb"
  },
  "content/miscellaneous/source-references.html": {
    "href": "content/miscellaneous/source-references.html",
    "title": "Source References",
    "keywords": "Source References You might want to include source references in entity and link data to preserve the route back to the external source. Entity and link objects support the inclusion of source reference objects. The structure of a source reference object is as follows: { \"id\": String, \"source\": { \"name\": String, \"type\": String, \"description\": String, \"location\": String, \"image\": String }, \"userModifiable\": Boolean } At version 4.3.3 of i2 Analyze, the id field must be present, but its value is ignored and you can set it to an empty string ( \"\" ). The source object must also be present, with at least its name field populated. Example The following JSON object shows how a source reference fits into the structure of an entity object: { \"id\": \"123\", \"typeId\": \"ET1\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" }, \"sourceReference\": { \"id\": \"\", \"source\": { \"name\": \"Source Dataset\", \"type\": \"Open source data\", \"description\": \"A source reference to the corresponding record from the dataset.\", \"location\": \"https://data.cityofnewyork.us/resource/7x9x-zpz6.json?$where=cmplnt_num=123456789\", \"image\": \"https://github.ibm.com/ibmi2/Analyze-Connect/tree/master/docs/images/nypd-dataset-webpage.png?raw=true\" }, \"userModifiable\": false } }"
  },
  "content/miscellaneous/spi-examples.html": {
    "href": "content/miscellaneous/spi-examples.html",
    "title": "i2 Connect SPI examples",
    "keywords": "i2 Connect SPI examples You can use the following example requests and responses to help understand how parameterized and seeded search services work. Example data Throughout, assume you are querying the following set of entities and links. The following are the type IDs corresponding to each of the entity, link and property types used in the example schema: Entity ID Complaint ET1 Location ET2 Person ET3 Link ID Located At LT1 Suspect Of LT2 Victim Of LT3 Properties For Entity ID Complaint Number Complaint PT1 Borough Name Location PT16 Age Group Person PT26 Race Person PT27 Gender Person PT28 This can be represented as a set of entities and links in JSON as follows: { \"entities\": [ { \"typeId\": \"ET1\", \"id\": \"complaint-1\", \"version\": 1, \"properties\": { \"PT1\": \"1\", } }, { \"typeId\": \"ET2\", \"id\": \"manhattan\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" } }, { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-B\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"<18\", \"PT27\": \"Black\" } }, { \"typeId\": \"ET1\", \"id\": \"complaint-2\", \"version\": 1, \"properties\": { \"PT1\": \"2\", } }, { \"typeId\": \"ET3\", \"id\": \"person-C\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-D\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" } }, { \"typeId\": \"ET1\", \"id\": \"complaint-3\", \"version\": 1, \"properties\": { \"PT1\": \"3\", } }, { \"typeId\": \"ET2\", \"id\": \"bronx\", \"version\": 1, \"properties\": { \"PT16\": \"BRONX\" } }, { \"typeId\": \"ET3\", \"id\": \"person-E\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-F\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"45-64\", \"PT27\": \"Black\" } } ], \"links\": [ { \"typeId\": \"LT1\", \"id\": \"located-at-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"manhattan\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-A\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-B\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT1\", \"id\": \"located-at-2\", \"version\": 1, \"fromEndId\": \"complaint-2\", \"toEndId\": \"bronx\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-2\", \"version\": 1, \"fromEndId\": \"complaint-2\", \"toEndId\": \"person-C\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-2\", \"version\": 1, \"fromEndId\": \"complaint-2\", \"toEndId\": \"person-D\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT1\", \"id\": \"located-at-3\", \"version\": 1, \"fromEndId\": \"complaint-3\", \"toEndId\": \"bronx\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-3\", \"version\": 1, \"fromEndId\": \"complaint-3\", \"toEndId\": \"person-E\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-3\", \"version\": 1, \"fromEndId\": \"complaint-3\", \"toEndId\": \"person-F\", \"linkDirection\": \"WITH\", } ] } Parameterized search To implement a service to search for people by age group, you can define a clientConfig in config.json like the following: { \"id\": \"age-search-form\", \"type\": \"FORM\", \"config\": { \"sections\": [ { \"conditions\": [ { \"id\": \"age-group-search-term\", \"label\": \"Age Group\", \"logicalType\": \"SINGLE_LINE_STRING\", \"mandatory\": true } ] } ] } } The id should be a unique identifier for this clientConfig . Then the services you define can use this form by supplying this id in the service's clientConfigId field. The config contains sections and each section contains a JSON object defining a condition field depicted below: The condition id is used as a reference for the value in the request. The label is the field title as shown in [1] above. The logicalType defines the accepted data type of the request value entry [2] . The mandatory field specifies whether a value is required for the field. Empty mandatory fields are highlighted red and shown a warning message [3] . A DaodRequest issued by i2 Analyze when a user runs this search might look like this: { \"payload\": { \"conditions\": [ { \"id\": \"age-group-search\", \"logicalType\": \"SINGLE_LINE_STRING\", \"value\": \"18-24\" } ], \"seeds\": {} } } The request searches for Person entities where the Age Group property is equal to \"18-24\". In the implementation of the parameterized search service, you would filter through the data and find entities which satisfy the request requirements, i.e. have typeId equal to \"ET3\" and have the Age Group property PT26 equal to \"18-24\" . The response, from the example request above, would look like this: { \"entities\": [ { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-E\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } } ], \"links\": [] } Seeded search Seeded searches take as input a set of entities and links that a user already has on their chart and uses this information when finding results. The example operations to go over are: Find-Like-This, where a user is able to select a single entity and search for other entities of the same type with similar properties; and Expand, where a user can select an entity on their chart and search for all other entities that are connected to it by a link, and all those entities and links will be returned. Find-Like-This A DaodRequest received by the connector for a Find-Like-This search on the example data could look something like the following: { \"payload\": { \"conditions\": {}, \"seeds\": { \"entities\": [ { \"accessDimensionValues\": [], \"extensions\": [], \"label\": \"\", \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" }, \"seedId\": \"d8ee0564-57bb-40ed-9409-79f8d13497a5\", \"sourceIds\": [ { \"itemTypeId\": \"ET3\", \"key\": [\"nypd-connector\", \"ET3\", \"person-D\"], \"type\": \"OI.DAOD\" } ], \"typeId\": \"ET3\" } ], \"links\": [], \"allItemTypes\": [] } } } You can deduce which of the entities the DaodSeedEntity in this request corresponds to by looking at the key in the sourceIds field. The third element of this list gives us the ID we have assigned the entity in our connector, \"person-D\" . You also have its type ID \"ET3\" , so it is a Person entity. Have a look at the data above and find this entity. To perform a Find-Like-This search using this seed entity, you need only use its properties. We can filter through our list of entities for those which have typeId equal to \"ET3\" (are Person entities) and have the properties: PT26 equal to \"F\" , i.e. they are female; PT27 equal to \"White\" , i.e. they are a white female; and PT28 equal to \"<18\" , i.e. they are a white female under 18 years of age. After excluding the seed entity itself, you would return the following: { \"entities\": [ { \"typeId\": \"ET3\", \"id\": \"person-C\", \"version\": 1, \"properties\": { \"PT28\": \"F\", \"PT26\": \"<18\", \"PT27\": \"White\" } } ], \"links\": [] } Expand A DaodRequest received by the connector for an Expand service might look like this: { \"payload\": { \"conditions\": {}, \"seeds\": { \"entities\": [ { \"accessDimensionValues\": [], \"extensions\": [], \"label\": \"\", \"properties\": { \"PT1\": \"1\", }, \"seedId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"sourceIds\": [ { \"itemTypeId\": \"ET1\", \"key\": [\"nypd-connector\", \"ET1\", \"complaint-1\"], \"type\": \"OI.DAOD\" } ], \"typeId\": \"ET1\" } ], \"links\": [], \"allItemTypes\": [] } } } Again, you can deduce which of our entities the DaodSeedEntity corresponds to by looking at the sourceIds . The id of the entity in question is \"complaint-1\" and it has typeID equal to \"ET1\" , so it is a complaint. Look at the example data above and find which entity you are expanding. What would you expect an Expand operation to return? To perform an Expand operation with this entity as the seed, you need to: Find all links connected to the corresponding entity. This means going through all the links and finding those with a fromEndId or a toEndId equal to the id of the entity, \"person-D\" . Find all entities at the other end of these links. This can be done by using the fromEndId s and toEndId s of the links found in step 1 - just use the end ID that does not correspond to the seed entity. If returning these entities and links as-is, along with the entity corresponding to the seed, you would respond with: { \"entities\": [ { \"typeId\": \"ET1\", \"id\": \"complaint-1\", \"version\": 1, \"properties\": { \"PT1\": \"1\", } }, { \"typeId\": \"ET2\", \"id\": \"manhattan\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" } }, { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-B\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"<18\", \"PT27\": \"Black\" } } ], \"links\": [ { \"typeId\": \"LT1\", \"id\": \"located-at-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"manhattan\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-A\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-1\", \"version\": 1, \"fromEndId\": \"complaint-1\", \"toEndId\": \"person-B\", \"linkDirection\": \"WITH\", } ] } When copying these results to a chart, the seed entity would be duplicated along with all its links and connected entities that may already be on the chart which would all be connected to the duplicate. Depending on how you want the service to function, you might prefer to have the returned items connected to the entity that you selected on the chart rather than to a duplicate. In this case, you need to change all id , fromEndId and toEndId fields that refer to the ID of the seed entity (in this case \"complaint-1\") to seedId of the DaodSeedEntity in the request, i.e. \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\" . In this case, you would return the following response: { \"entities\": [ { \"typeId\": \"ET2\", \"id\": \"manhattan\", \"version\": 1, \"properties\": { \"PT16\": \"MANHATTAN\" } }, { \"typeId\": \"ET3\", \"id\": \"person-A\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"18-24\", \"PT27\": \"White\" } }, { \"typeId\": \"ET3\", \"id\": \"person-B\", \"version\": 1, \"properties\": { \"PT28\": \"M\", \"PT26\": \"<18\", \"PT27\": \"Black\" } } ], \"links\": [ { \"typeId\": \"LT1\", \"id\": \"located-at-1\", \"version\": 1, \"fromEndId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"toEndId\": \"manhattan\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT2\", \"id\": \"suspect-of-1\", \"version\": 1, \"fromEndId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"toEndId\": \"person-A\", \"linkDirection\": \"WITH\", }, { \"typeId\": \"LT3\", \"id\": \"victim-of-1\", \"version\": 1, \"fromEndId\": \"1e756171-fb3c-40a4-b7c5-5c537fbf0adc\", \"toEndId\": \"person-B\", \"linkDirection\": \"WITH\", } ] }"
  },
  "content/miscellaneous/troubleshoot.html": {
    "href": "content/miscellaneous/troubleshoot.html",
    "title": "Troubleshooting guide",
    "keywords": "Troubleshooting guide This is a list of common issues you may face during the development of your connector and how to solve them. They are categorized by the stage of development in which they may arise. If you come across a problem that is not covered below, please raise an issue here . Where to check for errors Log files The i2 Analyze server logs can be found in the i2analyze\\deploy\\wlp\\usr\\servers\\opal-server\\logs\\ directory. console.log is the main server log file. opal-services-daod\\IBM_i2_Analysis_Repository.log is where errors with your connector configuration and/or data will appear. Connector configuration errors \"Failed to retrieve configuration information for the connector with identifier '<CONNECTOR_ID>' from the URL '<URL>'\" You will see this in the output when running setup -t start if i2 Analyze fails to retrieve your connector's configuration from its config endpoint. Check that the base URL printed here is correct. If not, update the connector base url given in the i2 Analyze topology.xml file (in the i2analyze\\toolkit\\configuration\\environment directory). If the URL is correct, then you need to check the implementation of your config endpoint and that you have assigned to it the correct URL path. This should be the base URL suffixed with \"/config\". \"Some queries are not configured correctly. Contact your system administrator.\" You will see this in the External Searches window if there is a problem with your connector configuration. Clicking DETAILS will provide more information. Problems might include: no acquireUrl defined for a service: make sure you define one in the service definition invalid default timezone: make sure you set a timeZoneId in the defaultValues section of your connector configuration The application is communicating with the connector through a protocol that is insecure. You will see this because i2 Analyze and your connector communicate via HTTP. In a production environment, you should secure this connection by using HTTPS. This is beyond the scope of this guide, but is covered in the Knowledge Center here . Errors running connector services \"Failed to open the selected query. Contact your system administrator\" If you see the above error message when trying to run a query defined by your connector via External Searches in Analysts's Notebook Premium, there are a number of potential causes. To understand what went wrong: Look at the log file: i2analyze\\deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services-daod\\IBM_i2_Analysis_Repository.log You should see at the bottom a more detailed description of the error that occurred. You will most likely see an error message starting with the following: d484-4e04-97d5-86e771fed434 - Validation error while retrieving a record. - The service with identifier '<SERVICE_NAME>' on the connector with identifier '<CONNECTOR_ID>' returned invalid data Below this line will be more specific information about why the query failed. Examples and their solutions are given below. \"The schema does not contain an entity/a link/a property with identifier '<TYPE_ID>'\" This means you have supplied an entity or link with a typeId that does not exist in your schema. Check that the typeId you supplied is not a typo. \"The value '<PROPERTY_VALUE>' is not valid for the property type with identifier '<TYPE_ID>'. The value '<PROPERTY_VALUE>' is incompatible with the '<LOGICAL_TYPE>' data type.\" This means that the property value you supplied is not in the correct format, which is defined by the logical type of the property. This is especially common when dealing with DATE , TIME and DATETIME properties as it can be finicky to ensuring input of such values is in the right format. Check that you have assigned an appropriate logical type for this property and see the data model examples to understand the required format for each logical type."
  },
  "content/rest/i2_connect_spi.html": {
    "href": "content/rest/i2_connect_spi.html",
    "title": "i2 Connect gateway REST SPI",
    "keywords": "i2 Connect gateway REST SPI This documentation describes the server programming interface that you must implement when you write a connector to an external data source. Clients discover the URLs of the three connector endpoints through a combination of the topology file for the deployment and your SPI implementation. The {configurationUrl} comes from the topology file. By default, the i2 Connect gateway makes a POST request to your connector at the URL that you specify in the base-url attribute of the element. If the value is http://localhost:3700/ , the request comes to http://localhost:3700/config . Your response from the {configurationUrl} can contain information about one or more services. The information for each service contains, in its acquireUrl and (optional) validateUrl fields, the URLs of the {acquireUrl} and {validateUrl} endpoints described here."
  },
  "content/rest/i2_connect_spi/acquire.html": {
    "href": "content/rest/i2_connect_spi/acquire.html",
    "title": "acquire",
    "keywords": "acquire Fetches data synchronously from the service that has the specified acquire URL. When a client queries an external data source, the i2 Analyze server calls the gateway, and the i2 Connect gateway contacts the connector, passing the payload that the client provided. By default, the gateway prefixes {acquireUrl} with the base URL from the topology. If the service in question also specifies the validateUrl property, then the gateway uses the {validateUrl} endpoint before it uses {acquireUrl} . In the response, the properties maps that appear in the contents of the entities and links arrays must have the following general structure: \"properties\": { \"PT1\": false, \"PT2\": 0, \"PT3\": \"\", \"PT4\": { \"localDateAndTime\": \"\", \"timeZoneId\": \"\", \"isDST\": false }, \"PT5\": { \"type\": \"\", \"coordinates\": [ 0, 0, 0 ] } } In the example, PT1 , PT2 , PT3 , PT4 , and PT5 are valid property types that belong to the entity or link type that was specified in typeId . PT1 has the 'BOOLEAN' logical type, and PT2 has one of the numeric logical types. PT3 might have any of the others, although there are extra rules about 'DATE_AND_TIME' and 'GEOSPATIAL' properties that are reflected in PT4 and PT5 . 'DATE_AND_TIME' properties are transported as objects that contain the local date & time and the timezone identifier as strings, together with a Boolean that indicates whether daylight saving time is in effect. However, the connector configuration's default settings allow you to simplify values in the response to ISO-8601 strings. If you do that, i2 Analyze sets timeZoneId from the default settings, and isDST to false . 'GEOSPATIAL' properties are transported as objects that contain the type of the shape and the coordinates that define it. In this version of the SPI, the only valid type is 'Point', for which the coordinates are a triple of decimals that specify longitude, latitude, and altitude. (Setting the altitude is optional; i2 Analyze ignores it at present.) acquire Request POST /{acquireUrl} Parameters Name In Type Default Notes *request body Daod Request A payload that the service can interpret to modify its behavior. If the service has a client configuration of type 'NONE', the payload never contains conditions. For type 'FORM', it contains conditions that have a fixed structure. For a 'CUSTOM' client configuration, the contents of the conditions object are free-form. The client configuration type has no effect on whether the request payload contains seed record data. When it does, the seeds object includes summary item type information in the following form: \"allItemTypes\": [ { \"displayName\": \"\", \"typeId\": \"ET5\", \"typeLocation\": \"INFOSTORE\", \"semanticTypeId\": \"\", \"propertyTypes\": { \"PT1\": { \"semanticTypeId\": \"\", \"displayName\": \"\", \"logicalType\": \"\" }, \"PT2\": { \"semanticTypeId\": \"\", \"displayName\": \"\", \"logicalType\": \"\" } } } ] An implementation of the acquire endpoint can choose to act upon the information in the allItemTypes array, or to ignore it and use only the contents of entities and links . Note: Earlier versions of this SPI transported type information in an itemTypes object rather than the allItemTypes array. That mechanism has been retained for compatibility with existing deployments. For information about the structure of the itemTypes object, see the previous version of this documentation. Responses Status Code Type Description Samples 200 I2 Connect Data The POST method completed successfully. 500 The connector failed to process the request for data. Definitions Daod Request When a client queries an external data source, the i2 Analyze server calls the gateway, and the i2 Connect gateway contacts the connector, passing the payload that the client provided. By default, the gateway prefixes {acquireUrl} with the base URL from the topology. If the service in question also specifies the validateUrl property, then the gateway uses the {validateUrl} endpoint before it uses {acquireUrl} . In the response, the properties maps that appear in the contents of the entities and links arrays must have the following general structure: \"properties\": { \"PT1\": false, \"PT2\": 0, \"PT3\": \"\", \"PT4\": { \"localDateAndTime\": \"\", \"timeZoneId\": \"\", \"isDST\": false }, \"PT5\": { \"type\": \"\", \"coordinates\": [ 0, 0, 0 ] } } In the example, PT1 , PT2 , PT3 , PT4 , and PT5 are valid property types that belong to the entity or link type that was specified in typeId . PT1 has the 'BOOLEAN' logical type, and PT2 has one of the numeric logical types. PT3 might have any of the others, although there are extra rules about 'DATE_AND_TIME' and 'GEOSPATIAL' properties that are reflected in PT4 and PT5 . 'DATE_AND_TIME' properties are transported as objects that contain the local date & time and the timezone identifier as strings, together with a Boolean that indicates whether daylight saving time is in effect. However, the connector configuration's default settings allow you to simplify values in the response to ISO-8601 strings. If you do that, i2 Analyze sets timeZoneId from the default settings, and isDST to false . 'GEOSPATIAL' properties are transported as objects that contain the type of the shape and the coordinates that define it. In this version of the SPI, the only valid type is 'Point', for which the coordinates are a triple of decimals that specify longitude, latitude, and altitude. (Setting the altitude is optional; i2 Analyze ignores it at present.) Name Type Notes payload Daod Request- Payload[] A custom payload that an i2 Connect service can use to determine what data to retrieve from a connected source Daod Request- Payload A custom payload that an i2 Connect service can use to determine what data to retrieve from a connected source Name Type Notes conditions Daod Request- Condition[] If the service uses a client configuration of type 'FORM', the conditions that a user has specified to refine their query seeds Daod Seeds[] If the service supports being seeded by existing i2 Analyze records, the data from those records for it to use Daod Request- Condition If the service uses a client configuration of type 'FORM', the conditions that a user has specified to refine their query Name Type Notes id string The identifier of the condition, as specified in the client configuration for the service logicalType string The logical type of the value in the condition value string The value that a user supplied for the condition Daod Seeds If the service supports being seeded by existing i2 Analyze records, the data from those records for it to use Name Type Notes entities Daod Seed Entity Data[] Data from the entity records that were specified as seeds for the request links Daod Seed Link Data[] Data from the link records that were specified as seeds for the request Daod Seed Entity Data Data from the entity records that were specified as seeds for the request Name Type Notes accessDimensionValues Security Dimension And Values[] The security dimension values of the record identified by seedId extensions object Free-form, custom information for the record identified by seedId label string The label of the record identified by seedId properties object The property data of the record identified by seedId seedId object The identifier of a seed record sourceIds Origin Identifier[] The source identifiers of a seed record typeId string The type identifier of the record identified by seedId Security Dimension And Values The security dimension values of the record identified by seedId Name Type Notes dimensionId string The identifier of the security dimension that has the values in ids ids array The identifiers of values in the security dimension with dimensionId Origin Identifier The source identifiers of a seed record Name Type Notes itemTypeId string The identifier of the item type of the record that has this origin identifier key array The values that identify some data in its original source type string The type of this origin identifier Daod Seed Link Data Data from the link records that were specified as seeds for the request Name Type Notes accessDimensionValues Security Dimension And Values[] The security dimension values of the record identified by seedId extensions object Free-form, custom information for the record identified by seedId fromEndId object The identifier of the record at the \"from\" end of the link record identified by seedId fromEndTypeId string The type identifier of the record at the \"from\" end of the link record identified by seedId label string The label of the record identified by seedId linkDirection string The direction of the link record identified by seedId , which overrides any default setting properties object The property data of the record identified by seedId seedId object The identifier of a seed record sourceIds Origin Identifier[] The source identifiers of a seed record toEndId object The identifier of the record at the \"to\" end of the link record identified by seedId toEndTypeId string The type identifier of the record at the \"to\" end of the link record identified by seedId typeId string The type identifier of the record identified by seedId I2 Connect Data When a client queries an external data source, the i2 Analyze server calls the gateway, and the i2 Connect gateway contacts the connector, passing the payload that the client provided. By default, the gateway prefixes {acquireUrl} with the base URL from the topology. If the service in question also specifies the validateUrl property, then the gateway uses the {validateUrl} endpoint before it uses {acquireUrl} . In the response, the properties maps that appear in the contents of the entities and links arrays must have the following general structure: \"properties\": { \"PT1\": false, \"PT2\": 0, \"PT3\": \"\", \"PT4\": { \"localDateAndTime\": \"\", \"timeZoneId\": \"\", \"isDST\": false }, \"PT5\": { \"type\": \"\", \"coordinates\": [ 0, 0, 0 ] } } In the example, PT1 , PT2 , PT3 , PT4 , and PT5 are valid property types that belong to the entity or link type that was specified in typeId . PT1 has the 'BOOLEAN' logical type, and PT2 has one of the numeric logical types. PT3 might have any of the others, although there are extra rules about 'DATE_AND_TIME' and 'GEOSPATIAL' properties that are reflected in PT4 and PT5 . 'DATE_AND_TIME' properties are transported as objects that contain the local date & time and the timezone identifier as strings, together with a Boolean that indicates whether daylight saving time is in effect. However, the connector configuration's default settings allow you to simplify values in the response to ISO-8601 strings. If you do that, i2 Analyze sets timeZoneId from the default settings, and isDST to false . 'GEOSPATIAL' properties are transported as objects that contain the type of the shape and the coordinates that define it. In this version of the SPI, the only valid type is 'Point', for which the coordinates are a triple of decimals that specify longitude, latitude, and altitude. (Setting the altitude is optional; i2 Analyze ignores it at present.) Name Type Notes entities I2 Connect Entity Data[] The entity data returned from a connector errorMessage string An error message that might be displayed to users links I2 Connect Link Data[] The link data returned from a connector I2 Connect Entity Data The entity data returned from a connector Name Type Notes id object The identifier of the data for a record in its source. If the record is from the Information Store, id should contain the record identifier in the form \"infoStoreRecordId\": \" record_identifier \" . If the record was a seed in the request, id should contain the seed identifier properties object The property data for a record sourceReference Source Reference[] The source reference for a record typeId string The type identifier for a record typeLocation string Optionally, the location of the schema in which typeId is defined. When absent, the gateway searches for the type in the connector schema, the associated gateway schema, and the Information Store schema in that order version integer (int64) The version for a record Source Reference The source reference for a record Name Type Notes id string The unique identifier of this source reference source Source Reference Info[] The information that this source reference contains userModifiable boolean true if users can edit or delete this source reference; false otherwise Source Reference Info The information that this source reference contains Name Type Notes description string The description of a source image string The URL of an image of a source location string The location of a source, which might be a URL name string The name of a source type string The type of a source I2 Connect Link Data The link data returned from a connector Name Type Notes fromEndId object The identifier of the data for the record at the \"from\" end of a link. If the record is from the Information Store, fromEndId should contain the record identifier in the form \"infoStoreRecordId\": \" record_identifier \" . If the record was a seed in the request, fromEndId should contain the seed identifier id object The identifier of the data for a record in its source. If the record is from the Information Store, id should contain the record identifier in the form \"infoStoreRecordId\": \" record_identifier \" . If the record was a seed in the request, id should contain the seed identifier linkDirection string The direction for a link record, which overrides any default setting properties object The property data for a record sourceReference Source Reference[] The source reference for a record toEndId object The identifier of the data for the record at the \"to\" end of a link, which is subject to the same considerations as fromEndId typeId string The type identifier for a record typeLocation string Optionally, the location of the schema in which typeId is defined. When absent, the gateway searches for the type in the connector schema, the associated gateway schema, and the Information Store schema in that order version integer (int64) The version for a record"
  },
  "content/rest/i2_connect_spi/acquireAsync.html": {
    "href": "content/rest/i2_connect_spi/acquireAsync.html",
    "title": "acquireAsync",
    "keywords": "acquireAsync Starts an asynchronous query to fetch data from the service in the specified query resource. When a client queries an external data source, the i2 Analyze server calls the gateway, and the i2 Connect gateway contacts the connector, passing the payload that the client provided. By default, the gateway prefixes {queriesResource} with the base URL from the topology. The other requests that {queriesResource} supports provide ways to determine the status of a started asynchronous query, and to fetch its results upon success. If the service in question also specifies the validateUrl property, then the gateway uses the {validateUrl} endpoint before it uses {queriesResource} . The response must contain a query ID that the client can use in subsequent requests to retrieve further information. acquireAsync Request POST /{queriesResource} Parameters Name In Type Default Notes *request body Daod Request A payload that the service can interpret to modify its behavior. If the service has a client configuration of type 'NONE', the payload never contains conditions. For type 'FORM', it contains conditions that have a fixed structure. For a 'CUSTOM' client configuration, the contents of the conditions object are free-form. The client configuration type has no effect on whether the request payload contains seed record data. When it does, the seeds object includes summary item type information in the following form: \"allItemTypes\": [ { \"displayName\": \"\", \"typeId\": \"ET5\", \"typeLocation\": \"INFOSTORE\", \"semanticTypeId\": \"\", \"propertyTypes\": { \"PT1\": { \"semanticTypeId\": \"\", \"displayName\": \"\", \"logicalType\": \"\" }, \"PT2\": { \"semanticTypeId\": \"\", \"displayName\": \"\", \"logicalType\": \"\" } } } ] An implementation of the queries resource endpoint can choose to act upon the information in the allItemTypes array, or to ignore it and use only the contents of entities and links . Responses Status Code Type Description Samples 200 Async Query Response The POST method completed successfully. 500 The connector failed to start the asynchronous query. Definitions Daod Request When a client queries an external data source, the i2 Analyze server calls the gateway, and the i2 Connect gateway contacts the connector, passing the payload that the client provided. By default, the gateway prefixes {queriesResource} with the base URL from the topology. The other requests that {queriesResource} supports provide ways to determine the status of a started asynchronous query, and to fetch its results upon success. If the service in question also specifies the validateUrl property, then the gateway uses the {validateUrl} endpoint before it uses {queriesResource} . The response must contain a query ID that the client can use in subsequent requests to retrieve further information. Name Type Notes payload Daod Request- Payload[] A custom payload that an i2 Connect service can use to determine what data to retrieve from a connected source Daod Request- Payload A custom payload that an i2 Connect service can use to determine what data to retrieve from a connected source Name Type Notes conditions Daod Request- Condition[] If the service uses a client configuration of type 'FORM', the conditions that a user has specified to refine their query seeds Daod Seeds[] If the service supports being seeded by existing i2 Analyze records, the data from those records for it to use Daod Request- Condition If the service uses a client configuration of type 'FORM', the conditions that a user has specified to refine their query Name Type Notes id string The identifier of the condition, as specified in the client configuration for the service logicalType string The logical type of the value in the condition value string The value that a user supplied for the condition Daod Seeds If the service supports being seeded by existing i2 Analyze records, the data from those records for it to use Name Type Notes entities Daod Seed Entity Data[] Data from the entity records that were specified as seeds for the request links Daod Seed Link Data[] Data from the link records that were specified as seeds for the request Daod Seed Entity Data Data from the entity records that were specified as seeds for the request Name Type Notes accessDimensionValues Security Dimension And Values[] The security dimension values of the record identified by seedId extensions object Free-form, custom information for the record identified by seedId label string The label of the record identified by seedId properties object The property data of the record identified by seedId seedId object The identifier of a seed record sourceIds Origin Identifier[] The source identifiers of a seed record typeId string The type identifier of the record identified by seedId Security Dimension And Values The security dimension values of the record identified by seedId Name Type Notes dimensionId string The identifier of the security dimension that has the values in ids ids array The identifiers of values in the security dimension with dimensionId Origin Identifier The source identifiers of a seed record Name Type Notes itemTypeId string The identifier of the item type of the record that has this origin identifier key array The values that identify some data in its original source type string The type of this origin identifier Daod Seed Link Data Data from the link records that were specified as seeds for the request Name Type Notes accessDimensionValues Security Dimension And Values[] The security dimension values of the record identified by seedId extensions object Free-form, custom information for the record identified by seedId fromEndId object The identifier of the record at the \"from\" end of the link record identified by seedId fromEndTypeId string The type identifier of the record at the \"from\" end of the link record identified by seedId label string The label of the record identified by seedId linkDirection string The direction of the link record identified by seedId , which overrides any default setting properties object The property data of the record identified by seedId seedId object The identifier of a seed record sourceIds Origin Identifier[] The source identifiers of a seed record toEndId object The identifier of the record at the \"to\" end of the link record identified by seedId toEndTypeId string The type identifier of the record at the \"to\" end of the link record identified by seedId typeId string The type identifier of the record identified by seedId Async Query Response When a client queries an external data source, the i2 Analyze server calls the gateway, and the i2 Connect gateway contacts the connector, passing the payload that the client provided. By default, the gateway prefixes {queriesResource} with the base URL from the topology. The other requests that {queriesResource} supports provide ways to determine the status of a started asynchronous query, and to fetch its results upon success. If the service in question also specifies the validateUrl property, then the gateway uses the {validateUrl} endpoint before it uses {queriesResource} . The response must contain a query ID that the client can use in subsequent requests to retrieve further information. Name Type Notes queryId string The identifier of the asynchronous query"
  },
  "content/rest/i2_connect_spi/config.html": {
    "href": "content/rest/i2_connect_spi/config.html",
    "title": "config",
    "keywords": "config Fetches configuration information from a connector. At startup, the i2 Connect gateway retrieves configuration information from each connector that is defined in the topology file. By default, the gateway uses the endpoint {baseURL}/config , but you can specify a different one in the definition. In the response, for client configurations of type 'FORM', the config object must have the following general structure: \"config\": { \"sections\": [ { \"title\": \"\", \"conditions\": [ { \"id\": \"\", \"label\": \"\", \"description\": \"\", \"mandatory\": false, \"logicalType\": \"\" } ] } ] } The sections array can contain multiple objects, and each section has a conditions array that can also contain multiple objects. Within each condition, id must be unique, while the client displays label and description to users to explain what the condition does. mandatory determines whether users must provide a value for the condition, and logicalType determines what kind of value it is. Some logical types allow you to specify validation that the client must enforce on any value that the user provides. When logicalType is 'DOUBLE' or 'INTEGER', you can add minValue and maxValue fields to the condition to constrain the numeric range. When logicalType is 'SINGLE_LINE_STRING' or 'MULTI_LINE_STRING', you can add two objects to the condition that constrain the length or the contents of the string: \"maxStringLength\": { \"size\": 0, // Defaults to \"uf16codeunits\" if not specified // The other legitimate value is \"utf8bytes\" \"units\": \"\" } \"extraStringValidation\": { // A regular expression that the value must match \"regex\": \"\", // A message that explains the extra validation to users \"message\": \"\" } When logicalType is 'SUGGESTED_FROM', you can add a possibleValues array to the condition. (See below.) When logicalType is 'SELECTED_FROM', you must add a possibleValues array to the condition: \"possibleValues\": [ { \"value\": \"\", \"displayName\": \"\" } ] Each object in possibleValues must have a value field that contains the value to be sent back when the client uses the service. If present, the client can display the value in displayName to users in place of the value itself. When logicalType is 'BOOLEAN' or 'GEOSPATIAL' or 'DATE' or 'TIME' or 'DATE_AND_TIME', no additional validation is possible. config Request GET /{configurationUrl} Responses Status Code Type Description Samples 200 Connector Config The GET method completed successfully. 500 The connector failed to provide configuration information. Definitions Connector Config At startup, the i2 Connect gateway retrieves configuration information from each connector that is defined in the topology file. By default, the gateway uses the endpoint {baseURL}/config , but you can specify a different one in the definition. In the response, for client configurations of type 'FORM', the config object must have the following general structure: \"config\": { \"sections\": [ { \"title\": \"\", \"conditions\": [ { \"id\": \"\", \"label\": \"\", \"description\": \"\", \"mandatory\": false, \"logicalType\": \"\" } ] } ] } The sections array can contain multiple objects, and each section has a conditions array that can also contain multiple objects. Within each condition, id must be unique, while the client displays label and description to users to explain what the condition does. mandatory determines whether users must provide a value for the condition, and logicalType determines what kind of value it is. Some logical types allow you to specify validation that the client must enforce on any value that the user provides. When logicalType is 'DOUBLE' or 'INTEGER', you can add minValue and maxValue fields to the condition to constrain the numeric range. When logicalType is 'SINGLE_LINE_STRING' or 'MULTI_LINE_STRING', you can add two objects to the condition that constrain the length or the contents of the string: \"maxStringLength\": { \"size\": 0, // Defaults to \"uf16codeunits\" if not specified // The other legitimate value is \"utf8bytes\" \"units\": \"\" } \"extraStringValidation\": { // A regular expression that the value must match \"regex\": \"\", // A message that explains the extra validation to users \"message\": \"\" } When logicalType is 'SUGGESTED_FROM', you can add a possibleValues array to the condition. (See below.) When logicalType is 'SELECTED_FROM', you must add a possibleValues array to the condition: \"possibleValues\": [ { \"value\": \"\", \"displayName\": \"\" } ] Each object in possibleValues must have a value field that contains the value to be sent back when the client uses the service. If present, the client can display the value in displayName to users in place of the value itself. When logicalType is 'BOOLEAN' or 'GEOSPATIAL' or 'DATE' or 'TIME' or 'DATE_AND_TIME', no additional validation is possible. Name Type Notes chartingSchemesUrl string The URL from which to retrieve a charting scheme for the connector, which can be absolute or relative to its base URL. No value means that there is no charting scheme clientConfigs Client Config[] The client configurations that the services on the connector use defaultValues Connector Default Values[] Default values to apply to data retrieved by the services on the connector gatewaySchema string The short name of the gateway schema whose types the connector uses, or no value if the connector does not use a gateway schema. The topology for the deployment can override the gateway schema specified here schemaShortName string The short name for the connector schema, or no value when there is no connector schema. The topology for the deployment can override the name specified here schemaUrl string The URL from which to retrieve a schema for the connector, which can be absolute or relative to its base URL. No value means that there is no schema services Service[] The services available on the connector Client Config The client configurations that the services on the connector use Name Type Notes config object The client configuration itself id string The unique identifier of the client configuration type string The type of the client configuration, which can be 'NONE', 'FORM', or 'CUSTOM' Connector Default Values Default values to apply to data retrieved by the services on the connector Name Type Notes entityTypeId string The identifier of the default entity type to apply to retrieved data entityTypeLocation string The location of the default entity type to apply to retrieved data, which can be 'CONNECTOR', 'GATEWAY', or 'INFOSTORE'. linkDirection string The default link direction to apply to retrieved data, which can be 'NONE', 'WITH', 'AGAINST', or 'BOTH' linkTypeId string The identifier of the default link type to apply to retrieved data linkTypeLocation string The location of the default link type to apply to retrieved data, which can be 'CONNECTOR', 'GATEWAY', or 'INFOSTORE'. resultIdsPersistent boolean The default indicator of whether identifiers for the same retrieved data are ( true ) or are not ( false ) persistent from one set of results to the next. timeZoneId string The identifier of the default time zone to apply to retrieved data Service The services available on the connector Name Type Notes acquireUrl string The URL for the endpoint that provides result data from the service. If this is present, then async must not be present async Service- Async[] Indicates that the service must be called asynchronously, and provides configuration settings. If this is present, then acquireUrl must not be present clientConfigId string The identifier of the client configuration for the service, if the type is not 'NONE' clientConfigType string The type of the client configuration for the service, which can be 'NONE', 'FORM', or 'CUSTOM' description string A description of the service, which might be displayed to users id string The unique identifier of the service name string The name of the service, which might be displayed to users resultIdsPersistent boolean true if identifiers for the same data retrieved from the service are persistent from one set of results to the next; false otherwise resultItemTypeIds object The identifiers of the item types that can appear in results from the service, in a map with the following structure: {\" LOCATION \": [\"TYPEID1\", \"TYPEID2\", ...], ...} . Here, LOCATION indicates which schema the item type is defined in. Legal values are 'INFOSTORE', 'GATEWAY', and 'CONNECTOR'. If a service uses types from all three locations, the map has three elements seedConstraints Connector Seed Constraints[] The constraints on the seeds that users can specify, or null if the service does not support seeds validateUrl string The URL for the endpoint that validates requests for data from the service Service- Async Indicates that the service must be called asynchronously, and provides configuration settings. If this is present, then acquireUrl must not be present Name Type Notes pollingIntervalInSeconds integer (int32) The recommended interval at which clients should poll asynchronous endpoints for changes queriesResource string The base URL of the endpoints for the asynchronous request mechanism to use Connector Seed Constraints The constraints on the seeds that users can specify, or null if the service does not support seeds Name Type Notes connectorIds array The identifiers of the connectors from which seed records can originate max integer (int32) The maximum number of seed records that users must specify min integer (int32) The minimum number of seed records that users must specify seedTypes Connector Seed Types[] The constraints on the types that seed records can have Connector Seed Types The constraints on the types that seed records can have Name Type Notes allowedTypes string The subset of item types by which a seed record can be constrained, which must be 'ENTITY' or 'LINK' itemTypes Connector Seed Constraint Item Type[] The item types to which a seed record is constrained, which are limited to the subset in allowedTypes . Connector Seed Constraint Item Type The item types to which a seed record is constrained, which are limited to the subset in allowedTypes . Name Type Notes id string The identifier of the item type max integer (int32) The maximum number of seed records of this type min integer (int32) The minimum number of seed records of this type typeLocation string The type location value for the item type, used to determine in which schema the item type resides. Possible values are GATEWAY, CONNECTOR and INFOSTORE."
  },
  "content/rest/i2_connect_spi/delete.html": {
    "href": "content/rest/i2_connect_spi/delete.html",
    "title": "delete",
    "keywords": "delete Deletes the specified asynchronous query from the specified queries resource when it is no longer needed. The i2 Connect gateway calls the DELETE method when a query has succeeded, been canceled, or failed. A connector can take the call as a cue to clean up any resources associated with processing the query. delete Request DELETE /{queriesResource}/{queryId} Responses Status Code Type Description Samples 200 No response was specified. 204 The DELETE method completed successfully. 500 The connector failed to delete the query."
  },
  "content/rest/i2_connect_spi/results.html": {
    "href": "content/rest/i2_connect_spi/results.html",
    "title": "results",
    "keywords": "results Fetches the results of the specified asynchronous query from the specified queries resource. Clients can attempt to retrieve results only from an asynchronous query whose state is 'SUCCEEDED'. An attempt to fetch results from a query in any other state must fail. The structure of the response is identical to the response from a synchronous query. results Request GET /{queriesResource}/{queryId}/results Responses Status Code Type Description Samples 200 I2 Connect Data The GET method completed successfully. 500 The connector failed to retrieve the results of the query. Definitions I2 Connect Data Clients can attempt to retrieve results only from an asynchronous query whose state is 'SUCCEEDED'. An attempt to fetch results from a query in any other state must fail. The structure of the response is identical to the response from a synchronous query. Name Type Notes entities I2 Connect Entity Data[] The entity data returned from a connector errorMessage string An error message that might be displayed to users links I2 Connect Link Data[] The link data returned from a connector I2 Connect Entity Data The entity data returned from a connector Name Type Notes id object The identifier of the data for a record in its source. If the record is from the Information Store, id should contain the record identifier in the form \"infoStoreRecordId\": \" record_identifier \" . If the record was a seed in the request, id should contain the seed identifier properties object The property data for a record sourceReference Source Reference[] The source reference for a record typeId string The type identifier for a record typeLocation string Optionally, the location of the schema in which typeId is defined. When absent, the gateway searches for the type in the connector schema, the associated gateway schema, and the Information Store schema in that order version integer (int64) The version for a record Source Reference The source reference for a record Name Type Notes id string The unique identifier of this source reference source Source Reference Info[] The information that this source reference contains userModifiable boolean true if users can edit or delete this source reference; false otherwise Source Reference Info The information that this source reference contains Name Type Notes description string The description of a source image string The URL of an image of a source location string The location of a source, which might be a URL name string The name of a source type string The type of a source I2 Connect Link Data The link data returned from a connector Name Type Notes fromEndId object The identifier of the data for the record at the \"from\" end of a link. If the record is from the Information Store, fromEndId should contain the record identifier in the form \"infoStoreRecordId\": \" record_identifier \" . If the record was a seed in the request, fromEndId should contain the seed identifier id object The identifier of the data for a record in its source. If the record is from the Information Store, id should contain the record identifier in the form \"infoStoreRecordId\": \" record_identifier \" . If the record was a seed in the request, id should contain the seed identifier linkDirection string The direction for a link record, which overrides any default setting properties object The property data for a record sourceReference Source Reference[] The source reference for a record toEndId object The identifier of the data for the record at the \"to\" end of a link, which is subject to the same considerations as fromEndId typeId string The type identifier for a record typeLocation string Optionally, the location of the schema in which typeId is defined. When absent, the gateway searches for the type in the connector schema, the associated gateway schema, and the Information Store schema in that order version integer (int64) The version for a record"
  },
  "content/rest/i2_connect_spi/status.html": {
    "href": "content/rest/i2_connect_spi/status.html",
    "title": "status",
    "keywords": "status Fetches the status of the specified asynchronous query from the specified queries resource. After a client starts an asynchronous query, it uses polling to determine its progress. The client polls the i2 Connect gateway, and the gateway calls through to the connector to retrieve its status. The response must contain one of three state values that indicate the status of the query: 'STARTED', 'SUCCEEDED', or 'FAILED'. The response can also contain a series of substatuses that report progress from the underlying data source. The four valid substatus types are 'INFORMATION', 'WARNING', 'ERROR', and 'SUCCESS'. status Request GET /{queriesResource}/{queryId} Responses Status Code Type Description Samples 200 Async Query Status The GET method completed successfully. 304 There are no changes to the status of the query. 500 The connector failed to determine the status of the query. Definitions Async Query Status After a client starts an asynchronous query, it uses polling to determine its progress. The client polls the i2 Connect gateway, and the gateway calls through to the connector to retrieve its status. The response must contain one of three state values that indicate the status of the query: 'STARTED', 'SUCCEEDED', or 'FAILED'. The response can also contain a series of substatuses that report progress from the underlying data source. The four valid substatus types are 'INFORMATION', 'WARNING', 'ERROR', and 'SUCCESS'. Name Type Notes errorMessage string An error message that explains why an asynchronous query is in the 'FAILED' state state string The state of an asynchronous query, which can be 'STARTED', 'SUCCEEDED', or 'FAILED' substatuses Async Query Substatus[] More detailed information about the progress of an asynchronous query, in the form of messages that might be displayed to users Async Query Substatus More detailed information about the progress of an asynchronous query, in the form of messages that might be displayed to users Name Type Notes message string The message itself, which might be displayed to users type string The type of the message, which can be 'INFORMATION', 'WARNING', 'ERROR', or 'SUCCESS'"
  },
  "content/rest/i2_connect_spi/validate.html": {
    "href": "content/rest/i2_connect_spi/validate.html",
    "title": "validate",
    "keywords": "validate Validates a request for data from the service that has the specified validate URL. The i2 Connect gateway uses the {validateUrl} endpoint in two places: when a client makes a direct request, and immediately before it uses the {acquireUrl} or a {queriesResource} endpoint. By default, the gateway prefixes {validateUrl} with the base URL from the topology. If validation fails in front of an attempt to acquire data, the gateway does not continue with the attempt. validate Request POST /{validateUrl} Parameters Name In Type Default Notes *request body Daod Request A payload that the service can interpret to modify its behavior. If the service has a client configuration of type 'NONE', the payload never contains conditions. For type 'FORM', it contains conditions that have a fixed structure. For a 'CUSTOM' client configuration, the contents of the conditions object are free-form. The client configuration type has no effect on whether the request payload contains seed record data. When it does, the seeds object includes summary item type information in the following form: \"allItemTypes\": [ { \"displayName\": \"\", \"typeId\": \"ET5\", \"typeLocation\": \"INFOSTORE\", \"semanticTypeId\": \"\", \"propertyTypes\": { \"PT1\": { \"semanticTypeId\": \"\", \"displayName\": \"\", \"logicalType\": \"\" }, \"PT2\": { \"semanticTypeId\": \"\", \"displayName\": \"\", \"logicalType\": \"\" } } } ] If you implement the {validateUrl} endpoint, you can use the contents of the allItemTypes array in the request to improve the message in the response. Responses Status Code Type Description Samples 200 Payload Validation Response The POST method completed successfully. 500 The connector failed to validate the request for data. Definitions Daod Request The i2 Connect gateway uses the {validateUrl} endpoint in two places: when a client makes a direct request, and immediately before it uses the {acquireUrl} or a {queriesResource} endpoint. By default, the gateway prefixes {validateUrl} with the base URL from the topology. If validation fails in front of an attempt to acquire data, the gateway does not continue with the attempt. Name Type Notes payload Daod Request- Payload[] A custom payload that an i2 Connect service can use to determine what data to retrieve from a connected source Daod Request- Payload A custom payload that an i2 Connect service can use to determine what data to retrieve from a connected source Name Type Notes conditions Daod Request- Condition[] If the service uses a client configuration of type 'FORM', the conditions that a user has specified to refine their query seeds Daod Seeds[] If the service supports being seeded by existing i2 Analyze records, the data from those records for it to use Daod Request- Condition If the service uses a client configuration of type 'FORM', the conditions that a user has specified to refine their query Name Type Notes id string The identifier of the condition, as specified in the client configuration for the service logicalType string The logical type of the value in the condition value string The value that a user supplied for the condition Daod Seeds If the service supports being seeded by existing i2 Analyze records, the data from those records for it to use Name Type Notes entities Daod Seed Entity Data[] Data from the entity records that were specified as seeds for the request links Daod Seed Link Data[] Data from the link records that were specified as seeds for the request Daod Seed Entity Data Data from the entity records that were specified as seeds for the request Name Type Notes accessDimensionValues Security Dimension And Values[] The security dimension values of the record identified by seedId extensions object Free-form, custom information for the record identified by seedId label string The label of the record identified by seedId properties object The property data of the record identified by seedId seedId object The identifier of a seed record sourceIds Origin Identifier[] The source identifiers of a seed record typeId string The type identifier of the record identified by seedId Security Dimension And Values The security dimension values of the record identified by seedId Name Type Notes dimensionId string The identifier of the security dimension that has the values in ids ids array The identifiers of values in the security dimension with dimensionId Origin Identifier The source identifiers of a seed record Name Type Notes itemTypeId string The identifier of the item type of the record that has this origin identifier key array The values that identify some data in its original source type string The type of this origin identifier Daod Seed Link Data Data from the link records that were specified as seeds for the request Name Type Notes accessDimensionValues Security Dimension And Values[] The security dimension values of the record identified by seedId extensions object Free-form, custom information for the record identified by seedId fromEndId object The identifier of the record at the \"from\" end of the link record identified by seedId fromEndTypeId string The type identifier of the record at the \"from\" end of the link record identified by seedId label string The label of the record identified by seedId linkDirection string The direction of the link record identified by seedId , which overrides any default setting properties object The property data of the record identified by seedId seedId object The identifier of a seed record sourceIds Origin Identifier[] The source identifiers of a seed record toEndId object The identifier of the record at the \"to\" end of the link record identified by seedId toEndTypeId string The type identifier of the record at the \"to\" end of the link record identified by seedId typeId string The type identifier of the record identified by seedId Payload Validation Response The i2 Connect gateway uses the {validateUrl} endpoint in two places: when a client makes a direct request, and immediately before it uses the {acquireUrl} or a {queriesResource} endpoint. By default, the gateway prefixes {validateUrl} with the base URL from the topology. If validation fails in front of an attempt to acquire data, the gateway does not continue with the attempt. Name Type Notes errorMessage string An error message that might be displayed to users"
  },
  "content/schemas/admin-cac.html": {
    "href": "content/schemas/admin-cac.html",
    "title": "Admin user interface",
    "keywords": "Admin user interface The admin user interface enables you to reload the gateway, preview configured services on the gateway, and configure schema mappings from a web browser. To access the admin user interface, the user that you log in as must be a member of a user group that has the i2:Administrator command access control permission. Verify that your user is an Administrator Check that your user has the i2:Administrator command access control permission when using the example deployment: Check that the Administrator user group in the example-command-access-control.xml file in toolkit\\configuration\\fragments\\opal-services\\WEB-INF\\classes has the i2:Administrator permission: <CommandAccessPermissions UserGroup=\"Administrator\"> <Permission Value=\"i2:Administrator\" /> </CommandAccessPermissions> Check that your user belongs to the Administrator group in the user.registry.xml file in i2analyze\\deploy\\wlp\\usr\\shared\\config : For example, if the name of the user is \"Jenny\", you should have the following: <group name=\"Administrator\"> <member name=\"Jenny\"/> </group> If this does not exist, add your user to the Administrator user group to reflect the snippet above. Deploy with changes If you've had to make changes to the example-command-access-control.xml file above redeploy and restart the Liberty server. setup -t deployLiberty setup -t startLiberty Check access to admin user interface In a web browser, go to <i2-Analyze-URL>/admin and check that you can access the admin user interface using your login details. Here, <i2-Analyze-URL> is the URL used to access your i2 Analyze deployment. For example, this might be http://localhost:9082/opaldaod/admin . More information For more information about command access control, see Controlling access to features ."
  },
  "content/schemas/connector-schema.html": {
    "href": "content/schemas/connector-schema.html",
    "title": "Connector Schemas",
    "keywords": "Connector Schemas A connector schema is a type of i2 Analyze schema that is provided by a connector that defines some or all of the item types the connector can return. Connector schemas allow for connectors to be added to, or removed from, i2 Analyze deployments more easily without the need to change the existing Information Store or gateway schemas. They also make it easier for connectors to be shared and used by multiple deployments. For more information about the kinds of schema that you can deploy, see i2 Analyze schemas . To create a connector schema, use i2 Analyze Schema Designer. Schema Designer provides an interface for creating the XML file containing the schema and, optionally, an additional XML file containing the charting schemes. To learn more about how to develop i2 Analyze schemas, see Designing an i2 Analyze schema . Configuring a connector schema A connector schema is provided by the connector itself. To supply a schema, a connector must provide: An endpoint that returns the schema in its XML form from a GET request. The URL for the endpoint. The URL is provided in the schemaUrl field of its configuration, and it must be relative to the connector's base URL. A connector schema can optionally be supplemented with a connector charting scheme. To supply a charting scheme, a connector must provide: An endpoint that returns the charting scheme in its XML form from a GET request. The URL for the endpoint. The URL is provided in the chartingSchemesUrl field of its configuration, and it must be relative to the connector's base URL. For example, if a connector's base URL is http://exampleconnector.com:3700 , the schema and charting schemes endpoints might be accessible at the following URLs: http://exampleconnector.com:3700/schema http://exampleconnector.com:3700/charting-schemes Then, you add the schemaUrl and chartingSchemesUrl fields to the configuration that is returned from the configuration endpoint. For example: { \"schemaUrl\": \"/schema\", \"chartingSchemesUrl\": \"/charting-schemes\", \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ ... ] } Connector schema short names All connector schemas have a short name that is displayed to analysts in i2 Analyst's Notebook Premium when they interact with entity and link types from that schema. The short name of a connector schema can be configured in two ways: The connector can specify the short name the configuration that is returned from its configuration endpoint. You can specify the short name of the schema when defining the connector in the i2 Analyze topology ( topology.xml ). The short name that is supplied in the i2 Analyze topology takes precedence over the one supplied in the connector configuration. If a short name is not specified in either location, then the ID of the connector in the i2 Analyze topology is used. Specifying the schema short name in the connector configuration To specify the short name in the configuration that is returned from the configuration endpoint, use the schemaShortName field of the ConnectorConfig object. For example: { \"schemaUrl\": \"/schema\", \"chartingSchemesUrl\": \"/chartingschemes\", \"schemaShortName\": \"Social Media\", \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ ... ] } For more information about the ConnectorConfig object, see the i2 Connect gateway REST SPI . Specifying the short name in the i2 Analyze topology To specify the short name in the i2 Analyze topology, use the schema-short-name attribute of the connector element. For example: <connector id=\"example-connector\" name=\"Example Connector\" base-url=\"http://localhost:3700/\" schema-short-name=\"Social Media\" />"
  },
  "content/schemas/gateway-schema.html": {
    "href": "content/schemas/gateway-schema.html",
    "title": "Gateway schemas",
    "keywords": "Gateway schemas A gateway schema is a type of i2 Analyze schema that defines the item types that can be used by any number of connectors. A gateway schema allows multiple connectors to return the same item types, which makes them useful in situations where multiple connectors return similar data. Gateway schemas can be used in i2 Analyze deployments that include the i2 Connect gateway. Gateway schemas are optional, and there is no limit to the number of gateway schemas that can be deployed. A connector can return item types from one gateway schema only. For more information about the kinds of schema that you can deploy, see i2 Analyze schemas . To create a gateway schema, use i2 Analyze Schema Designer. This provides an interface for creating the XML file containing the schema and, optionally, an additional XML file containing the charting schemes. To learn more about how to develop i2 Analyze schemas, see Designing an i2 Analyze schema . Gateway schema short names All gateway schemas have a short name that is displayed to analysts in i2 Analyst's Notebook Premium when they interact with entity and link types from that schema. The short name must be unique within the i2 Analyze deployment. You specify the short name when you configure the gateway schema. For a connector to return item types defined in a gateway schema, the connector must be assigned to that gateway schema using the short name of the schema. Configuring a gateway schema To configure a gateway schema, you must have a schema XML file and provide a short name for it. You can also provide a charting schemes XML file. To configure a gateway schema: Copy your schema and charting schemes files to the configuration/fragments/common/WEB-INF/classes directory in your i2 Analyze configuration. Open the ApolloServerSettingsMandatory.properties file in the same directory, and add the following properties Gateway.<SCHEMA_SHORT_NAME>.SchemaResource=<SCHEMA_XML_FILE_NAME> Gateway.<SCHEMA_SHORT_NAME>.ChartingSchemesResource=<CHARTING_SCHEMES_XML_FILE_NAME> For example, to configure a gateway schema that defines item types relating to telecommunications data: The schema file is telecom-schema.xml The charting schemes file is telecom-schema-charting-schemes.xml The short name is Telecom Add the following in ApolloServerSettingsMandatory.properties : Gateway.Telecom.SchemaResource=telecom-schema.xml Gateway.Telecom.ChartingSchemesResource=telecom-schema-charting-schemes.xml Assigning a connector to a gateway schema For a connector to be able to return item types defined in a gateway schema, you must assign it to the gateway schema using the short name of the gateway schema. You can assign the connector in two ways: The connector can supply the short name of the gateway schema to be assigned to in the configuration that is returned from its configuration endpoint. You can specify the short name of the gateway schema that the connector is assigned to when defining the connector in the i2 Analyze topology ( topology.xml ). The gateway schema that a connector is assigned to in the i2 Analyze topology takes precedence over the one specified by the connector in its configuration. Assigning a connector to a gateway schema in the connector configuration In the configuration returned from configuration endpoint, a gateway schema short name can be specified using the gatewaySchema field of the ConnectorConfig object. For example: { \"gatewaySchema\": \"Telecom\", \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ ... ] } For more information about the ConnectorConfig object, see the i2 Connect gateway REST SPI . Assigning a connector to a gateway schema in the i2 Analyze topology To assign a connector to a gateway schema in the i2 Analyze topology, specify the gateway schema short name in the connector element using the gateway-schema attribute. For example: <connector id=\"example-connector\" name=\"Example Connector\" base-url=\"http://localhost:3700/\" gateway-schema=\"Telecom\" />"
  },
  "content/schemas/item-type-mapping.html": {
    "href": "content/schemas/item-type-mapping.html",
    "title": "Item Type Mapping",
    "keywords": "Item Type Mapping With the addition of gateway and connector schemas, it is possible that different schemas contain similar item types. i2 Analyze provides a way for you to map similar item types together. Examples For a detailed walkthrough of how to define item type mappings, see here . To learn more about how you can use item type mapping in your own i2 Analyze deployment, see the following examples using the connectors and schemas provided in this repository: Adding a connector with a connector schema to an existing deployment with a gateway schema Adding a connector with a connector schema to an existing deployment with a gateway schema and an Information Store schema Merging two gateway schemas using item type mapping The importance of item type mapping Having similar item types can cause confusion for analysts, or provide inaccurate analytic results. For example, it might cause issues if you have two distinct item types for modelling a person. Issues can arise because all of the person data that enters the system is split between the two item types. If the two item types for modelling people are Person-A and Person-B, then: It is confusing for users. Analysts might not understand why there are two person types and if they are creating a person record, it may not be clear which type to choose; Person-A or Person-B. It allows incomplete analysis. Having person data split between two types allows users to analyze one type without realising the other exists. For example, an analyst executes a query and wants to add all of the person records in the results to a chart. When filtering the results, it is possible for them to select the Person-A type without selecting the Person-B type, and only add a subset of results to the chart. It encourages duplication of data. i2 Analyze provides matching capabilities that allow users to identify duplicate records and unite them. However, only records of the same type can be matched and united. Therefore, allowing person data to be split between two types prevents i2 Analyze from being able to identify all the duplicate person records. It is possible to have a seeded external search that accepts records of type Person-A but not Person-B. Then, in order to use the search with a Person-B record, an analyst could create a new Person-A record containing the same, or almost the same, data as the Person-B record and use that as the seed. The result would be two person records containing the same data. You should aim to have a single item type for each real-world object modelled by your schemas. To achieve this with multiple schemas providing similar types, i2 Analyze allows you to map types defined in one schema to types defined in another. Continuing with the example above, this means that you could, for example, map Person-A (the source type) to Person-B (the target type). This means that: The only type visible to users is the target type. i2 Analyze maps any records of the source type that are returned by queries to records of the target type. Any seeded searches provided by connectors that accept the source type accept the target type. i2 Analyze applies the mapping in reverse to produce records of the source type, then uses these as seeds when sending requests to connectors. Only users with i2:Administrator command access control permission are able to define the item type mapping configuration. The rules of item type mapping The process shown applies to any mapping you define. There are, however, some restrictions on which types can be mapped to others: Item types in the Information Store schema cannot be the source type of a mapping Item types in connector schemas cannot be the target of any mappings. Only types in the Information Store schema or gateway schemas can be the target of mappings. Any item type mappings between two schemas can only go in one direction. For example, if you map a type from schema A to a type in schema B , you cannot then map a different type from schema B to a type in schema A . Chained mappings If you define a mapping from type A to type B and another mapping from type B to type C , then: Any records of type A are mapped to records of type C by applying both mappings in sequence Any records of type B are mapped to records of type C by applying the relevant mapping Only type C is visible to users Mapping to the Information Store schema versus a gateway schema If your i2 Analyze deployment contains: an Information Store a gateway schema at least one more gateway schema or connector schema Then for any mapping you define, you have the choice to map to either a type in the Information Store schema, or a type in a gateway schema. It's recommended that you map to an Information Store schema item type where possible. This means that: The mapped records can be uploaded to the Information Store You can find records in the Information Store that match the mapped records according to the system match rules, which can reduce duplicated data. Information Store services can be used to analyze the mapped records. For example, you can perform an expand operation on the records."
  },
  "content/schemas/item-type-mapping-config.html": {
    "href": "content/schemas/item-type-mapping-config.html",
    "title": "Configuring item type mappings",
    "keywords": "Configuring item type mappings To show the process of mapping one type to another, you can map the Person type in the NYPD-Complaints schema to the Person type in the KCPD-Crime schema. Examples of both Person types are shown below. Both Person types contain the same property types: age, sex, and race. The main difference between the two is that the NYPD Person type models age in ranges, and the KCPD Person type holds specific values for ages. NYPD Person KCPD Person Accessing the mapping configuration interface In a web browser, go to <i2-Analyze-URL>/admin , where <i2-Analyze-URL> is the URL used to access your i2 Analyze deployment. For example, http://localhost:9082/opaldaod/admin . Log in using credentials with i2:Administrator command access control permission. For more information about ensuring you have the correct permissions, see Admin Interface . The gateway configuration summary is shown as below. For the mapping configuration interface, click the button in the sidebar that is highlighted in red. A list of all the item types across all gateway and connector schemas is displayed, with some information about any item type mappings that have been defined. In this example, no mappings are configured yet. Defining a mapping 1. Select the source type In the list of item types, select the one to map. For this example, the Person type from the NYPD-Complaints connector schema is selected. Then, click on either the arrow shown on the list item, or Create Mapping in the pane on the right. 2. Select the target type A dialog box that contains a list of item types to which your selected source type can be mapped is displayed. Choose an appropriate item type. For this example, the Person type from the KCPD-Complaints gateway schema is selected. After you select the target type, click Create mapping . 3. Define the property mappings At this stage, define how the property values of the source type are mapped to property values of the target type. The properties shown in the dialog are the properties of the target type. For each target property, you can do one of the following: Map a property from the source item type to the target property. When a record of the source item type is mapped to a record of the target item type, the target property is populated with the value of the source property. The logical type of the source property type must be compatible with the logical type of the target property type. Map to a fixed value. When a record of the source item type is mapped to a record of the target item type, the target property is populated with the fixed value provided in the mapping configuration. Leave the property unmapped. When a record of the source item type is mapped to a record of the target type, the target property is not populated. Target property types that are mandatory cannot be left unmapped. Automatically mapped properties A property type of the source item type is automatically mapped to a property type in the target item type if they have the same: display name logical type If the two properties have the logical type SELECTED_FROM or SUGGESTED_FROM, any possible values of the source property that are also present in the target property are mapped automatically to those values in the target property. You can see in the example that this has applied to the race and sex property types. These automatically-generated mappings can still be edited if they do not meet your requirements. Map a property from the source item type To map a property from the source item type to a target property: Select Property value in the left-most dropdown menu below the target property name. Then, choose a property from the source item type in the right-most dropdown. Only compatible properties from the source type are shown. For all logical types, except SELECTED_FROM and SUGGESTED_FROM, this is all that is required. For properties with a logical type of SELECTED_FROM or SUGGESTED_FROM, you might also need to specify how the values are mapped. This is the case in the example when choosing to map the age group property from the NYPD schema to the age property in the KCPD schema. The source age group property is a SUGGESTED_FROM of age groups; the target age property is a SUGGESTED_FROM of specific ages. The values shown in the dialog box are the possible values for the target age property. For each target property, you can do one of the following: Map a value from the source property to it. Select which value of the source property to be mapped to the target value. Each value of the source property cannot be mapped to more than one value of the target value. Leave it unmapped. Select <No value> in the dropdown. Then, no value of the source property type is mapped to this value of the target property type. Similar to how property types are automatically mapped, values of SELECTED_FROM and SUGGESTED_FROM properties are automatically mapped if the value of the target property type is the same as a value of the source property type. This is the case for the \"<18\" and \"65+\" values. For the other target values, you either have to choose an age group from the source value to effectively collapse to a single age, or you can leave them unmapped. Map to a fixed value To map a fixed value: Select Fixed value in the left-most dropdown menu below the target property name. Enter a fixed value to use in the left-most field. Choosing the value for reverse conversions If a connector accepts a particular item type for the seeds of a seeded search and that item type is the source type of a mapping, then users are able to use records of the mapping's target type as seeds when using the service. However, when i2 Analyze sends seed records to the connector, it must apply the mapping in reverse. This is because the connector is expecting seeds of the source item type and is unaware of any mapping configured on the i2 Analyze server. For mappings of properties where the logical type of both the source property and the target property are the same and are not SELECTED_FROM or SUGGESTED_FROM, the reverse conversion is done by simply populating the source property with the value of the target property. When mapping between SELECTED_FROM and SUGGESTED_FROM properties, you can choose how i2 Analyze maps the values of the target property to the possible values of the source property. To do this: Click the button to the right of the dropdown menu for a target value that has been mapped From the dropdown shown, select which one of the source values you have mapped to the target value i2 Analyze should use when applying the mapping in reverse for seeded searches. Confirm your choice by clicking OK . 4. Add a description After you define the property mappings, you can add a description of the item type mapping. 5. Confirm the Mapping Click OK in the bottom-right of the dialog to save your item type mapping. The list of item types shows how the source type has been mapped. Note, at this point your mapping has not yet been deployed on the server. You are able to edit the mapping by clicking Edit mapping in the right-hand pane, or delete the mapping by clicking Delete mapping . Applying the mappings for testing After you define the item type mappings, you can test the i2 Connect services available on your i2 Analyze deployment to see how they would look if you were to apply the mappings you have configured. To do this: Click Apply . This applies the current mapping configuration for testing. Click Preview services . This opens the External Searches dialog as it would appear in Analyst's Notebook Premium. You can use the services defined and see the results as they would appear if the mapping that has been applied for testing was deployed on the server. To revert the mapping configuration back to the current deployed configuration on the server, you can also use the Restore button that is next to Apply . Applying the mapping to the server After you preview the i2 Connect services using your new mappings and you are happy with them, you can deploy your mapping configuration to the i2 Analyze server. To do this: In the mapping configuration interface, click Export in the top right corner. This downloads a mapping-configuration.json file. Move the mapping-configuration.json file into the toolkit/configuration/fragments/common/WEB-INF/classes directory of your configuration. Redeploy the i2 Analyze server: setup -t stopLiberty setup -t deployLiberty setup -t startLiberty"
  },
  "content/schemas/mapping-connector-to-gateway.html": {
    "href": "content/schemas/mapping-connector-to-gateway.html",
    "title": "Adding a connector with a connector schema to an existing deployment",
    "keywords": "Adding a connector with a connector schema to an existing deployment In this example scenario, a new connector with its own connector schema will be added to an i2 Analyze deployment with an existing gateway schema. Item types in the connector schema will be mapped to item types in the gateway schema where possible. Note: These instructions can also apply to mapping item types from one gateway schema to another gateway schema. Setting up the scenario To follow this scenario, start by deploying i2 Analyze with the example NYPD connector configured to use a gateway schema. You should now be able to connect to the i2 Analyze server in Analyst's Notebook Premium and use the services provided by the NYPD connector. For example, you might use the \"Get All\" service and copy some of the results to a chart. Adding the new connector Now, add the example KCPD connector to the deployment , ensuring it is configured to use its own connector schema. You should now also be able to use the services provided by the KCPD connector and copy some results to the chart. But you can see that the gateway schema used by the NYPD connector and the connector schema used by the KCPD connector have different item types to model the same real-world object. This is evident by the different icons used and, in the image shown, you can see pairs of the similar types highlighted in the same color. By looking at all the available item types, you can see that both schemas have their own: Person entity type Location entity type Complaint/Report entity type (these have different names, but they model the same concept) Located At link type Suspect Of link type Victim Of link type This duplication can be removed using item type mappings. Configuring item type mappings Go to the item type mapping configuration in the i2 Analyze Server Admin Console in a web browser to see the list of item types that can be mapped. You should see the list of all item types in the KCPD-Crime schema, and that none of them have been mapped. Where appropriate, KCPD-Crime item types can be mapped to similar item types in the pre-existing NYPD-Complaints gateway schema. Reports and Complaints The Report entity type in the KCPD-Crime schema can be mapped to the Complaint entity type in the NYPD-Complaints schema. Select the Report (KCPD-Crime) item type in the list, then click Create mapping in the right-hand pane. This shows the list of item types to which the Report item type can be mapped. Select the Complaint item type from the NYPD-Complaints schema, then click Create mapping . You will then see all the properties of the Complaint type in the NYPD-Complaints schema, with options available to choose how they should be populated when mapping from a KCPD-Crime Report record. In the example below, the properties of the Report (KCPD-Crime) type are mapped to the properties of the Complaint (NYPD-Complaints) according to the following table. Report (KCPD-Crime) Complaint (NYPD-Complaints) Report Number Complaint Number From Date Complaint Start Date To Date Complaint End Date From Time Complaint Start Time To Time Complaint End Time Offense Description Offence Description Offense Classification Description Report Date Date Reported There is no comparable property in the Report (KCPD-Crime) type for the Complaint (NYPD-Complaints) properties: Crime Status Jurisdiction Code Jurisdiction Description Offence Classification Code Level Of Offence Internal Classification Code Location Of Occurrence So these are left unmapped. This means that they will not be populated for the Complaints records that are mapped from Report records. Once you are satisfied with all the property mappings, confirm the mapping by clicking OK . You should see in the list of types that Report (KCPD-Crime) has been mapped to Complaint (NYPD-Complaints). Locations Following the same process, the Location type from the KCPD-Crime schema can be mapped to the Location type in the NYPD-Complaints schema. The Coordinates property of Location (KCPD-Crime) is automatically mapped to the Coordinates property of Location (NYPD-Complaints). Below is an example configuration you could use. You can see that the Borough Name property of mapped Locations will be populated by the value of the City property from KCPD-Crime Location records. These properties aren't an exact match with one another, but this can be used to make it clear when Locations are in Kansas City, instead of New York City, in case Coordinates are not provided. You can go further and ensure the Borough Name property is always populated in mapped Locations by setting a default value, which will be used if the source KCPD-Crime Location record does not contain a value for the City property. To do this: Click the button highlighted to the right of the Borough Name configuration shown below. Enter a default value to use for the Borough Name property if a source KCPD-Crime Location record has no City value, e.g. \"Kansas City\". Click OK . The mapping of Address to Premises Description does not seem like a perfect match either. But, the Address of a Location might be too important not to have and the Premises Description is a suitable target property in which to store it. Again, once you are happy with the mapping configuration for Location records, click OK to confirm the mapping. People Following the same process, you can map the Person (KCPD-Crime) type to the Person (NYPD-Complaints) type. The Race and Sex property mappings will be automatically-generated, so the only property left is Age. Choose to map the value of the Age property from the Person (KCPD-Crime) type as shown below. The Age property of the source type and the target type are both SUGGESTED_FROM properties, but they have different suggested values. The source property, from the KCPD-Crime schema, has suggested values: <18 , 19 , 20 , 21 , ..., 64 , and 65+ ; whereas the target property, from the NYPD-Complaints schema, has suggested values: <18 , 18-24 , 25-44 , 45-64 , and 65+ . Notice how the <18 and 65+ values are automatically mapped to one another. For the remaining target values ( 18-24 , 25-44 , and 45-64 ), you must choose which source values should map to them. You have the option to select a single value of the source property from the dropdown, but that wouldn't make much sense. For example, the source property values 18 , 19 , 20 , 21 , 22 , 23 , and 24 should all map to the 18-24 target value. To do this, click the button to the right of the dropdown menu beneath 18-24 highlighted below. Then, select all the appropriate values as shown below and select a source value to use when the conversion is applied in reverse during any seeded searches. Located At The Located At (KCPD-Crime) link type can be mapped to the Located At (NYPD-Complaints) link type by following the same process: Select Located At (KCPD-Crime) in the list of types, then click Create mapping in the right-hand pane. Select Located At from the NYPD-Complaints link types, then click Create mapping . There are no properties to map, so just add a description if you wish. Confirm the mapping by clicking OK . Suspect Of Map Suspect Of (KCPD-Crime) to Suspect Of (NYPD-Complaints) in the same way. Victim Of Map Victim Of (KCPD-Crime) to Victim Of (NYPD-Complaints) in the same way. Testing the item type mappings Once you have defined the item type mappings, you should see the updated list of types like the example shown below. The Arrested, Charged, and Complicit In links are left unmapped, because there are no suitable target link types in the existing NYPD-Complaints schema. i2 Analyze will recognize that their end types have been mapped and allow the NYPD-Complaints schema's Person and Complaint types to be connected by these links. To test the item type mappings by previewing the external searches provided by the NYPD and KCPD connectors: Click Apply in the top-right. This applies the mappings to the test environment that is available only through the Admin Console. It does not apply the mappings to the live server. Click Preview services to open a preview of how the services would behave with the mappings you have configured. Notice how the KCPD Connector's Get All service now returns item types from the NYPD-Complaints schema. Go back and make any changes to the mappings, repeating steps 1 and 2 until you are satisfied with the configuration. Applying the item type mappings to the i2 Analyze server To apply the mapping configuration you have created on the i2 Analyze server for all users, see Applying the mapping configuration to the i2 Analyze server . The result In Analyst's Notebook Premium, use some of the services provided by the two connectors and copy some results from each connector to your chart. Notice how there are now no duplicate item types. For example, compare the chart below to the one at the very beginning of this walkthrough in Adding the new connector . You can see that all of the records have types from the NYPD-Complaints schema, except for the unmapped Arrested and Charged links in the top-right corner."
  },
  "content/schemas/mapping-connector-to-gateway-to-infostore.html": {
    "href": "content/schemas/mapping-connector-to-gateway-to-infostore.html",
    "title": "Adding a connector with a connector schema to an existing deployment with a gateway schema and an Information Store schema",
    "keywords": "Adding a connector with a connector schema to an existing deployment with a gateway schema and an Information Store schema <!-- TODO: Doc review --> In this example scenario, a new connector with its own connector schema will be added to an i2 Analyze deployment with an existing gateway schema and an Information Store schema. Item types in the connector schema will be mapped to item types in the gateway schema and item types in the gateway schema will be mapped to item types in the Information Store schema. This will demonstrate how i2 Analyze resolves item type mappings that are chained together. That is, how mapping type A to type B and type B to type C results in an implicit mapping from type A to type C , so all records of type A are mapped to records of type C . You will also learn how mapping to item types in the Information Store schema allows mapped records to be uploaded to the Information Store. Setting up the scenario To follow this scenario: Ensure that you have deployed i2 Analyze with the Information Store and the i2 Connect Gateway using the NYPD schema and charting scheme found in the schema directory of this repository. Add the example KCPD connector to the deployment configured to use a gateway schema. You should now be able to connect to the i2 Analyze server in Analyst's Notebook Premium and use the services provided by the KCPD connector. For example, you might use the \"Get All\" service and copy some of the results to a chart. Adding a new connector with a connector schema Now, add the example ERI connector to the deployment , ensuring it is configured to use its own connector schema. You should now also be able to use the services provided by the ERI connector and copy some results to the chart. You can see that the gateway schema used by the KCPD connector and the connector schema used by the ERI connector have different item types to model the same real-world object. This is evident by the different icons used and, in the image shown, you can see pairs of the similar types highlighted in the same color. By looking at all the available item types, you can see that both schemas have their own: Location entity type Report/Incident entity type (these have different names, but they model the same concept) Located At link type This duplication can be removed using item type mapping. Configuring item type mappings from a connector schema to a gateway schema Go to the item type mapping configuration in the i2 Analyze Server Admin Console in a web browser to see the list of item types that can be mapped. You should see the list of all item types in the ERI-Reports schema, and that none of them have been mapped. Where appropriate, ERI-Reports item types can be mapped to similar item types in the pre-existing KCPD-Crime gateway schema. Incidents and Reports The Incident entity type in the ERI-Reports schema can be mapped to the Report entity type in the KCPD-Crime schema. Select the Incident (ERI-Reports) item type in the list, then click Create mapping in the right-hand pane. This shows the list of item types to which the Incident item type can be mapped. Select the Report item type from the KCPD-Crime schema types, then click Create mapping . You will then see all the properties of the Report type in the gateway schema, with options available to choose how they should be populated when mapping from an ERI-Reports Incident record. In the example below, the properties of the Incident (ERI-Reports) type are mapped to the properties of the Report (KCPD-Crime) according to the following table. Incident (ERI-Reports) Report (KCPD-Crime) Creation Date From Date Closed Date To Date Creation Time From Time Closed Time To Time Incident Subtype Offence Description There is no comparable property in the Incident (ERI-Reports) type for the Report (KCPD-Crime) properties: Report Number Report Date Offense Domestic Violence So these are left unmapped. This means that they will not be populated for the Report records that are mapped from Incident records. Once you are satisfied with all the property mappings, confirm the mapping by clicking OK . You should now see that there is a validation warning message for the mapping. This warning occurs when a mandatory property of the target type is unmapped. This is the case for the mandatory property type Report Number of the Report (KCPD-Crime) item type. Any Report entity created from an Incident entity using this mapping would not have a Report Number, which would make that Report invalid. To resolve this, select the Incident (ERI-Reports) item type in the list, then click Edit mapping in the right-hand pane to update the mapping. In the mandatory Report (KCPD-Crime) property, Report Number, select the dropdown menu and change the type to Fixed value . Populate the field with a constant value. Once the property mapping has been updated, confirm the mapping by clicking OK . Locations Following the same process, the Location type from the ERI-Reports schema can be mapped to the Location type in the KCPD-Crime schema. The Address and Coordinates properties of Location (ERI-Reports) are automatically mapped to the Address and Coordinates properties of Location (KCPD-Crime). Below is an example configuration you could use. Again, once you are satisfied with the mapping configuration for Location records, click OK to confirm the mapping. Located At The Located At (ERI-Reports) link type can be mapped to the Located At (KCPD-Crime) link type by following the same process: Select Located At (ERI-Reports) in the list of types, then click Create mapping in the right-hand pane. Select Located At from the KCPD-Crime schema types, then click Create mapping . There are no properties to map, so just add a description if you wish. Confirm the mapping by clicking OK . Testing the item type mappings Once you have defined the item type mappings, you should see the updated list of types like the example shown below. To test the item type mappings by previewing the external searches provided by the KCPD and ERI connectors: Click Apply in the top-right. This applies the mappings to the test environment that is available only through the Admin Console. It does not apply the mappings to the live server. Click Preview services to open a preview of how the services would behave with the mappings you have configured. Notice how the ERI Connector's All data service now returns item types from the KCPD-Crime schema. Go back and make any changes to the mappings, repeating steps 1 and 2 until you are satisfied with the configuration. Applying the item type mappings to the i2 Analyze server To apply the mapping configuration you have created on the i2 Analyze server for all users, see Applying the mapping configuration to the i2 Analyze server . The result In Analyst's Notebook Premium, use some of the services provided by the connectors and copy some results from each connector to your chart. Notice how there are now no duplicate item types. For example, compare the chart below to the one at the very beginning of this walkthrough in Adding a new connector with a connector schema . You can see that all of the records have types from the KCPD-Crime schema including the records from the ERI connector. This is because of the mapping configuration created that maps the item types in the ERI-Reports schema to the item types in the KCPD-Crime schema. Uploading gateway mapped records to the Information Store Notice that these records cannot be uploaded to the Information Store. This is because the item types of the records are defined in the KCPD-Crime gateway schema, not the Information Store schema. Next, you can define mappings from the gateway schema to the Information Store schema and see that the records returned from both connectors can then be uploaded to the Information Store. Configuring item type mappings from a gateway schema to an Information Store schema Go to the item type mapping configuration in the i2 Analyze Server Admin Console in a web browser to see the list of item types that can be mapped. You should see the list of all item types in the KCPD-Crime schema, and that none of them have been mapped. Where appropriate, KCPD-Crime item types can be mapped to similar item types in the NYPD-Complaints Information Store schema. Reports and Complaints The Report entity type in the KCPD-Crime schema can be mapped to the Complaint entity type in the Information Store schema. Select the Report (KCPD-Crime) item type in the list, then click Create mapping in the right-hand pane. This shows the list of item types to which the Report item type can be mapped. Select the Complaint item type from the Information Store schema types, then click Create mapping . You will then see all the properties of the Complaint type in the Information Store schema, with options available to choose how they should be populated when mapping from a KCPD-Crime Report record. In the example below, the properties of the Report (KCPD-Crime) type are mapped to the properties of the Complaint (InfoStore) according to the following table. Report (KCPD-Crime) Complaint (InfoStore) Report Number Complaint Number From Date Complaint Start Date To Date Complaint End Date From Time Complaint Start Time To Time Complaint End Time Offense Description Offence Description Offense Classification Description Report Date Date Reported There is no comparable property in the Report (KCPD-Crime) type for the Complaint (InfoStore) properties: Crime Status Jurisdiction Code Jurisdiction Description Offence Classification Code Level Of Offence Internal Classification Code Location Of Occurrence So these are left unmapped. This means that they will not be populated for the Complaints records that are mapped from Report records. Once you are satisfied with all the property mappings, confirm the mapping by clicking OK . Locations Following the same process, the Location type from the KCPD-Crime schema can be mapped to the Location type in the Information Store schema. The Coordinates property of Location (KCPD-Crime) is automatically mapped to the Coordinates property of Location (InfoStore). Below is an example configuration you could use. You can see that the Borough Name property of mapped Locations will be populated by the value of the City property from KCPD-Crime Location records. These properties are not an exact match for one another, but this can be used to make it clear when Locations are in Kansas City, instead of New York City, in case Coordinates are not provided. You can go further and ensure the Borough Name property is always populated in mapped Locations by setting a default value, which will be used if the source KCPD-Crime Location record does not contain a value for the City property. To do this: Click the button highlighted to the right of the Borough Name configuration shown below. Enter a default value to use for the Borough Name property if a source KCPD-Crime Location record has no City value, e.g. \"Kansas City\". Click OK . Though the mapping of Address to Premises Description does not seem like a perfect match either, the Address of a Location might be too important not to have and the Premises Description is a suitable target property in which to store it. Again, once you are satisfied with the mapping configuration for Location records, click OK to confirm the mapping. People Following the same process, you can map the Person (KCPD-Crime) type to the Person (InfoStore) type. The Race and Sex property mappings will be automatically-generated, so the only property left is Age. Choose to map the value of the Age property from the Person (KCPD-Crime) type as shown below. The Age property of the source type and the target type are both SUGGESTED_FROM properties, but they have different suggested values. The source property, from the KCPD-Crime schema, has suggested values: <18 , 19 , 20 , 21 , ..., 64 , and 65+ ; whereas the target property, from the Information Store schema, has suggested values: <18 , 18-24 , 25-44 , 45-64 , and 65+ . Notice how the <18 and 65+ values are automatically mapped to one another. For the remaining target values ( 18-24 , 25-44 , and 45-64 ), you must choose which source values should map to them. You have the option to select a single value of the source property from the dropdown, but that wouldn't make much sense. For example, the source property values 18 , 19 , 20 , 21 , 22 , 23 , and 24 should all map to the 18-24 target value. To do this, click the button to the right of the dropdown menu next to the 18-24 as highlighted below. Then, select all the appropriate values as shown below and select a source value to use when the conversion is applied in reverse during any seeded searches. Located At The Located At (KCPD-Crime) link type can be mapped to the Located At (InfoStore) link type by following the same process: Select Located At (KCPD-Crime) in the list of types, then click Create mapping in the right-hand pane. Select Located At from the Information Store schema types, then click Create mapping . There are no properties to map, so just add a description if you wish. Confirm the mapping by clicking OK . Suspect Of Map Suspect Of (KCPD-Crime) to Suspect Of (InfoStore) in the same way. Victim Of Map Victim Of (KCPD-Crime) to Victim Of (InfoStore) in the same way. Testing the item type mappings Once you have updated the item type mappings, you should see the list of types like the example shown below. The Arrested, Charged, and Complicit In links are left unmapped, because there are no suitable target link types in the existing Information Store schema. i2 Analyze will recognize that their end types have been mapped and allow the Information Store schema's Person and Complaint types to be connected by these links. To test the item type mappings by previewing the external searches provided by the KCPD and ERI connectors: Click Apply in the top-right. This applies the mappings to the test environment that is available only through the Admin Console. It does not apply the mappings to the live server. Click Preview services to open a preview of how the services would behave with the mappings you have configured. Notice how the KCPD Connector's Get All service and ERI Connector's All data service now returns item types from the Information Store schema. Go back and make any changes to the mappings, repeating steps 1 and 2 until you are satisfied with the configuration. Applying the item type mappings to the i2 Analyze server To apply the mapping configuration you have created on the i2 Analyze server for all users, see Applying the mapping configuration to the i2 Analyze server . The result In Analyst's Notebook Premium, use some of the services provided by the connectors and copy some results from each connector to your chart. Notice how the KCPD and ERI connectors now return records whose item types are defined in the Information Store schema. The KCPD connector returns records whose item types are defined in the KCPD-Crime schema, which are mapped to item types in the Information Store schema directly. The ERI connector returns records whose item types are defined in the ERI-Reports schema. These item types are mapped to item types in the KCPD-Crime schema that are themselves mapped to item types defined in the Information Store schema. What happens as a result is that the records returned by the ERI connector are mapped to records that align with the Information Store schema. For example, compare the chart below to the one at the very beginning of this walkthrough in Adding a new connector with a connector schema . You can see that all of the records have types from the Information Store schema, except for the unmapped Complicit In link in the top-left corner. Uploading Information Store mapped records to the Information Store If you want to upload the selected records to the Information Store, you will notice that the button now enabled. Click Upload Records and in the open window, you'll notice that one of the records would not be uploaded because their item type is not from the Information Store schema. This record would be the unaligned Complicit In link from the KCPD-Crime connector in the top-left corner. If you want to upload the unaligned Complicit In link item type, you need to return to the earlier step in Configuring item type mappings from a gateway schema to an Information Store schema to configure a mapping for that item type. Otherwise, proceed to click on Upload to upload the aligned records to the Information Store."
  },
  "content/schemas/mapping-merge-two-schemas.html": {
    "href": "content/schemas/mapping-merge-two-schemas.html",
    "title": "Merging two schemas",
    "keywords": "Merging two schemas <!--TODO: Doc review --> In this example scenario, two connectors - each with their own gateway schema - will be used in an i2 Analyze deployment. There will be similar item types in the two schemas, so item type mappings will be used to remove duplicate types. Item type mappings cannot be defined in both directions between the two schemas. That is, it is not possible to map item types from schema A to schema B and, in turn, map item types from schema B to schema A. You will see how to overcome this by creating a third gateway schema that contains all the types we want to include from the two original schemas before defining mappings to the new schema. Setting up the scenario To follow this scenario, deploy i2 Analyze with the example NYPD connector and KCPD connector both configured to use gateway schemas. Reviewing the item types Open the schema/nypd-complaint-data-schema.xml and schema/kcpd-crime-data-schema.xml schemas in Schema Designer to see the item types they provide. Notice that both schemas have entity types to represent: people complaints/reports locations They both also have link types to represent: a complaint having occurred at a location a person being the victim of a complaint a person being the suspect of a complaint The KCPD-Crime schema has additional links to represent: a person being complicit in a complaint a person having been arrested for a complaint a person having been charged for a complaint There are obviously duplicate item types, which can be resolved by defining item type mappings. Review the item types and decide which of the duplicate types you prefer. For the purpose of this example, the preferred item types where there are duplicates will be: Person (NYPD-Complaints) Report (KCPD-Crime) Location (KCPD-Crime) Located At (NYPD-Complaints) Suspect Of (NYPD-Complaints) Victim Of (NYPD-Complaints) However, item type mappings can only be defined between two schemas in one direction. So it is not possible to map the Person type in the KCPD-Crime schema to the Person type in the NYPD-Complaints schema and map the Complaint type in the NYPD-Complaints schema to the Report type in the KCPD-Crime schema. To overcome this, you can create a new schema containing all of the preferred item types from both schemas. Then, once that is configured as a gateway schema, types from the NYPD-Complaints and KCPD-Crime schema can both be mapped to it. Creating a new schema Use Schema Designer to create a new schema containing types identical to the preferred item types you chose from the NYPD-Complaints and KCPD-Crime schemas. An example containing the types listed above is provided in schema/nypd-kcpd-merged-schema.xml . Configure this new schema as a gateway schema in your deployment. For more information on how to do this, see Gateway schemas . Be sure to choose an appropriate short name for this schema, e.g. \"NYPD-KCPD-Merged\". Configuring item type mappings Following the process outlined in Configuring item type mappings , you can define mappings of types from the NYPD-Complaints and KCPD-Crime schemas to types in the NYPD-KCPD-Merged schema. Examples of specific mappings you might define are outlined below, but the general idea is to map pairs of duplicate types in the NYPD-Complaints and KCPD-Schema to their corresponding type in the new merged schema. Start by opening the i2 Analyze Server Admin Console . Entities People The preferred Person type listed above is the one defined in the NYPD-Complaints schema. This is copied by the Person type in the new NYPD-KCPD-Merged schema, so mappings can be defined from the Person types in the NYPD-Complaints and KCPD-Crime schemas to the Person type in the NYPD-KCPD-Merged schema. First, create a mapping from Person (NYPD-Complaints) to Person (NYPD-KCPD-Merged). Since these two types are identical, all the properties will have mappings generated automatically, as shown below. All that is needed is to click OK to confirm the mapping. Second, create a mapping from Person (KCPD-Crime) to Person (NYPD-KCPD-Merged) and map the properties how you would if you were mapping to the NYPD-Complaints Person type. Example property mappings you might define are shown below. Locations Similarly, the Location (NYPD-Complaints) and Location (KCPD-Crime) types can both be mapped to Location (NYPD-KCPD-Merged). The Location (NYPD-KCPD-Merged) type is identical to Location (KCPD-Crime), since that is the preferred Location type as listed above. Start by mapping Location (KCPD-Crime) to Location (NYPD-KCPD-Merged), making use of the automatically-generated property mappings. Then, map Location (NYPD-Complaints) to Location (NYPD-KCPD-Merged). Examples of property mappings you might define are shown below. Reports Following the same process, map the Complaint (NYPD-Complaints) and Report (KCPD-Crime) types to the Report (NYPD-KCPD-Merged) type. The Report type in the merged schema is identical to the Report type in the KCPD-Crime schema, so once again all the property mappings will be populated for you in that case. The mapping of Complaint (NYPD-Complaints) to Report (NYPD-KCPD-Merged) might be defined as follows. Links The Located At, Suspect Of, and Victim Of link types in both the NYPD-Complaints and KCPD-Crime schemas can be mapped to the corresponding link types in the NYPD-KCPD-Merged schema, which are identical to the appropriate NYPD-Complaints link types. Once all mappings are defined, you should see that all types - except the Complicit In, Arrested, and Charged links from the KCPD-Crime schema - have been mapped to the NYPD-KCPD-Merged schema. Click Apply in the top-right. This applies the mappings to the test environment that is available only through the Admin Console. It does not apply the mappings to the live server. Click Preview services to open a preview of how the services would behave with the mappings you have configured. Go back and make any changes to the mappings, repeating steps 1 and 2 until you are satisfied with the configuration. Applying the item type mappings to the i2 Analyze server To apply the mapping configuration you have created on the i2 Analyze server for all users, see Applying the mapping configuration to the i2 Analyze server . The result By creating a new gateway schema and defining item type mappings to it, you have mitigated the problems caused by duplicate or similar item types in the NYPD-Complaints and KCPD-Crime schemas without being limited to defining mappings in just one direction between them."
  },
  "content/schemas/type-location.html": {
    "href": "content/schemas/type-location.html",
    "title": "Type Location",
    "keywords": "Type Location <!-- Doc Review --> The type location of an item type used by a connector indicates where - between the Information Store schema, the gateway schema used by the connector, or its own connector schema - the item type is defined. With the introduction of gateway and connector schemas, it has become important for i2 Analyze to determine the origin of item types. It is possible for two schemas to have item types with the same item type ID; in which case, the gateway will attempt to infer the type location by looking in the following order: The connector's own schema The gateway schema used by the connector The Information Store schema A connector can explicitly specify the type locations for item types to avoid ambiguity and ensure that the desired item type from the appropriate schema is used. Default Type Locations You can specify the default type location for all entities and links returned from the connector in the defaultValues field of the connector configuration as follows: { \"defaultValues\": { \"entityTypeLocation\": \"[LOCATION]\", \"linkTypeLocation\": \"[LOCATION]\" }, \"services\": { /* ... */ } } Legal values for [LOCATION] are as follows: INFOSTORE - Indicates that the item type returned is from the Information Store schema. GATEWAY - Indicates that the item type returned is from the gateway schema used by the connector. CONNECTOR - Indicates that the item type returned is from the connector's own schema. Result Item Type Locations A service's resultItemTypeIds property specifies the item types expected to be returned from the service. It also provides type location overrides for specific item types using the following structure: { \"<LOCATION>\": [\"TYPEID1\", \"TYPEID2\", /* ... */ ] } Where <LOCATION> can be INFOSTORE , GATEWAY or CONNECTOR . Since a service can use item types from all three type locations, all three type locations can be specified. For example, a service which returns entity type ET1 from the Information Store schema, ET2 the gateway schema and ET3 from a connector schema will look like the following: { \"services\": [ { /* ... */ \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"LT1\"], \"GATEWAY\": [\"ET2\", \"LT2\"], \"CONNECTOR\": [\"ET3\", \"LT3\"] } } ] } Specifying type locations in this way will override the defaultValues set for entityTypeLocation and linkTypeLocation . Type Locations for Seed Constraints It is also possible to specify the type locations of seed items by including typeLocation attributes for each allowed item type of a service's seed constraint definition. This is shown as follows: { \"services\": [ { /* ... */ \"resultItemTypeIds\": { \"INFOSTORE\": [\"ET1\", \"LT1\"], \"GATEWAY\": [\"ET2\", \"LT2\"], \"CONNECTOR\": [\"ET3\", \"LT3\"] }, \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"ET1\", \"typeLocation\": \"INFOSTORE\", \"min\": 1, \"max\": 1 }, { \"id\": \"ET3\", \"typeLocation\": \"CONNECTOR\", \"min\": 1, \"max\": 1 } ] } } } ] }"
  },
  "content/walkthrough/1-schema-design-guide.html": {
    "href": "content/walkthrough/1-schema-design-guide.html",
    "title": "Designing an i2 Analyze schema",
    "keywords": "Designing an i2 Analyze schema i2 Analyze schemas are at the core of any i2 Analyze deployment. They define the types of entities, links, and properties that users can view and analyze in Analyst's Notebook Premium. This section guides you through the process of developing a schema that aligns with the data the connector will retrieve. The process of developing a schema is iterative. Schema design requires fine tuning, especially if you are unfamiliar with the data set that you are creating the schema for. After you deploy i2 Analyze, you will be able to experiment with and test the schema you create. You can refer back to this document to repeat the process until you have a schema that you believe represents the NYPD Complaint Dataset. If you already understand the schema development process, you can use the example schema from this repository. Prerequisites Before you start, ensure that you have installed Analyst's Notebook Premium with the i2 Analyze Schema Designer. The i2 Analyze schema There are three integral components of an i2 Analyze schema: Entity types . These represent real-world concepts such as person, location, bank account, or event (\"meeting\", for example). Link types . A link type describes a relationship between two entity types, such as a person's ownership of a vehicle or a transaction between two bank accounts. Link types can specify exactly which entity types can appear at their ends, or they can represent a general association between entities of any types. A link between entities on a chart is represented by a line between them, which will be an arrow if the link is directed. Property types . Entities and links have properties, which store information about the object, event, or relationship. For example, a person might have a name, date of birth, and eye color; while a meeting might have a date, a time, and a duration. The property types of an entity or link type define exactly what properties the records of that type can have. Getting to know the data The data that your connector will retrieve comes from the NYPD Complaint Dataset . Before you build your schema, you should explore the data and decide how to model it as entities, links, and properties. You can also consult the data API and look at the raw data if you want to. Creating the schema Open i2 Analyze Schema Designer. From the File menu, click New Schema . Adding entity types You need to create entity types that can represent different objects. These types will relate to the supplied NYPD Complaint Dataset, and you can decide what they will be. One example of an entity type used in the sample schema is a Complaint . You can follow the instructions below to simulate creating the example schema with this entity type, or you can create your own types. From the Insert menu, click New Entity Type . Enter a name and a description for the entity type. (For example, Name: Complaint, Description: \"A complaint regarding an incident that occurred between some individuals\".) Select an icon to represent entities of this type in Analyst's Notebook Premium. (For example, a notepad.) You can create any entity types you like, provided that they align with the data. For example, you could have entity types to represent people and locations. Look at the example schema for inspiration if you need to. Adding property types to entity types For the entity types that you created, decide which fields in the data provide information that can be stored in properties. To continue with the Complaint example, some property types that might be suitable include: Complaint Number Complaint Start Date Complaint End Date Level of Offense Offense Description These property types are valuable because they allow for additional analysis to occur in Analyst's Notebook Premium. They provide the opportunity to conduct live formatting on the data, as well as to create different charting schemes for when data needs to be displayed in a different fashion. This is explained in more detail in Creating a charting scheme , below. To add a property type to an entity type: Select the entity type in the navigation tree. From the Insert menu, click New Property Type . Enter a name and a description for the new property type. (For example, Name: Complaint Number, Description: \"Persistent ID for each complaint\".) Assign a logical type that describes the type of data that properties of this type will contain. (For example, Integer .) Select the Is Mandatory checkbox if you want to enforce that all entities of this type have a property of this type. Adding link types Decide how the entity types you have created can be related. An example of a link type in the supplied schema is Located At. You can follow the instructions below to simulate creating the example schema with this link type, or you can create your own link types: From the Insert menu, click New Link Type . Enter a name and a description for the link type. (For example, Name: Located At, Description: \"Where the incident was located\".) Links often make sense only between certain entity types. To choose which entities can be connected by a link of this type, open the Link Ends tab and select the appropriate From End Types and To End Types . (For example, From End: Complaint, To End: Location.) Define as many link types as you like, ensuring that they align with the data. For example, a person might be linked to a complaint if they are listed as the victim in the complaint. You might therefore have a Victim In link type that connects Person entities to Complaint entities. Similarly, you might define a a Suspect In link type. Adding property types to link types For the link types that you have created, decide if there are fields in the data set that can provide properties for them. The example schema contains no links with properties, and you can complete the demonstration without them, but they might be useful for the connectors that you make after following this guide. To add a property type to a link type: Select the link type in the navigation tree. From the Insert menu, click New Property Type . Enter a name and a description for the new property type. Assign a logical type that describes the type of data that properties of this type will contain. For example, Single Line String or Integer . Select the Is Mandatory checkbox if you want to enforce that all links have a property of this type. Creating a charting scheme When you have defined all the entity, link, and property types you want to model, you must create a charting scheme to determine how items that contain your entity and link records appear in Analyst's Notebook Premium. For example, the charting scheme determines how labels on the chart are generated from properties of entities and links. To create a charting scheme: From the File menu, click Edit Charting Schemes . Expand the first charting scheme in the left panel to reveal Entity Types and Link Types . Expand Entity Types and Link Types to reveal the entity and link types that you created earlier. Right-click Properties and select Insert Chart Item Property Type\" . From the dropdown, select Label . In the Label tab, set the text to be displayed with entities of this type in Analyst's Notebook Premium to distinguish them on the chart. You can use the values of properties assigned to a record to construct your label. To use a property value, click the downward arrow on the Insert button and select the property type of your choice. In the example charting scheme, the Complaint Number is used in the label of Complaint entities. You can prefix the value of a property with a space, a newline, or some custom text. The prefix can clarify what data is represented in the label. For example, a Complaint Number might be prefixed with the text \"Complaint \". This is displayed in Schema Designer as follows: A Complaint with the Complaint Number 667574574 will then be displayed in Analyst's Notebook Premium like this: You can also add a suffix in the same way, which can be useful if you use multiple property values in the label. For example, you might display values on separate lines, or with spaces between them. For example, Person entities in the example are shown as follows: You can also set a default value to use when propert values are not set. In this case, the default value is \"NO PROPERTIES\". This range of options allows you to customize your charting scheme in a way that best represents your data. To combine multiple properties into a single label, follow the above process again. You can add more charting schemes to visualize the data in different ways. The supplied example schema contains two charting schemes - one with labels (detailed version) and one without labels (simplified version). To create further charting schemes, repeat the above instructions with different (or no) labels. A group of entities and links with a detailed charting scheme might look like this: This charting scheme provides a substantial amount of information to the analyst, allowing them to see details of their entities and links at a glance. However, there might be times when this much information is overwhelming, especially in a chart with lots of records. For such cases, you might use a simpler charting scheme that just gives an overview of data in the records. A group of entities and links with a simplified charting scheme might look like this: When you have finished making your changes, click OK to close the charting scheme editor. See the Knowledge Center for information about adding property types that are not used in the example schema. Link summarizations Sometimes, multiple links exist between the same two entities on a chart in Analyst's Notebook Premium. You can use the charting scheme to choose how to represent these links. The different link summarizations that you can use are: Link Option Description Single Link All links of the selected type between the same two entities are combined into a single link. This option is useful if you are producing a summary chart and do not want to show all the detail of the data between entities. Directed All links of the selected type in the same direction between the same two entities are combined. This option is useful if you are charting information such as telephone calls or transactions. Multiple Each link of the selected type between the same two entities is charted separately. This option is useful if you want to show all the detail. If there are many links, you might make the chart cluttered or hard to read. Flow All links of the selected type between the same two entities are combined into a single, directed link. The direction (or flow) is determined from a property that you specify. For example, if there are several financial transactions between two bank accounts, this option is useful to determine the direction that the aggregate amount of the money is flowing. The example schema uses link multiplicity (Multiple) to show all the data flowing to and from entities. If you want to change this setting, this is the time to do so! You are likely to use other settings in your own connectors. Live Formatting in Analyst's Notebook Premium Live formatting in Analyst's Notebook Premium is a type of Conditional Formatting that changes the appearance of chart items, by applying conditional formatting specifications in real time. This allows analysts to format their charts to their own specifications. The default appearance of chart items is defined by your schema, but Live Formatting extends this to provide for filtering and quick, real time insight. An example of this functionality in action with the example schema is with the Sex property of the Person entity. Live Formatting allows for quick, visual confirmation of whether a Person is a Male or a Female (pink for Male, blue for Female). This example can be seen as follows. If you wish to know more about Live Formatting, including information regarding its setup, how to define a specification, and formatting system messages, you can visit the Knowledge Center . Saving the schema From the File menu, click Save Schema . Choose a location to save your schema, then click Save . The schema and charting schemes will both be saved in your chose location as XML files. Now that you have defined your schema, you need to deploy i2 Analyze ."
  },
  "content/walkthrough/2-deploy-i2-analyze.html": {
    "href": "content/walkthrough/2-deploy-i2-analyze.html",
    "title": "Deploying i2 Analyze with the i2 Connect gateway",
    "keywords": "Deploying i2 Analyze with the i2 Connect gateway <!-- TODO: Doc review this paragraph --> This guide walks you through how to deploy IBM i2 Analyze with the i2 Connect gateway only. To deploy IBM i2 Analyze with the Information Store and the i2 Connect gateway, follow this guide. Prerequisites Before you start, ensure that you have: installed i2 Analyze, and defined a schema to model the data. License acknowledgement Open the license_acknowledgement file in your i2 Analyze directory. Set the value to ACCEPT . It should now look like this: LIC_AGREEMENT = ACCEPT Configuration Create the configuration directory In your i2 Analyze directory, navigate to toolkit\\examples\\configurations\\daod-opal . Copy the configuration directory to the toolkit directory. This provides a starting point for a deployment that includes only the i2 Connect gateway. Specify the credentials for deployment Using a text editor, open the toolkit\\configuration\\environment\\credentials.properties file. Specify a user name and password to use for the Solr indexes in the solr.user-name and solr.password properties. Enter the password to encrypt LTPA tokens in the ltpakeys.password property. Save and close the file. Command access control To gain access to certain features, including the ability to use Postman to reload the connector when making schema changes, you need to copy and modify some files in the i2 Analyze deployment: Navigate to the toolkit\\configuration\\examples\\security-schema directory and copy the file named example-command-access-control-daod.xml . Navigate to the toolkit\\configuration\\fragments\\opal-services\\WEB-INF\\classes directory and paste the file from the previous step. Open the DiscoServerSettingsCommon.properties file and add the name of file you just copied to the CommandAccessControlResource field, including the .xml extension. Save and close the properties file. Configure the schema To test your schema and use it in Analyst's Notebook before deploying a connector, you can configure it as a gateway schema. The steps below describe this process. Copy your schema and charting schemes to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to your schema files by setting the following properties: Gateway.NYPD-Complaints.SchemaResource=schema-filename.xml Gateway.NYPD-Complaints.ChartingSchemesResource=charting-schemes-filename.xml \"NYPD-Complaints\" is the short name chosen for the schema in this example. You may choose a different short name by replacing \"NYPD-Complaints\" if you wish. The schema short name will appear in the labels of items on a chart in Analyst's Notebook Premium. Configure the security schema All i2 Analyze deployments require a security schema, which defines the level of access users have to the data in the system. You can learn about the i2 Analyze security model in the Knowledge Center but, for the purpose of this guide, follow the steps below to use an example security schema. Copy example-dynamic-security-schema.xml from toolkit\\configuration\\examples\\security-schema to the toolkit\\configuration\\fragments\\common\\WEB-INF\\classes directory. Update the ApolloServerSettingsMandatory.properties file in the same directory to point to the security schema by setting the following property: DynamicSecuritySchemaResource=example-dynamic-security-schema.xml Generate the default configuration For the purposes of this guide, only a basic configuration is required, so you can use the default. Open the toolkit\\scripts directory in a command prompt. To populate some of the mandatory settings with default values, run: setup -t generateDefaults In the i2analyze directory, navigate to the toolkit\\configuration\\environment directory and open the topology.xml file in a text editor. Replace every instance of the host-name tag with the value localhost . There will be three to change in total. This will be used to test your application in Postman. Deployment Open the toolkit\\scripts directory in a command prompt. To deploy i2 Analyze with the configuration you have just created, run setup -t deploy To add an example user whose name and password are both \"Jenny\", run: setup -t ensureExampleUserRegistry To start i2 Analyze, run setup -t start You can now connect Analyst's Notebook Premium to i2 Analyze. The output of the start command includes the URL to use for the i2 Analyze server. This will be the best time to test the schema you created in the previous step, or the existing one if you are using the example schema provided. You can open Analyst's Notebook and start to drag/drop entities and links onto your chart, testing different charting schemes and seeing what labels appear and how they are formatted. For these changes to take effect, you need to update the i2 Analyze connectors configuration and run the internal Reload command via Postman. First, navigate to i2analyze\\toolkit\\scripts in your console and run: setup -t updateConnectorsConfiguration Next you need to open Postman , select the i2 Analyze with i2 Connect Gateway environment and run the Form Based Login request. This will authenticate you as the default user in i2 Analyze, then you can run the Reload request which will configure your schema changes to the topology. (NOTE: You will need to perform a reload command for every iteration of your schema design, otherwise the changes will not take effect). You now have a running i2 Analyze deployment, if you are happy with your schema you can now deploy a connector . However, if there are changes you wish to make to the schema and associated charting schemes, you can run yourself through the schema design guide again."
  },
  "content/walkthrough/3-deploy-connector.html": {
    "href": "content/walkthrough/3-deploy-connector.html",
    "title": "Deploying a connector",
    "keywords": "Deploying a connector In this task, you deploy a minimal connector with a configuration endpoint that defines just one service. This is the minimum required for any connector to interact with the i2 Connect gateway. If you have any issues during this task, the troubleshooting guide might be helpful. Prerequisites Before you start, ensure that you have: deployed i2 Analyze with the i2 Connect gateway, and defined a schema to model the data, and installed Analyst's Notebook Premium and connected it to your i2 Analyze deployment Adding the connector to the i2 Analyze topology This is how to configure the i2 Analyze server so that it knows about your connector. Add the connector to the topology.xml In the i2analyze directory, navigate to the toolkit\\configuration\\environment directory and open the topology.xml file in a text editor. In the opal-services-daod <war> element, if it is not already there, add a <connector-ids> element. Inside that, add a <connector-id> element with a value attribute that matches the unique identifier of your connector. For example: <wars> <war ... name=\"opal-services-daod\" ... > ... <connector-ids> <connector-id value=\"nypd-crime-connector\"/> </connector-ids> ... </war> </wars> Inside the <topology> element, add a <connectors> element. In here, add a <connector> element with id , name , base-url , and gateway-schema attributes. Choose whatever id and name you like, but set the base-url to http://localhost:9081/ and the gateway-schema to the short name of your schema. For example: <ns1:topology ...> ... <connectors> <connector base-url=\"http://localhost:9081/\" name=\"NYPD Crime Connector\" id=\"nypd-crime-connector\" gateway-schema=\"NYPD-Complaints\"/> </connectors> </ns1:topology> Reload the i2 Analyze configuration For these changes to take effect, you need to update the i2 Analyze connectors configuration and use Postman to make the server reload it. Navigate to i2analyze\\toolkit\\scripts in your console and run: setup -t updateConnectorsConfiguration Open Postman and run the Form Based Login request to authenticate as the default user in i2 Analyze. Call the POST method on the reload endpoint to enact your changes to the topology. Use Postman to verify the changes To confirm that you have successfully added a connector to the topology, you can send an HTTP GET request to the i2 Analyze server. Although you don't yet have a running connector, this ensures that you have configured i2 Analyze so that it can find the one you will soon create. Open Postman and call GET on the connectors endpoint. In the response, the connectors array includes an entry that corresponds to the connector you just defined. However, there is also an error and a message saying that i2 Analyze failed to retrieve configuration information for your connector. This is because the connector doesn't exist yet! Deploying the connector Now that i2 Analyze is expecting a connector, you had better create one! For the purposes of this guide, an example Spring Boot application is provided. Set up the example starting point Java Open the example starting code from the stage1/nypd-connector folder. Copy the nypd-connector directory to wherever you would like to work, and open it in VSCode or the IDE of your choice. Have a look at the contents: Resources: config.json and application.properties Code: a simple Spring Boot application The config.json file tells Analyst's Notebook what services are available and how to execute them. The application.properties file defines the server ports, the database URLs, and the API token to aid in the connection. Start the connector. See running example connectors in Java . Node.js Open the example starting code from the stage1/nypd-connector folder in VSCode, or the IDE of your choice. Have a look at the contents: Resources: public/json/config.json Code: a simple Express application The config.json file tells Analyst's Notebook what services are available and how to execute them. The app.js file is where the CONTEXT_ROOT and all the connector's endpoints are defined (for now, just the /config endpoint). The routes folder is where all defined routes are implemented (for now, only the config route). Start the connector through the command line . Python Open the example starting code from the stage1/nypd-connector folder. Copy the nypd-connector directory to wherever you would like to work, and open it in VSCode or the IDE of your choice. Have a look at the contents: Resources: config.json Code: a simple Flask application. The config.json file tells Analyst's Notebook what services are available and how to execute them. Start the connector through the command line . Use Postman to investigate the config endpoint The supplied starting code has a config endpoint that is ready to use: In Postman, call the GET method of the config endpoint and look at the output. It matches the contents of the config.json file that you set up earlier. Update the i2 Analyze connectors configuration Now that you have a connector running, you need to tell i2 Analyze to reload its connector configurations. Run setup -t updateConnectorsConfiguration . Call the POST method on the reload endpoint to configure your changes to the topology. View the i2 Analyze connectors configurations in Postman Exactly as before, open Postman and call GET on the connectors endpoint. You no longer have the \"Failed to retrieve configuration\" error, but it has been replaced by two others. Investigate in Analyst's Notebook Premium Before you fix these issues, take a look at what happens when you try to use your connector from Analyst's Notebook Premium. Open Analyst's Notebook Premium and log in to the i2 Analyze server. In the top ribbon, click the External Searches button. Notice the banner at the top of the resulting window that says \"Some queries are not configured correctly.\" Click the DETAILS button and look at the messages. You can see that it corresponds to the output you saw when using the connectors endpoint from Postman. Unsecured protocol warning message You might see the following warning message: \"Configuration error for the connector with identifier '<CONNECTOR-ID>': The application is communicating with the connector through a protocol that is not secure.\" You can ignore this in a development environment. This is shown because i2 Analyze and your connector communicate via HTTP. In a production environment, you should secure this connection using HTTPS. For information about configuring the connection to use HTTPS, see Client authenticated Secure Sockets Layer with the i2 Connect gateway . Fix the timezone error Look at the error message about the default timezone. You need to update the config.json file to include a timezone. Use the GET method on the core/temporal/timezones endpoint to get a list of valid timezones. Add your chosen timezone to the top of the config.json file. Update the i2 Analyze connector configurations again, and then use either Postman or Analyst's Notebook Premium to check that the problem is resolved. Adding a service You still have the error about the connector not having any defined services. A valid connector must have at least one service. Next, you need to define and implement a service ."
  },
  "content/walkthrough/4-add-service.html": {
    "href": "content/walkthrough/4-add-service.html",
    "title": "Adding a service",
    "keywords": "Adding a service In this task, you add a simple service with the minimum configuration required for your connector to be valid. For the moment, the service returns to i2 Analyze some dummy data that you create. Later on, it will retrieve real data from the NYPD Complaint Dataset. Remember to consult the troubleshooting guide if you face any issues. Prerequisites Before starting, ensure that you have: Deployed i2 Analyze with the i2 Connect gateway Configured a basic connector Define the service To make i2 Analyze aware that your service exists, you must define it in the connector configuration returned by the config endpoint. This means editing the config.json file in the resources directory. Add a services array At the top level of the configuration, add a services array: { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [] } Add a service In the services array, define a service object. The mandatory fields to provide are the following. Field Description id The unique identifier of the service name The name of the service acquireUrl This specifies the URL for i2 Analyze to use to get data synchronously from the service. If this is present, then async must not be present. For more information about the async option, see Async Connector . clientConfigType This specifies the type of interface a user will interact with when using the service, and takes one of the three following values: NONE - there is no interface required, the service simply runs and returns whatever data it retrieves FORM - the user fills in a form to provide the service with extra information that it needs to run. This is used to provide text-search services, for example. CUSTOM - specifies that a custom client configuration will be used In the case of FORM and CUSTOM client configurations, the specific configuration to be used must be specified by the additional field clientConfigId and there must be a client configuration with the given ID defined in the connector configuration. This will be covered later. It is also common and recommended to add a description field. This appears to users in Analyst's Notebook Premium, so can be used to give more information about what the service does. You should have something like the following. { \"defaultValues\": { \"timeZoneId\": \"Europe/London\", \"resultIdsPersistent\": true }, \"services\": [ { \"id\": \"nypd-service\", \"name\": \"NYPD Connector: Get All\", \"description\": \"A service which retrieves all data\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/all\" } ] } Update the connector's configuration To tell i2 Analyze to pick up these changes, you need to tell i2 Analyze to reload it's connectors' configurations, run: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. If this does not add the relevant changes to Analyst's Notebook, redeploy: setup -t stop setup -t deploy setup -t start View the service in Analyst's Notebook Premium Now that you have defined a service, you can try to use it. Open Analyst's Notebook Premium and login to the i2 Analyze server. If you were already logged in, log out and then back in again. Open External Searches. The service you have defined should appear. Try to run it. Although the service is defined, since it has not yet been implemented, you should see a red banner appear with the message \"Failed to open the selected query. Contact your system administrator.\" To see the full error, look in the IBM_i2_Analysis_Repository.log found in deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services within the i2analyze directory. You should see a 404 (Page Not Found) Error. This is because an endpoint for the acquireUrl defined in the config.json has not been implemented. Implement the service Now that i2 Analyze knows to expect the service, you had better create it. Look at the SPI The SPI you need to implement is documented in the Knowledge Center . Think about the relevant objects you will need to implement in order to build a service that returns a fixed set of entities and links that you will create. Add an acquire endpoint i2 Analyze knows the acquire URL for this service. Now you need to add an endpoint for it in the connector. Java A template has been provided to get started with; see the stage2/nypd-connector directory. This includes: an example configuration config.json with a service defined some changes to ConnectorController a new class ConnectorDataService some REST transport classes. Apply these changes to your code, either manually, or by copying the files over directly. You may need to change the path of the new method in ConnectorController to match the acquireUrl of your service. You need not copy the example config.json if you have defined your service correctly. You can also just use the directory provided as it is setup to be used straight away. Look at how the endpoint is defined in the ConnectorController class and the code that produces the response that is returned to i2 Analyze. Node.js A template has been provided to get started with; see the stage2/nypd-connector directory. This includes: an example configuration config.json with a service defined a new acquire route in app.js a new acquire route with a /test endpoint in the routes folder a new file /helpers/data-service.js containing functions for acquiring data Open the code from the stage2/nypd-connector in VSCode, or any IDE of your choice, and start the connector. Python A template has been provided to get started with; see the stage2/nypd-connector directory. This includes: an example configuration config.json with a service defined some additions to controller.py a new file for service functions, service.py a new file classes.py containing some REST transport classes. Apply these changes to your code, either manually, or by copying the files over directly. You may need to change the path of the new method in controller.py to match the acquireUrl of your service. You need not copy the example config.json if you have defined your service correctly. You can also just use the directory provided as it is setup to be used straight away. Look at how the endpoint is defined in the controller.py class and the code that produces the response that is returned to i2 Analyze. Test the service in Analyst's Notebook Premium Redeploy the connector. Depending on how you are running the connector, this may be done automatically for you when changes are made. Re-run the service in Analyst's Notebook Premium via the External Searches window. You should no longer get a 404 Error, but the search will return nothing Return some data Java Make changes so that the service returns some entities and links - just create some dummy data for now. You will need to complete the implementation of the LinkData class provided. Test your changes in Analyst's Notebook in the same way. If you don't see what you expect to, or you come across an error, investigate the IBM_i2_Analysis_Repository.log in the deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services directory. Node.js Make changes so that the service returns some entities and links - just create some dummy data for now. Test your changes in Analyst's Notebook in the same way. If you don't see what you expect to, or you come across an error, investigate the IBM_i2_Analysis_Repository.log in the deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services directory. Python Make changes so that the service returns some entities and links - just create some dummy data for now. You will need to complete the implementation of the Link class in classes.py . Ensure you are importing the classes into service.py . Test your changes in Analyst's Notebook in the same way. If you don't see what you expect to, or you come across an error, investigate the IBM_i2_Analysis_Repository.log in the deploy\\wlp\\usr\\servers\\opal-server\\logs\\opal-services directory. It is vital that you return property values in the format expected, which is defined by the logical type of each property. See the data model examples for examples of each supported logical type. Querying data from an external source Now that you have a basic connector with a working simple service, you can make it more useful by returning real data ."
  },
  "content/walkthrough/5-connect-to-eds.html": {
    "href": "content/walkthrough/5-connect-to-eds.html",
    "title": "Connect to an external datasource",
    "keywords": "Connect to an external datasource Here, you will connect to the NYPD Complaint Dataset as your external datasource and marshal the data into entities, links, and properties so that you can return results which can be displayed in Analyst's Notebook Premium. Again, use the troubleshooting guide if you need to. Create a Socrata app token You need an app token which will allow you make unlimited requests to Socrata's API (within reason). If you don't use an app token, the APIs will throttle by IP address. Visit this link to register your account. Click on your name in the top right to access My Profile . Click on Edit Account Settings . In the side-pane, click on Developer Settings . At the bottom of the page, click Create New App Token , specify your own \"Application Name\" and \"Description\" and save. If you leave the site for any reason, you can always retrieve your app token again by logging into your account again. Query the external datasource Retrieve the raw data Java Look at the version of code in the stage3/nypd-connector directory. There are changes to ConnectorController and the application.properties file. Apply these to your code manually or copy over these two files. If you're copying them, you may need to change the paths of the endpoints defined in ConnectorController . In application.properties shown below, specify the NYPD Complaint Dataset API resource for the socrata.url key as the URL in the comment and your Socrata API Token for the socrata.api.token key. server.port=9081 # Resource URL, for example https://data.cityofnewyork.us/resource/7x9x-zpz6.json socrata.url= # API Token. Create a Socrata account and create an API Token. Paste it here socrata.api.token= You need to implement the ExternalConnectorDataService and SocrataResponseData classes so that they retrieve data from the NYPD Complaint Dataset and use it to create entities and links to be returned to i2 Analyze. It should not be necessary to modify the example SocrataClient . The dataset can be queried using SoQL (Socrata Query Language). To do this, you must construct a URL with specified parameters (if necessary) to retrieve the data. By default, a $limit parameter has been set to the value of 1 to restrict the number of records retrieved. It's best to keep this value small to reduce the response time of each request until you are more comfortable with SoQL. final Map<String, Object> params = new HashMap<>(); params.put(\"limitValue\", 1); // Only returning 1 entity for the moment. increase when ready final String url = \"?$limit={limitValue}\"; // Make the request and map the whole response body as a string so that you can // see what is returned // TODO: Remove this since it's just for debugging System.out.println(socrataClient.get(url, String.class, params)); Node.js Look at the version of code in the stage3/nypd-connector directory. This includes: Changes to acquire.js route A new socrata-config.js file A new socrata-data-service.js file socrata-config.js is where the NYPD Complaint Dataset API resource and the Socrata token are defined. module.exports = { url: \"https://data.cityofnewyork.us/resource/7x9x-zpz6.json\", token: \"SET YOUR TOKEN\" }; socrata-data-service.js contains functions to query NYPD Complaint Dataset. The dataset can be queried using SoQL (Socrata Query Language). To do this, you must construct a URL with specified parameters (if necessary) to retrieve the data. By default, a $limit parameter has been set to the value of 1 to restrict the number of records retrieved. It's best to keep this value small to reduce the response time of each request until you are more comfortable with SoQL. const URL = `${socrata.url}?$$app_token=${socrata.token}&$limit=${limitValue}`; Open the code from the stage3/nypd-connector in VSCode, or any IDE of your choice, and start the connector. You need to map the data received into entities and links and return them to i2 Analyze. Python Look at the version of code in the stage3/nypd-connector directory. Changes have been made to controller.py and there is now an additional resource file: application.yml . Apply these to your code manually or copy over these two files. If you're copying them, you may need to change the paths of the endpoints defined in controller.py . In application.yml shown below, specify the NYPD Complaint Dataset API resource for the socrata.url key as the URL in the comment and your Socrata API Token for the socrata.token key. socrata: url: https://data.cityofnewyork.us/resource/7x9x-zpz6.json token: # Replace with Socrata API token You need to implement the query_external_datasource function in service.py so that it retrieves data from the NYPD Complaint Dataset and uses it to create entities and links to be returned to i2 Analyze. The dataset can be queried using SoQL (Socrata Query Language). To do this, you must construct a URL with specified parameters (if necessary) to retrieve the data. By default, a $limit parameter has been set to the value of 1 to restrict the number of records retrieved. It's best to keep this value small to reduce the response time of each request until you are more comfortable with SoQL. with open('static/application.yml') as yml_file: config = yaml.safe_load(yml_file) base_url = config['socrata']['url'] api_token = config['socrata']['token'] limit = 1 request_url = f\"{base_url}?$limit={limit}\" x = requests.get(request_url, headers = { 'X-App-Token': api_token }) (Optional) Verify the data It's worth testing that you are successfully querying the data and returning results. Print the returned value to the console and check that it matches with the data you see when you make a request to the acquire endpoint via Postman. Marshal the data to objects To make it easier to create entities and links using the data retrieved, you can create a class (Java or Python) or JavaScript object (Node.js) to represent a single row of the dataset. This will have a field for each of the columns of the data. You can then write a function that serializes the incoming data into a collection of these objects. Note that in Java, there exists a library which makes this process much easier: jackson-annotations . You might want to add source references to the entities and links that are returned by your connector. This allows users trace the source of the data represented by those entities and links. For information on adding source references, see here . (Optional) Verify your marshalling function Test that you are successfully marshalling the data. You should be able to assert against the properties of your object to verify the expected and actual results are equal. Extract entities and links from objects You can create entities and links from the objects that represent rows of the dataset and define their properties using the relevant fields. Implement this extraction; deriving entities from each record as well as establishing links between them. Create a response object with a list of entities and links to be returned. You need to take care not to duplicate entities in the list. Also take care again to assign property values in the correct format. Refer to the data model examples again if you need. (Optional) Validate your return response Verify that the response returned from your function is valid and is as expected. View results in Analyst's Notebook Premium You should now be able to log into Analyst's Notebook Premium and run your query. If there are any errors, you may want to check that your schema is in the right shape, that your data is clean and that there are no missing values. Next steps Next, you can configure your own parameterized search ."
  },
  "content/walkthrough/6-parameterised-search.html": {
    "href": "content/walkthrough/6-parameterised-search.html",
    "title": "Parameterized search",
    "keywords": "Parameterized search A parameterized search passes defined conditions that you can use to drive your searches. If you have any problems during this task, remember to consult the troubleshooting guide . Configuration Add a new service You will need to add a service for parameterized searches to the services array in the config.json . For this service, you need to set the clientConfigType . In this example, you will set the value to be FORM so that you can specify your conditions via fields in a form that will be shown in Analyst's Notebook Premium. { ... \"services\": [ { \"id\": \"nypd-search-service\", \"name\": \"NYPD Connector: Search\", \"description\": \"A service for conditional searches\", \"clientConfigType\": \"FORM\", \"clientConfigId\": \"searchForm\", \"acquireUrl\": \"/search\" } ] } Define search fields You will need to define the search fields in a clientConfigs array at the root level of your config.json . For example: { \"defaultValues\": { ... }, \"services\": [ ... ], \"clientConfigs\": [ { \"id\": \"searchForm\", \"config\": { \"sections\": [ { \"conditions\": [ { \"id\": \"searchTerm\", \"label\": \"made-up-field (e.g. Complaint Number)\", \"mandatory\": false, \"logicalType\": \"SINGLE_LINE_STRING\" } ] } ] } } ] } For more information on client configuration, refer to the Knowledge Center . Test search fields Check that your fields work in Analyst's Notebook Premium, you need to tell i2 Analyze to reload it's connectors' configurations, run: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. You can then: Log out and log back into Analyst's Notebook Premium to see the configuration changes and your newly-defined service. Try running it. You should receive an error as your condition fields have been defined but not yet implemented. Implementation It's time to implement these conditions. Add an acquire endpoint for your service Java i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage4/nypd-connector directory provided. This includes: An example config.json with a parameterized search service and search fields defined. This is just a template in case you have not already defined a new service Changes to the ConnectorController class Some extra REST transport classes Apply these changes to your code, either manually, or by copying the relevant files. If you are copying the files, you may need to change the path of the new method in ConnectorController . Look at how the endpoint is defined in the ConnectorController class and think about how you should implement this service. Node.js i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage4/nypd-connector directory provided. This includes: An example config.json with a parameterized search service and search fields defined. This is just a template in case you have not already defined a new service A new validate route A new /search endpoint in the acquire route Open the code from the stage4/nypd-connector in VSCode, or any IDE of your choice, and start the connector. You will need to implement findComplaint function in socrata-data-service.js file and fix all TODO's. Python i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage4/nypd-connector directory provided. This includes: An example config.json with a parameterized search service and search fields defined. This is just a template in case you have not already defined a new service Changes to the controller.py file Some extra REST transport classes Apply these changes to your code, either manually, or by copying the relevant files. If you are copying the files, you may need to change the path of the new method in controller.py . Look at how the endpoint is defined in the controller.py class and think about how you should implement this service. Access conditions You will need to parse the conditions passed in the request according to the SPI and return a response containing entities and links. For an example of how your connector may receive requests, and the responses it may return, you can view some SPI examples here . You will need to create basic POJOs to parse the request and access the condition information. To do that please refer to the DaodRequest model in the SPI. The list of conditions can be accessed via request.payload.conditions . Filter data by conditions When you have a list of conditions, you can use their id and value fields to determine which of the entities retrieved from the datasource match the parameters given by the user in the form. Run your query Update the connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Open Analyst's Notebook Premium. Click on \"External Search\". Select your parameterized search service. Provide a value to the condition field and click \"Run\". You should now see a resulting list of entities that satisfy your condition. Next steps Now you can implement seeded searches ."
  },
  "content/walkthrough/7-seeded-search.html": {
    "href": "content/walkthrough/7-seeded-search.html",
    "title": "Seeded search",
    "keywords": "Seeded search A seeded search passes information from an entity on the chart to the connector so that you can use that information to drive your searches. In order to pass an entity as a seed, select an entity in Analyst's Notebook Premium and open \"External Search\". Again, if you face any issues during this task, remember to consult the troubleshooting guide . Configuration You need to configure a service to allow for seeded searches. Add a new service You will need to add a new service for seeded searches to the services array in the config.json . You will also need to add seedConstraints which define the entities allowed by the seeded search. For example: { \"services\": [ { \"id\": \"nypd-find-like-this-complaint-service\", \"name\": \"NYPD Connector: Find like this Complaint\", \"description\": \"A service which finds a similar complaint\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/find-like-this-complaint\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\", \"min\": 1, \"max\": 1 } ] } } }, { \"id\": \"nypd-expand-service\", \"name\": \"NYPD Connector: Expand\", \"description\": \"A service which executes an Expand operation on a seed\", \"clientConfigType\": \"NONE\", \"acquireUrl\": \"/expand\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\" }, { \"id\": \"made-up-schema-type-id (e.g. ET1)\" } ] } } } ] } Check service in Analyst's Notebook Premium Check that your service now appears in the list of defined services in Analyst's Notebook Premium. Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Log out and log back into Analyst's Notebook Premium to see the configuration changes and your newly-defined service. (If the service does not show up, try unchecking the Hide queries whose requirements are not met box). Try running it by pre-clicking on an entity. You should receive an error as your seeded search has been defined but not yet implemented. Implementation It's time to implement these conditions. Add an acquire endpoint for your service Java i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage5/nypd-connector folder provided. This includes: An example config.json with a seeded search service defined. This is just a template in case you have not already defined a new service. Changes to ConnectorController Some new REST transport classes Apply the code changes, either manually or by copying the relevant files over. You may need to change the path of the new method in ConnectorController if copying the files completely. Notice how the endpoint is defined in the ConnectorController class. Node.js i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage5/nypd-connector folder provided. This includes: An example config.json with a seeded search service defined. This is just a template in case you have not already defined a new service. Changes to acquire route to reflect changes in config.json Changes to socrata-data-service.js file to have all the functions needed for acquire route Open the code from the stage5/nypd-connector in VSCode, or any IDE of your choice, and start the connector. You will need to implement functions defined in socrata-data-service.js file and fix all TODO's. Python i2 Analyze knows the acquire URL decided on for this service. Now you need to add the corresponding endpoint in the connector. You have a template to get started with; see the stage5/nypd-connector folder provided. This includes: An example config.json with a seeded search service defined. This is just a template in case you have not already defined a new service. Changes to controller.py Some new REST transport classes Apply the code changes, either manually or by copying the relevant files over. You may need to change the path of the new method in controller.py if copying the files completely. Notice how the endpoint is defined in the controller.py class. Access seeds You will need to manipulate a seed passed into the request according to the SPI and return a response containing entities and links. You will also need to create basic POJOs to parse the request and access the seed information. Filter data based on seed How you will use the seed depends on what you are trying to achieve. Find Like This seeded search A Find Like This query looks at the property values of a selected record and searches for data in the external source that has the same or similar property values. For this service you will need to filter out your entities based on the matching properties of the seed entity that can be accessed via request.payload.seeds.entities.get(0) . NOTE : Do not return the entity that was passed as the seed . Expand seeded search An Expand query takes an entity as a seed and returns a list of entities and links that are connected to the seed. For this service you will need to find all links connected to the seed entity that can be accessed via request.payload.seeds.entities.get(0) . Then you will need to find all entities connected to these links. Finally, you need to make sure that the link is pointing to the seedId . To do that, you will need to change toEndId or fromEndId to the seedId that can be accessed via request.payload.seeds.entities.get(0).sourceIds.get(0).key.get(2) . Run your query Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Open Analyst's Notebook Premium. Select an entity on the chart. Click on \"External Search\". Click on your seeded search service to run it. You should now see a resulting list of entities which are connected to the entity you initially selected. Next steps Next, you can combine what you've learned from these past two sections to implement seeded parameterized searches ."
  },
  "content/walkthrough/8-seeded-parameterised-search.html": {
    "href": "content/walkthrough/8-seeded-parameterised-search.html",
    "title": "Seeded parameterized search",
    "keywords": "Seeded parameterized search Simply put, a seeded parameterized search is a combination of seeded and parameterized searches. This type of search passes information from an entity on the chart to the connector together with conditions that are used to drive searches. Should you have any problems during this task, please consult the troubleshooting guide . Configuration You need to configure a service to allow for seeded parameterized searches. Add a new service You will need to set the clientConfigType and clientConfigId values similar to configuring parameterized search configuration. You will also need to add seedConstraints to define constraints on the seed similar to configuring seeded search. For example: { \"services\": [ ... { \"id\": \"nypd-expand-with-conditions\", \"name\": \"NYPD Connector: Expand with Conditions\", \"description\": \"A service which executes an Expand operation on a seed with conditions\", \"clientConfigType\": \"FORM\", \"clientConfigId\": \"expandForm\", \"acquireUrl\": \"/expand-with-conditions\", \"seedConstraints\": { \"min\": 1, \"max\": 1, \"seedTypes\": { \"allowedTypes\": \"ENTITY\", \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\" }, { \"id\": \"made-up-schema-type-id (e.g. ET1)\" } ] } } } ] } Optionally, you can also define the type location for the item type with the field typeLocation . This can be used to determine in which schema the item type resides. Possible value are 'CONNECTOR', 'GATEWAY' and 'INFOSTORE'. The itemTypes object will look like this: { \"seedTypes\": { ... \"itemTypes\": [ { \"id\": \"made-up-schema-type-id (e.g. ET1)\", \"typeLocation\": \"CONNECTOR\" } ] } } You will also need to provide a clientConfig with the id set in the service configuration. It should look something like this: { ... \"clientConfigs\": [ { \"id\": \"expandForm\", \"config\": { \"sections\": [ { \"conditions\": [ { \"id\": \"made-up-id (e.g. searchBorough)\", \"label\": \"made-up-field (e.g. Borough)\", \"logicalType\": \"SINGLE_LINE_STRING\" } ] } ] } } ] } You can change these conditions to relate to your own schema and what you want to search for. Check the service in Analyst's Notebook Premium Let's check that your service now appears in the list of defined services in Analyst's Notebook Premium. Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Log out and log back into Analyst's Notebook Premium to see the configuration changes and our newly-defined service. Try running it. You should receive an error as your seeded parameterized search has been defined but not yet implemented. Implementation It's time to implement the seeded parameterized search. Add an acquire endpoint for your service In the same fashion as the other services you have defined, add an acquire endpoint for this service in your controller file. Access conditions and seeds You will need to manipulate the seeds and conditions passed in the request according to the SPI and return a response containing entities and links. You will need to create basic POJOs to parse the request and access the condition and seed information contained in request.payload . Filter data based conditions and seeds An Expand With Conditions query takes an entity as a seed and returns a list of entities and links that are connected to the seed that satisfy the list of conditions provided by the user. For this service you will need to find all links connected to the seed entity that also satisfy your conditions . In the example, only links that had been created after the date provided by the user were return. Then you will need to find all entities connected to these links. Finally, you need to make sure that the link is pointing to the seedId . To do that, you will need to change toEndId or fromEndId to the seedId that can be accessed via request.payload.seeds.entities.get(0).sourceIds.get(0).key.get(2) . Run your query Update connectors' configuration by running: setup -t updateConnectorsConfiguration Use Postman to run the Reload request (running the Form Based Login request if you have not already), which will configure your changes to the topology. Then: Open Analyst's Notebook. Select an entity on the chart. Click on \"External Search\". Click on your seeded parameterized search service. Provide a value to the condition field and click \"Run\". You should now see a resulting list of entities which are connected to the entity you initially selected and also satisfy the conditions you defined. Next steps Now that you've completed this, you can look into validating your requests ."
  },
  "content/walkthrough/9-validation.html": {
    "href": "content/walkthrough/9-validation.html",
    "title": "Validation",
    "keywords": "Validation You can validate requests at the gateway to ensure they are in the correct form before sending them to the respective service. Consult the troubleshooting guide if needed during this task. Client-side validation Validation can be performed on the client for simple input checks before sending a request to the gateway, such as ensuring the presence of values for mandatory fields, or verifying that input values are in the correct format. Client-side validation is configured via the connector's configuration in the same way as parameterized searches. Below is an example configuration for validating that input values for the 'Offence' field start with two letters and two numbers: { \"conditions\": [ { \"id\": \"searchTerm\", \"label\": \"Offence\", \"logicalType\": \"SINGLE_LINE_STRING\", \"mandatory\": false, \"extraStringValidation\": { \"regex\": \"[A-Za-z]{2}\\\\d{2}\", \"message\": \"Case number must start with 2 letters then 2 numbers\" } } ] } The mandatory property in the config.json file specifies whether a field is required or not. The extraStringValidation property property allows regex validation be performed with custom error messages to be sent back to the client when a value does not comply with the rule. You can find out more on accepted condition properties by looking at the /{configurationUrl} endpoint definition in the i2 Connect gateway REST SPI . Server-side validation Validation can be performed on the server for more complex inputs. For example, if there are 3 input fields and you require at least one to be set but they are otherwise optional. In your connector, you can check that the user has defined at least one condition in the request. In another case, if you have two date fields and want to support searching a range of dates, you can validate that the start date is before the end date. 1. Add the validation endpoint to the connector configuration In your connector configuration, add the validateUrl property after the existing acquireUrl property as shown in the snippet below. Set the value to the endpoint where you will implement your server-side validation logic. { \"id\": \"nypd-search-service\", \"name\": \"NYPD Connector: Search\", \"description\": \"A service for conditional searches\", \"clientConfigType\": \"FORM\", \"clientConfigId\": \"searchForm\", \"acquireUrl\": \"/search\", \"validateUrl\": \"/search/validate\" } 2. Implement the validation endpoint In your code, implement the server-side logic for the validate endpoint using the conditions in the request. The payload that the endpoint receives in the request contains all the same condition and seed information that the acquire endpoint receives. If validation succeeds according to your logic, return a 200 response code. The body of the response must either be an empty object or an object containing an errorMessage with a null value. For example: { \"errorMessage\": null } If it fails, return an object containing an errorMessage with your error message: { \"errorMessage\": \"This is the error message displayed.\" } When the i2 Connect gateway receives a non-null errorMessage , it does not subsequently send a request to the acquire endpoint. More information can be found looking at the /{validationUrl} endpoint definition in the i2 Connect gateway REST SPI . 3. Reload the i2 Connect gateway Instruct the i2 Connect gateway to reload the configuration. Test that your new validation has the correct behavior."
  },
  "index.html": {
    "href": "index.html",
    "title": "i2 Connect Example Connectors",
    "keywords": "i2 Connect Example Connectors Developing a connector walkthrough The documentation for understanding, deploying and configuring both i2 Analyze and the example connector is divided into the following sections: Building an i2 Analyze schema and working with the i2 Connect gateway Deploying i2 Analyze Configuring a connector Creating a service Querying an external data source Implementing parameterized searches Implementing seeded searches Implementing seeded, parameterized searches Validating requests Further materials for connector development: The example asynchronous connector Implementing Principal Propagation Environment setup The workshop requires the following tools and technologies: IBM i2 Analyze 4.3.3 : The server that hosts the i2 Connect gateway. IBM i2 Analyst's Notebook Premium 9.2.3 : The client that provides the user interface for interacting with your connector. The client displays the resulting entity and link records with their properties. IBM i2 Analyze Schema Designer : The tool that enables you to design and create your i2 Analyze schema. During installation of Analyst's Notebook Premium, ensure that you also install Schema Designer. Microsoft Visual Studio Code : The recommended IDE for developing your connector. You can use any IDE you like, but the Visual Studio Code Spring Boot Dashboard plugin can handle running and redeploying the connector for you. Download it here . Postman : An API development tool that you can use to create and execute requests against REST endpoints. Download Postman here . An application framework dependent on the language who are writing the connector with. Java (Spring Boot) : The connector is written in Java with Spring Boot. Spring Boot is a Spring framework with an embedded Tomcat server that makes it easy to spin up a web application. No installation is required. Node.js (Express) : The connector is written with in Node with Express. Express is the fast, unopinionated, minimalist web framework for Node.js that provides a robust set of features for web and mobile applications. Python (Flask) : The Python connector is written and run on a Flask application. Flask is a micro web framework which requires no particular tools or libraries. No manual installation other than the specified setup instructions is required as the Pipfile contains it as a dependency. Additional resources To help you to understand some of the tools and technologies you will use, here are a few resources that contain more information about them: i2 Analyze data model examples Example requests and responses Using Postman Running the example connectors: Java Node.js Python Troubleshooting Flexible schemas The documentation for understanding and configuring connector schemas, gateway schemas, and item type mappings is divided into the following sections: Connector schemas Gateway schemas Type location Item type mapping The example connectors used are: NYPD Connector KCPD Connector ERI Connector"
  }
}